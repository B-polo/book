<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.2 Dimensionality reduction techniques: Visualizing complex data sets in 2D | Computational Genomics with R</title>
  <meta name="description" content="A guide to computationa genomics using R. The book covers fundemental topics with practical examples for an interdisciplinery audience" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="4.2 Dimensionality reduction techniques: Visualizing complex data sets in 2D | Computational Genomics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://compmgenomr.github.io/book/" />
  <meta property="og:image" content="https://compmgenomr.github.io/book/images/cover.jpg" />
  <meta property="og:description" content="A guide to computationa genomics using R. The book covers fundemental topics with practical examples for an interdisciplinery audience" />
  <meta name="github-repo" content="compgenomr/book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.2 Dimensionality reduction techniques: Visualizing complex data sets in 2D | Computational Genomics with R" />
  
  <meta name="twitter:description" content="A guide to computationa genomics using R. The book covers fundemental topics with practical examples for an interdisciplinery audience" />
  <meta name="twitter:image" content="https://compmgenomr.github.io/book/images/cover.jpg" />

<meta name="author" content="Altuna Akalin" />


<meta name="date" content="2020-09-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="clustering-grouping-samples-based-on-their-similarity.html"/>
<link rel="next" href="exercises-2.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-83786243-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-83786243-1');
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Computational Genomics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="who-is-this-book-for.html"><a href="who-is-this-book-for.html"><i class="fa fa-check"></i>Who is this book for?</a><ul>
<li class="chapter" data-level="" data-path="who-is-this-book-for.html"><a href="who-is-this-book-for.html#what-will-you-get-out-of-this"><i class="fa fa-check"></i>What will you get out of this?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="structure-of-the-book.html"><a href="structure-of-the-book.html"><i class="fa fa-check"></i>Structure of the book</a></li>
<li class="chapter" data-level="" data-path="software-information-and-conventions.html"><a href="software-information-and-conventions.html"><i class="fa fa-check"></i>Software information and conventions</a><ul>
<li class="chapter" data-level="" data-path="software-information-and-conventions.html"><a href="software-information-and-conventions.html#assignment-operator-convention"><i class="fa fa-check"></i>Assignment operator convention</a></li>
<li class="chapter" data-level="" data-path="software-information-and-conventions.html"><a href="software-information-and-conventions.html#packages-needed-to-run-the-book-code"><i class="fa fa-check"></i>Packages needed to run the book code</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="data-for-the-book.html"><a href="data-for-the-book.html"><i class="fa fa-check"></i>Data for the book</a></li>
<li class="chapter" data-level="" data-path="exercises-in-the-book.html"><a href="exercises-in-the-book.html"><i class="fa fa-check"></i>Exercises in the book</a></li>
<li class="chapter" data-level="" data-path="reproducibility-statement.html"><a href="reproducibility-statement.html"><i class="fa fa-check"></i>Reproducibility statement</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="how-to-contribute.html"><a href="how-to-contribute.html"><i class="fa fa-check"></i>How to contribute</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to Genomics</a><ul>
<li class="chapter" data-level="1.1" data-path="genes-dna-and-central-dogma.html"><a href="genes-dna-and-central-dogma.html"><i class="fa fa-check"></i><b>1.1</b> Genes, DNA and central dogma</a><ul>
<li class="chapter" data-level="1.1.1" data-path="genes-dna-and-central-dogma.html"><a href="genes-dna-and-central-dogma.html#what-is-a-genome"><i class="fa fa-check"></i><b>1.1.1</b> What is a genome?</a></li>
<li class="chapter" data-level="1.1.2" data-path="genes-dna-and-central-dogma.html"><a href="genes-dna-and-central-dogma.html#what-is-a-gene"><i class="fa fa-check"></i><b>1.1.2</b> What is a gene?</a></li>
<li class="chapter" data-level="1.1.3" data-path="genes-dna-and-central-dogma.html"><a href="genes-dna-and-central-dogma.html#how-are-genes-controlled-transcriptional-and-post-transcriptional-regulation"><i class="fa fa-check"></i><b>1.1.3</b> How are genes controlled? Transcriptional and post-transcriptional regulation</a></li>
<li class="chapter" data-level="1.1.4" data-path="genes-dna-and-central-dogma.html"><a href="genes-dna-and-central-dogma.html#what-does-a-gene-look-like"><i class="fa fa-check"></i><b>1.1.4</b> What does a gene look like?</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="elements-of-gene-regulation.html"><a href="elements-of-gene-regulation.html"><i class="fa fa-check"></i><b>1.2</b> Elements of gene regulation</a><ul>
<li class="chapter" data-level="1.2.1" data-path="elements-of-gene-regulation.html"><a href="elements-of-gene-regulation.html#transcriptional-regulation"><i class="fa fa-check"></i><b>1.2.1</b> Transcriptional regulation</a></li>
<li class="chapter" data-level="1.2.2" data-path="elements-of-gene-regulation.html"><a href="elements-of-gene-regulation.html#post-transcriptional-regulation"><i class="fa fa-check"></i><b>1.2.2</b> Post-transcriptional regulation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="shaping-the-genome-dna-mutation.html"><a href="shaping-the-genome-dna-mutation.html"><i class="fa fa-check"></i><b>1.3</b> Shaping the genome: DNA mutation</a></li>
<li class="chapter" data-level="1.4" data-path="high-throughput-experimental-methods-in-genomics.html"><a href="high-throughput-experimental-methods-in-genomics.html"><i class="fa fa-check"></i><b>1.4</b> High-throughput experimental methods in genomics</a><ul>
<li class="chapter" data-level="1.4.1" data-path="high-throughput-experimental-methods-in-genomics.html"><a href="high-throughput-experimental-methods-in-genomics.html#the-general-idea-behind-high-throughput-techniques"><i class="fa fa-check"></i><b>1.4.1</b> The general idea behind high-throughput techniques</a></li>
<li class="chapter" data-level="1.4.2" data-path="high-throughput-experimental-methods-in-genomics.html"><a href="high-throughput-experimental-methods-in-genomics.html#high-throughput-sequencing"><i class="fa fa-check"></i><b>1.4.2</b> High-throughput sequencing</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="visualization-and-data-repositories-for-genomics.html"><a href="visualization-and-data-repositories-for-genomics.html"><i class="fa fa-check"></i><b>1.5</b> Visualization and data repositories for genomics</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Rintro.html"><a href="Rintro.html"><i class="fa fa-check"></i><b>2</b> Introduction to R for Genomic Data Analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html"><i class="fa fa-check"></i><b>2.1</b> Steps of (genomic) data analysis</a><ul>
<li class="chapter" data-level="2.1.1" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html#data-collection"><i class="fa fa-check"></i><b>2.1.1</b> Data collection</a></li>
<li class="chapter" data-level="2.1.2" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html#data-quality-check-and-cleaning"><i class="fa fa-check"></i><b>2.1.2</b> Data quality check and cleaning</a></li>
<li class="chapter" data-level="2.1.3" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html#data-processing"><i class="fa fa-check"></i><b>2.1.3</b> Data processing</a></li>
<li class="chapter" data-level="2.1.4" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html#exploratory-data-analysis-and-modeling"><i class="fa fa-check"></i><b>2.1.4</b> Exploratory data analysis and modeling</a></li>
<li class="chapter" data-level="2.1.5" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html#visualization-and-reporting"><i class="fa fa-check"></i><b>2.1.5</b> Visualization and reporting</a></li>
<li class="chapter" data-level="2.1.6" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html#why-use-r-for-genomics"><i class="fa fa-check"></i><b>2.1.6</b> Why use R for genomics ?</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html"><i class="fa fa-check"></i><b>2.2</b> Getting started with R</a><ul>
<li class="chapter" data-level="2.2.1" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#installing-packages"><i class="fa fa-check"></i><b>2.2.1</b> Installing packages</a></li>
<li class="chapter" data-level="2.2.2" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#installing-packages-in-custom-locations"><i class="fa fa-check"></i><b>2.2.2</b> Installing packages in custom locations</a></li>
<li class="chapter" data-level="2.2.3" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#getting-help-on-functions-and-packages"><i class="fa fa-check"></i><b>2.2.3</b> Getting help on functions and packages</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="computations-in-r.html"><a href="computations-in-r.html"><i class="fa fa-check"></i><b>2.3</b> Computations in R</a></li>
<li class="chapter" data-level="2.4" data-path="data-structures.html"><a href="data-structures.html"><i class="fa fa-check"></i><b>2.4</b> Data structures</a><ul>
<li class="chapter" data-level="2.4.1" data-path="data-structures.html"><a href="data-structures.html#vectors"><i class="fa fa-check"></i><b>2.4.1</b> Vectors</a></li>
<li class="chapter" data-level="2.4.2" data-path="data-structures.html"><a href="data-structures.html#matrices"><i class="fa fa-check"></i><b>2.4.2</b> Matrices</a></li>
<li class="chapter" data-level="2.4.3" data-path="data-structures.html"><a href="data-structures.html#data-frames"><i class="fa fa-check"></i><b>2.4.3</b> Data frames</a></li>
<li class="chapter" data-level="2.4.4" data-path="data-structures.html"><a href="data-structures.html#lists"><i class="fa fa-check"></i><b>2.4.4</b> Lists</a></li>
<li class="chapter" data-level="2.4.5" data-path="data-structures.html"><a href="data-structures.html#factors"><i class="fa fa-check"></i><b>2.4.5</b> Factors</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="data-types.html"><a href="data-types.html"><i class="fa fa-check"></i><b>2.5</b> Data types</a></li>
<li class="chapter" data-level="2.6" data-path="reading-and-writing-data.html"><a href="reading-and-writing-data.html"><i class="fa fa-check"></i><b>2.6</b> Reading and writing data</a><ul>
<li class="chapter" data-level="2.6.1" data-path="reading-and-writing-data.html"><a href="reading-and-writing-data.html#reading-large-files"><i class="fa fa-check"></i><b>2.6.1</b> Reading large files</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="plotting-in-r-with-base-graphics.html"><a href="plotting-in-r-with-base-graphics.html"><i class="fa fa-check"></i><b>2.7</b> Plotting in R with base graphics</a><ul>
<li class="chapter" data-level="2.7.1" data-path="plotting-in-r-with-base-graphics.html"><a href="plotting-in-r-with-base-graphics.html#combining-multiple-plots"><i class="fa fa-check"></i><b>2.7.1</b> Combining multiple plots</a></li>
<li class="chapter" data-level="2.7.2" data-path="plotting-in-r-with-base-graphics.html"><a href="plotting-in-r-with-base-graphics.html#saving-plots"><i class="fa fa-check"></i><b>2.7.2</b> Saving plots</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="plotting-in-r-with-ggplot2.html"><a href="plotting-in-r-with-ggplot2.html"><i class="fa fa-check"></i><b>2.8</b> Plotting in R with ggplot2</a><ul>
<li class="chapter" data-level="2.8.1" data-path="plotting-in-r-with-ggplot2.html"><a href="plotting-in-r-with-ggplot2.html#combining-multiple-plots-1"><i class="fa fa-check"></i><b>2.8.1</b> Combining multiple plots</a></li>
<li class="chapter" data-level="2.8.2" data-path="plotting-in-r-with-ggplot2.html"><a href="plotting-in-r-with-ggplot2.html#ggplot2-and-tidyverse"><i class="fa fa-check"></i><b>2.8.2</b> ggplot2 and tidyverse</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="functions-and-control-structures-for-ifelse-etc-.html"><a href="functions-and-control-structures-for-ifelse-etc-.html"><i class="fa fa-check"></i><b>2.9</b> Functions and control structures (for, if/else etc.)</a><ul>
<li class="chapter" data-level="2.9.1" data-path="functions-and-control-structures-for-ifelse-etc-.html"><a href="functions-and-control-structures-for-ifelse-etc-.html#user-defined-functions"><i class="fa fa-check"></i><b>2.9.1</b> User-defined functions</a></li>
<li class="chapter" data-level="2.9.2" data-path="functions-and-control-structures-for-ifelse-etc-.html"><a href="functions-and-control-structures-for-ifelse-etc-.html#loops-and-looping-structures-in-r"><i class="fa fa-check"></i><b>2.9.2</b> Loops and looping structures in R</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>2.10</b> Exercises</a><ul>
<li class="chapter" data-level="2.10.1" data-path="exercises.html"><a href="exercises.html#computations-in-r-1"><i class="fa fa-check"></i><b>2.10.1</b> Computations in R</a></li>
<li class="chapter" data-level="2.10.2" data-path="exercises.html"><a href="exercises.html#data-structures-in-r"><i class="fa fa-check"></i><b>2.10.2</b> Data structures in R</a></li>
<li class="chapter" data-level="2.10.3" data-path="exercises.html"><a href="exercises.html#reading-in-and-writing-data-out-in-r"><i class="fa fa-check"></i><b>2.10.3</b> Reading in and writing data out in R</a></li>
<li class="chapter" data-level="2.10.4" data-path="exercises.html"><a href="exercises.html#plotting-in-r"><i class="fa fa-check"></i><b>2.10.4</b> Plotting in R</a></li>
<li class="chapter" data-level="2.10.5" data-path="exercises.html"><a href="exercises.html#functions-and-control-structures-for-ifelse-etc.-1"><i class="fa fa-check"></i><b>2.10.5</b> Functions and control structures (for, if/else, etc.)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="stats.html"><a href="stats.html"><i class="fa fa-check"></i><b>3</b> Statistics for Genomics</a><ul>
<li class="chapter" data-level="3.1" data-path="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html"><a href="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html"><i class="fa fa-check"></i><b>3.1</b> How to summarize collection of data points: The idea behind statistical distributions</a><ul>
<li class="chapter" data-level="3.1.1" data-path="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html"><a href="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#describing-the-central-tendency-mean-and-median"><i class="fa fa-check"></i><b>3.1.1</b> Describing the central tendency: Mean and median</a></li>
<li class="chapter" data-level="3.1.2" data-path="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html"><a href="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#describing-the-spread-measurements-of-variation"><i class="fa fa-check"></i><b>3.1.2</b> Describing the spread: Measurements of variation</a></li>
<li class="chapter" data-level="3.1.3" data-path="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html"><a href="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#precision-of-estimates-confidence-intervals"><i class="fa fa-check"></i><b>3.1.3</b> Precision of estimates: Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="how-to-test-for-differences-between-samples.html"><a href="how-to-test-for-differences-between-samples.html"><i class="fa fa-check"></i><b>3.2</b> How to test for differences between samples</a><ul>
<li class="chapter" data-level="3.2.1" data-path="how-to-test-for-differences-between-samples.html"><a href="how-to-test-for-differences-between-samples.html#randomization-based-testing-for-difference-of-the-means"><i class="fa fa-check"></i><b>3.2.1</b> Randomization-based testing for difference of the means</a></li>
<li class="chapter" data-level="3.2.2" data-path="how-to-test-for-differences-between-samples.html"><a href="how-to-test-for-differences-between-samples.html#using-t-test-for-difference-of-the-means-between-two-samples"><i class="fa fa-check"></i><b>3.2.2</b> Using t-test for difference of the means between two samples</a></li>
<li class="chapter" data-level="3.2.3" data-path="how-to-test-for-differences-between-samples.html"><a href="how-to-test-for-differences-between-samples.html#multiple-testing-correction"><i class="fa fa-check"></i><b>3.2.3</b> Multiple testing correction</a></li>
<li class="chapter" data-level="3.2.4" data-path="how-to-test-for-differences-between-samples.html"><a href="how-to-test-for-differences-between-samples.html#moderated-t-tests-using-information-from-multiple-comparisons"><i class="fa fa-check"></i><b>3.2.4</b> Moderated t-tests: Using information from multiple comparisons</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="relationship-between-variables-linear-models-and-correlation.html"><a href="relationship-between-variables-linear-models-and-correlation.html"><i class="fa fa-check"></i><b>3.3</b> Relationship between variables: Linear models and correlation</a><ul>
<li class="chapter" data-level="3.3.1" data-path="relationship-between-variables-linear-models-and-correlation.html"><a href="relationship-between-variables-linear-models-and-correlation.html#how-to-fit-a-line"><i class="fa fa-check"></i><b>3.3.1</b> How to fit a line</a></li>
<li class="chapter" data-level="3.3.2" data-path="relationship-between-variables-linear-models-and-correlation.html"><a href="relationship-between-variables-linear-models-and-correlation.html#how-to-estimate-the-error-of-the-coefficients"><i class="fa fa-check"></i><b>3.3.2</b> How to estimate the error of the coefficients</a></li>
<li class="chapter" data-level="3.3.3" data-path="relationship-between-variables-linear-models-and-correlation.html"><a href="relationship-between-variables-linear-models-and-correlation.html#accuracy-of-the-model"><i class="fa fa-check"></i><b>3.3.3</b> Accuracy of the model</a></li>
<li class="chapter" data-level="3.3.4" data-path="relationship-between-variables-linear-models-and-correlation.html"><a href="relationship-between-variables-linear-models-and-correlation.html#regression-with-categorical-variables"><i class="fa fa-check"></i><b>3.3.4</b> Regression with categorical variables</a></li>
<li class="chapter" data-level="3.3.5" data-path="relationship-between-variables-linear-models-and-correlation.html"><a href="relationship-between-variables-linear-models-and-correlation.html#regression-pitfalls"><i class="fa fa-check"></i><b>3.3.5</b> Regression pitfalls</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>3.4</b> Exercises</a><ul>
<li class="chapter" data-level="3.4.1" data-path="exercises-1.html"><a href="exercises-1.html#how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions-1"><i class="fa fa-check"></i><b>3.4.1</b> How to summarize collection of data points: The idea behind statistical distributions</a></li>
<li class="chapter" data-level="3.4.2" data-path="exercises-1.html"><a href="exercises-1.html#how-to-test-for-differences-in-samples"><i class="fa fa-check"></i><b>3.4.2</b> How to test for differences in samples</a></li>
<li class="chapter" data-level="3.4.3" data-path="exercises-1.html"><a href="exercises-1.html#relationship-between-variables-linear-models-and-correlation-1"><i class="fa fa-check"></i><b>3.4.3</b> Relationship between variables: Linear models and correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="unsupervisedLearning.html"><a href="unsupervisedLearning.html"><i class="fa fa-check"></i><b>4</b> Exploratory Data Analysis with Unsupervised Machine Learning</a><ul>
<li class="chapter" data-level="4.1" data-path="clustering-grouping-samples-based-on-their-similarity.html"><a href="clustering-grouping-samples-based-on-their-similarity.html"><i class="fa fa-check"></i><b>4.1</b> Clustering: Grouping samples based on their similarity</a><ul>
<li class="chapter" data-level="4.1.1" data-path="clustering-grouping-samples-based-on-their-similarity.html"><a href="clustering-grouping-samples-based-on-their-similarity.html#distance-metrics"><i class="fa fa-check"></i><b>4.1.1</b> Distance metrics</a></li>
<li class="chapter" data-level="4.1.2" data-path="clustering-grouping-samples-based-on-their-similarity.html"><a href="clustering-grouping-samples-based-on-their-similarity.html#hiearchical-clustering"><i class="fa fa-check"></i><b>4.1.2</b> Hiearchical clustering</a></li>
<li class="chapter" data-level="4.1.3" data-path="clustering-grouping-samples-based-on-their-similarity.html"><a href="clustering-grouping-samples-based-on-their-similarity.html#k-means-clustering"><i class="fa fa-check"></i><b>4.1.3</b> K-means clustering</a></li>
<li class="chapter" data-level="4.1.4" data-path="clustering-grouping-samples-based-on-their-similarity.html"><a href="clustering-grouping-samples-based-on-their-similarity.html#how-to-choose-k-the-number-of-clusters"><i class="fa fa-check"></i><b>4.1.4</b> How to choose “k”, the number of clusters</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html"><i class="fa fa-check"></i><b>4.2</b> Dimensionality reduction techniques: Visualizing complex data sets in 2D</a><ul>
<li class="chapter" data-level="4.2.1" data-path="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#principal-component-analysis"><i class="fa fa-check"></i><b>4.2.1</b> Principal component analysis</a></li>
<li class="chapter" data-level="4.2.2" data-path="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#other-matrix-factorization-methods-for-dimensionality-reduction"><i class="fa fa-check"></i><b>4.2.2</b> Other matrix factorization methods for dimensionality reduction</a></li>
<li class="chapter" data-level="4.2.3" data-path="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#multi-dimensional-scaling"><i class="fa fa-check"></i><b>4.2.3</b> Multi-dimensional scaling</a></li>
<li class="chapter" data-level="4.2.4" data-path="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#t-distributed-stochastic-neighbor-embedding-t-sne"><i class="fa fa-check"></i><b>4.2.4</b> t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.3</b> Exercises</a><ul>
<li class="chapter" data-level="4.3.1" data-path="exercises-2.html"><a href="exercises-2.html#clustering"><i class="fa fa-check"></i><b>4.3.1</b> Clustering</a></li>
<li class="chapter" data-level="4.3.2" data-path="exercises-2.html"><a href="exercises-2.html#dimension-reduction"><i class="fa fa-check"></i><b>4.3.2</b> Dimension reduction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="supervisedLearning.html"><a href="supervisedLearning.html"><i class="fa fa-check"></i><b>5</b> Predictive Modeling with Supervised Machine Learning</a><ul>
<li class="chapter" data-level="5.1" data-path="how-are-machine-learning-models-fit.html"><a href="how-are-machine-learning-models-fit.html"><i class="fa fa-check"></i><b>5.1</b> How are machine learning models fit?</a><ul>
<li class="chapter" data-level="5.1.1" data-path="how-are-machine-learning-models-fit.html"><a href="how-are-machine-learning-models-fit.html#machine-learning-vs.-statistics"><i class="fa fa-check"></i><b>5.1.1</b> Machine learning vs. statistics</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="steps-in-supervised-machine-learning.html"><a href="steps-in-supervised-machine-learning.html"><i class="fa fa-check"></i><b>5.2</b> Steps in supervised machine learning</a></li>
<li class="chapter" data-level="5.3" data-path="use-case-disease-subtype-from-genomics-data.html"><a href="use-case-disease-subtype-from-genomics-data.html"><i class="fa fa-check"></i><b>5.3</b> Use case: Disease subtype from genomics data</a></li>
<li class="chapter" data-level="5.4" data-path="data-preprocessing.html"><a href="data-preprocessing.html"><i class="fa fa-check"></i><b>5.4</b> Data preprocessing</a><ul>
<li class="chapter" data-level="5.4.1" data-path="data-preprocessing.html"><a href="data-preprocessing.html#data-transformation"><i class="fa fa-check"></i><b>5.4.1</b> Data transformation</a></li>
<li class="chapter" data-level="5.4.2" data-path="data-preprocessing.html"><a href="data-preprocessing.html#filtering-data-and-scaling"><i class="fa fa-check"></i><b>5.4.2</b> Filtering data and scaling</a></li>
<li class="chapter" data-level="5.4.3" data-path="data-preprocessing.html"><a href="data-preprocessing.html#dealing-with-missing-values"><i class="fa fa-check"></i><b>5.4.3</b> Dealing with missing values</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="splitting-the-data.html"><a href="splitting-the-data.html"><i class="fa fa-check"></i><b>5.5</b> Splitting the data</a><ul>
<li class="chapter" data-level="5.5.1" data-path="splitting-the-data.html"><a href="splitting-the-data.html#holdout-test-dataset"><i class="fa fa-check"></i><b>5.5.1</b> Holdout test dataset</a></li>
<li class="chapter" data-level="5.5.2" data-path="splitting-the-data.html"><a href="splitting-the-data.html#cross-validation"><i class="fa fa-check"></i><b>5.5.2</b> Cross-validation</a></li>
<li class="chapter" data-level="5.5.3" data-path="splitting-the-data.html"><a href="splitting-the-data.html#bootstrap-resampling"><i class="fa fa-check"></i><b>5.5.3</b> Bootstrap resampling</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="predicting-the-subtype-with-k-nearest-neighbors.html"><a href="predicting-the-subtype-with-k-nearest-neighbors.html"><i class="fa fa-check"></i><b>5.6</b> Predicting the subtype with k-nearest neighbors</a></li>
<li class="chapter" data-level="5.7" data-path="assessing-the-performance-of-our-model.html"><a href="assessing-the-performance-of-our-model.html"><i class="fa fa-check"></i><b>5.7</b> Assessing the performance of our model</a><ul>
<li class="chapter" data-level="5.7.1" data-path="assessing-the-performance-of-our-model.html"><a href="assessing-the-performance-of-our-model.html#receiver-operating-characteristic-roc-curves"><i class="fa fa-check"></i><b>5.7.1</b> Receiver Operating Characteristic (ROC) curves</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="model-tuning-and-avoiding-overfitting.html"><a href="model-tuning-and-avoiding-overfitting.html"><i class="fa fa-check"></i><b>5.8</b> Model tuning and avoiding overfitting</a><ul>
<li class="chapter" data-level="5.8.1" data-path="model-tuning-and-avoiding-overfitting.html"><a href="model-tuning-and-avoiding-overfitting.html#model-complexity-and-bias-variance-trade-off"><i class="fa fa-check"></i><b>5.8.1</b> Model complexity and bias variance trade-off</a></li>
<li class="chapter" data-level="5.8.2" data-path="model-tuning-and-avoiding-overfitting.html"><a href="model-tuning-and-avoiding-overfitting.html#data-split-strategies-for-model-tuning-and-testing"><i class="fa fa-check"></i><b>5.8.2</b> Data split strategies for model tuning and testing</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="variable-importance.html"><a href="variable-importance.html"><i class="fa fa-check"></i><b>5.9</b> Variable importance</a></li>
<li class="chapter" data-level="5.10" data-path="how-to-deal-with-class-imbalance.html"><a href="how-to-deal-with-class-imbalance.html"><i class="fa fa-check"></i><b>5.10</b> How to deal with class imbalance</a><ul>
<li class="chapter" data-level="5.10.1" data-path="how-to-deal-with-class-imbalance.html"><a href="how-to-deal-with-class-imbalance.html#sampling-for-class-balance"><i class="fa fa-check"></i><b>5.10.1</b> Sampling for class balance</a></li>
<li class="chapter" data-level="5.10.2" data-path="how-to-deal-with-class-imbalance.html"><a href="how-to-deal-with-class-imbalance.html#altering-case-weights"><i class="fa fa-check"></i><b>5.10.2</b> Altering case weights</a></li>
<li class="chapter" data-level="5.10.3" data-path="how-to-deal-with-class-imbalance.html"><a href="how-to-deal-with-class-imbalance.html#selecting-different-classification-score-cutoffs"><i class="fa fa-check"></i><b>5.10.3</b> Selecting different classification score cutoffs</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="dealing-with-correlated-predictors.html"><a href="dealing-with-correlated-predictors.html"><i class="fa fa-check"></i><b>5.11</b> Dealing with correlated predictors</a></li>
<li class="chapter" data-level="5.12" data-path="trees-and-forests-random-forests-in-action.html"><a href="trees-and-forests-random-forests-in-action.html"><i class="fa fa-check"></i><b>5.12</b> Trees and forests: Random forests in action</a><ul>
<li class="chapter" data-level="5.12.1" data-path="trees-and-forests-random-forests-in-action.html"><a href="trees-and-forests-random-forests-in-action.html#decision-trees"><i class="fa fa-check"></i><b>5.12.1</b> Decision trees</a></li>
<li class="chapter" data-level="5.12.2" data-path="trees-and-forests-random-forests-in-action.html"><a href="trees-and-forests-random-forests-in-action.html#trees-to-forests"><i class="fa fa-check"></i><b>5.12.2</b> Trees to forests</a></li>
<li class="chapter" data-level="5.12.3" data-path="trees-and-forests-random-forests-in-action.html"><a href="trees-and-forests-random-forests-in-action.html#variable-importance-1"><i class="fa fa-check"></i><b>5.12.3</b> Variable importance</a></li>
</ul></li>
<li class="chapter" data-level="5.13" data-path="logistic-regression-and-regularization.html"><a href="logistic-regression-and-regularization.html"><i class="fa fa-check"></i><b>5.13</b> Logistic regression and regularization</a><ul>
<li class="chapter" data-level="5.13.1" data-path="logistic-regression-and-regularization.html"><a href="logistic-regression-and-regularization.html#regularization-in-order-to-avoid-overfitting"><i class="fa fa-check"></i><b>5.13.1</b> Regularization in order to avoid overfitting</a></li>
<li class="chapter" data-level="5.13.2" data-path="logistic-regression-and-regularization.html"><a href="logistic-regression-and-regularization.html#variable-importance-2"><i class="fa fa-check"></i><b>5.13.2</b> Variable importance</a></li>
</ul></li>
<li class="chapter" data-level="5.14" data-path="other-supervised-algorithms.html"><a href="other-supervised-algorithms.html"><i class="fa fa-check"></i><b>5.14</b> Other supervised algorithms</a><ul>
<li class="chapter" data-level="5.14.1" data-path="other-supervised-algorithms.html"><a href="other-supervised-algorithms.html#gradient-boosting"><i class="fa fa-check"></i><b>5.14.1</b> Gradient boosting</a></li>
<li class="chapter" data-level="5.14.2" data-path="other-supervised-algorithms.html"><a href="other-supervised-algorithms.html#support-vector-machines-svm"><i class="fa fa-check"></i><b>5.14.2</b> Support Vector Machines (SVM)</a></li>
<li class="chapter" data-level="5.14.3" data-path="other-supervised-algorithms.html"><a href="other-supervised-algorithms.html#neural-networks-and-deep-versions-of-it"><i class="fa fa-check"></i><b>5.14.3</b> Neural networks and deep versions of it</a></li>
<li class="chapter" data-level="5.14.4" data-path="other-supervised-algorithms.html"><a href="other-supervised-algorithms.html#ensemble-learning"><i class="fa fa-check"></i><b>5.14.4</b> Ensemble learning</a></li>
</ul></li>
<li class="chapter" data-level="5.15" data-path="predicting-continuous-variables-regression-with-machine-learning.html"><a href="predicting-continuous-variables-regression-with-machine-learning.html"><i class="fa fa-check"></i><b>5.15</b> Predicting continuous variables: Regression with machine learning</a><ul>
<li class="chapter" data-level="5.15.1" data-path="predicting-continuous-variables-regression-with-machine-learning.html"><a href="predicting-continuous-variables-regression-with-machine-learning.html#use-case-predicting-age-from-dna-methylation"><i class="fa fa-check"></i><b>5.15.1</b> Use case: Predicting age from DNA methylation</a></li>
<li class="chapter" data-level="5.15.2" data-path="predicting-continuous-variables-regression-with-machine-learning.html"><a href="predicting-continuous-variables-regression-with-machine-learning.html#reading-and-processing-the-data"><i class="fa fa-check"></i><b>5.15.2</b> Reading and processing the data</a></li>
<li class="chapter" data-level="5.15.3" data-path="predicting-continuous-variables-regression-with-machine-learning.html"><a href="predicting-continuous-variables-regression-with-machine-learning.html#running-random-forest-regression"><i class="fa fa-check"></i><b>5.15.3</b> Running random forest regression</a></li>
</ul></li>
<li class="chapter" data-level="5.16" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>5.16</b> Exercises</a><ul>
<li class="chapter" data-level="5.16.1" data-path="exercises-3.html"><a href="exercises-3.html#classification"><i class="fa fa-check"></i><b>5.16.1</b> Classification</a></li>
<li class="chapter" data-level="5.16.2" data-path="exercises-3.html"><a href="exercises-3.html#regression"><i class="fa fa-check"></i><b>5.16.2</b> Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="genomicIntervals.html"><a href="genomicIntervals.html"><i class="fa fa-check"></i><b>6</b> Operations on Genomic Intervals and Genome Arithmetic</a><ul>
<li class="chapter" data-level="6.1" data-path="operations-on-genomic-intervals-with-genomicranges-package.html"><a href="operations-on-genomic-intervals-with-genomicranges-package.html"><i class="fa fa-check"></i><b>6.1</b> Operations on genomic intervals with <code>GenomicRanges</code> package</a><ul>
<li class="chapter" data-level="6.1.1" data-path="operations-on-genomic-intervals-with-genomicranges-package.html"><a href="operations-on-genomic-intervals-with-genomicranges-package.html#how-to-create-and-manipulate-a-granges-object"><i class="fa fa-check"></i><b>6.1.1</b> How to create and manipulate a GRanges object</a></li>
<li class="chapter" data-level="6.1.2" data-path="operations-on-genomic-intervals-with-genomicranges-package.html"><a href="operations-on-genomic-intervals-with-genomicranges-package.html#getting-genomic-regions-into-r-as-granges-objects"><i class="fa fa-check"></i><b>6.1.2</b> Getting genomic regions into R as GRanges objects</a></li>
<li class="chapter" data-level="6.1.3" data-path="operations-on-genomic-intervals-with-genomicranges-package.html"><a href="operations-on-genomic-intervals-with-genomicranges-package.html#finding-regions-that-dodo-not-overlap-with-another-set-of-regions"><i class="fa fa-check"></i><b>6.1.3</b> Finding regions that do/do not overlap with another set of regions</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="dealing-with-mapped-high-throughput-sequencing-reads.html"><a href="dealing-with-mapped-high-throughput-sequencing-reads.html"><i class="fa fa-check"></i><b>6.2</b> Dealing with mapped high-throughput sequencing reads</a><ul>
<li class="chapter" data-level="6.2.1" data-path="dealing-with-mapped-high-throughput-sequencing-reads.html"><a href="dealing-with-mapped-high-throughput-sequencing-reads.html#counting-mapped-reads-for-a-set-of-regions"><i class="fa fa-check"></i><b>6.2.1</b> Counting mapped reads for a set of regions</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="dealing-with-continuous-scores-over-the-genome.html"><a href="dealing-with-continuous-scores-over-the-genome.html"><i class="fa fa-check"></i><b>6.3</b> Dealing with continuous scores over the genome</a><ul>
<li class="chapter" data-level="6.3.1" data-path="dealing-with-continuous-scores-over-the-genome.html"><a href="dealing-with-continuous-scores-over-the-genome.html#extracting-subsections-of-rle-and-rlelist-objects"><i class="fa fa-check"></i><b>6.3.1</b> Extracting subsections of Rle and RleList objects</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="genomic-intervals-with-more-information-summarizedexperiment-class.html"><a href="genomic-intervals-with-more-information-summarizedexperiment-class.html"><i class="fa fa-check"></i><b>6.4</b> Genomic intervals with more information: SummarizedExperiment class</a><ul>
<li class="chapter" data-level="6.4.1" data-path="genomic-intervals-with-more-information-summarizedexperiment-class.html"><a href="genomic-intervals-with-more-information-summarizedexperiment-class.html#create-a-summarizedexperiment-object"><i class="fa fa-check"></i><b>6.4.1</b> Create a SummarizedExperiment object</a></li>
<li class="chapter" data-level="6.4.2" data-path="genomic-intervals-with-more-information-summarizedexperiment-class.html"><a href="genomic-intervals-with-more-information-summarizedexperiment-class.html#subset-and-manipulate-the-summarizedexperiment-object"><i class="fa fa-check"></i><b>6.4.2</b> Subset and manipulate the SummarizedExperiment object</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="visualizing-and-summarizing-genomic-intervals.html"><a href="visualizing-and-summarizing-genomic-intervals.html"><i class="fa fa-check"></i><b>6.5</b> Visualizing and summarizing genomic intervals</a><ul>
<li class="chapter" data-level="6.5.1" data-path="visualizing-and-summarizing-genomic-intervals.html"><a href="visualizing-and-summarizing-genomic-intervals.html#visualizing-intervals-on-a-locus-of-interest"><i class="fa fa-check"></i><b>6.5.1</b> Visualizing intervals on a locus of interest</a></li>
<li class="chapter" data-level="6.5.2" data-path="visualizing-and-summarizing-genomic-intervals.html"><a href="visualizing-and-summarizing-genomic-intervals.html#summaries-of-genomic-intervals-on-multiple-loci"><i class="fa fa-check"></i><b>6.5.2</b> Summaries of genomic intervals on multiple loci</a></li>
<li class="chapter" data-level="6.5.3" data-path="visualizing-and-summarizing-genomic-intervals.html"><a href="visualizing-and-summarizing-genomic-intervals.html#making-karyograms-and-circos-plots"><i class="fa fa-check"></i><b>6.5.3</b> Making karyograms and circos plots</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>6.6</b> Exercises</a><ul>
<li class="chapter" data-level="6.6.1" data-path="exercises-4.html"><a href="exercises-4.html#operations-on-genomic-intervals-with-the-genomicranges-package"><i class="fa fa-check"></i><b>6.6.1</b> Operations on genomic intervals with the <code>GenomicRanges</code> package</a></li>
<li class="chapter" data-level="6.6.2" data-path="exercises-4.html"><a href="exercises-4.html#dealing-with-mapped-high-throughput-sequencing-reads-1"><i class="fa fa-check"></i><b>6.6.2</b> Dealing with mapped high-throughput sequencing reads</a></li>
<li class="chapter" data-level="6.6.3" data-path="exercises-4.html"><a href="exercises-4.html#dealing-with-contiguous-scores-over-the-genome"><i class="fa fa-check"></i><b>6.6.3</b> Dealing with contiguous scores over the genome</a></li>
<li class="chapter" data-level="6.6.4" data-path="exercises-4.html"><a href="exercises-4.html#visualizing-and-summarizing-genomic-intervals-1"><i class="fa fa-check"></i><b>6.6.4</b> Visualizing and summarizing genomic intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="processingReads.html"><a href="processingReads.html"><i class="fa fa-check"></i><b>7</b> Quality Check, Processing and Alignment of High-throughput Sequencing Reads</a><ul>
<li class="chapter" data-level="7.1" data-path="fasta-and-fastq-formats.html"><a href="fasta-and-fastq-formats.html"><i class="fa fa-check"></i><b>7.1</b> FASTA and FASTQ formats</a></li>
<li class="chapter" data-level="7.2" data-path="quality-check-on-sequencing-reads.html"><a href="quality-check-on-sequencing-reads.html"><i class="fa fa-check"></i><b>7.2</b> Quality check on sequencing reads</a><ul>
<li class="chapter" data-level="7.2.1" data-path="quality-check-on-sequencing-reads.html"><a href="quality-check-on-sequencing-reads.html#sequence-quality-per-basecycle"><i class="fa fa-check"></i><b>7.2.1</b> Sequence quality per base/cycle</a></li>
<li class="chapter" data-level="7.2.2" data-path="quality-check-on-sequencing-reads.html"><a href="quality-check-on-sequencing-reads.html#sequence-content-per-basecycle"><i class="fa fa-check"></i><b>7.2.2</b> Sequence content per base/cycle</a></li>
<li class="chapter" data-level="7.2.3" data-path="quality-check-on-sequencing-reads.html"><a href="quality-check-on-sequencing-reads.html#read-frequency-plot"><i class="fa fa-check"></i><b>7.2.3</b> Read frequency plot</a></li>
<li class="chapter" data-level="7.2.4" data-path="quality-check-on-sequencing-reads.html"><a href="quality-check-on-sequencing-reads.html#other-quality-metrics-and-qc-tools"><i class="fa fa-check"></i><b>7.2.4</b> Other quality metrics and QC tools</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="filtering-and-trimming-reads.html"><a href="filtering-and-trimming-reads.html"><i class="fa fa-check"></i><b>7.3</b> Filtering and trimming reads</a></li>
<li class="chapter" data-level="7.4" data-path="mappingaligning-reads-to-the-genome.html"><a href="mappingaligning-reads-to-the-genome.html"><i class="fa fa-check"></i><b>7.4</b> Mapping/aligning reads to the genome</a></li>
<li class="chapter" data-level="7.5" data-path="further-processing-of-aligned-reads.html"><a href="further-processing-of-aligned-reads.html"><i class="fa fa-check"></i><b>7.5</b> Further processing of aligned reads</a></li>
<li class="chapter" data-level="7.6" data-path="exercises-5.html"><a href="exercises-5.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="rnaseqanalysis.html"><a href="rnaseqanalysis.html"><i class="fa fa-check"></i><b>8</b> RNA-seq Analysis</a><ul>
<li class="chapter" data-level="8.1" data-path="what-is-gene-expression.html"><a href="what-is-gene-expression.html"><i class="fa fa-check"></i><b>8.1</b> What is gene expression?</a></li>
<li class="chapter" data-level="8.2" data-path="methods-to-detect-gene-expression.html"><a href="methods-to-detect-gene-expression.html"><i class="fa fa-check"></i><b>8.2</b> Methods to detect gene expression</a></li>
<li class="chapter" data-level="8.3" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><i class="fa fa-check"></i><b>8.3</b> Gene expression analysis using high-throughput sequencing technologies</a><ul>
<li class="chapter" data-level="8.3.1" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#processing-raw-data"><i class="fa fa-check"></i><b>8.3.1</b> Processing raw data</a></li>
<li class="chapter" data-level="8.3.2" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#alignment"><i class="fa fa-check"></i><b>8.3.2</b> Alignment</a></li>
<li class="chapter" data-level="8.3.3" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#quantification"><i class="fa fa-check"></i><b>8.3.3</b> Quantification</a></li>
<li class="chapter" data-level="8.3.4" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#within-sample-normalization-of-the-read-counts"><i class="fa fa-check"></i><b>8.3.4</b> Within sample normalization of the read counts</a></li>
<li class="chapter" data-level="8.3.5" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#computing-different-normalization-schemes-in-r"><i class="fa fa-check"></i><b>8.3.5</b> Computing different normalization schemes in R</a></li>
<li class="chapter" data-level="8.3.6" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#exploratory-analysis-of-the-read-count-table"><i class="fa fa-check"></i><b>8.3.6</b> Exploratory analysis of the read count table</a></li>
<li class="chapter" data-level="8.3.7" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#differential-expression-analysis"><i class="fa fa-check"></i><b>8.3.7</b> Differential expression analysis</a></li>
<li class="chapter" data-level="8.3.8" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#functional-enrichment-analysis"><i class="fa fa-check"></i><b>8.3.8</b> Functional enrichment analysis</a></li>
<li class="chapter" data-level="8.3.9" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#accounting-for-additional-sources-of-variation"><i class="fa fa-check"></i><b>8.3.9</b> Accounting for additional sources of variation</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="other-applications-of-rna-seq.html"><a href="other-applications-of-rna-seq.html"><i class="fa fa-check"></i><b>8.4</b> Other applications of RNA-seq</a></li>
<li class="chapter" data-level="8.5" data-path="exercises-6.html"><a href="exercises-6.html"><i class="fa fa-check"></i><b>8.5</b> Exercises</a><ul>
<li class="chapter" data-level="8.5.1" data-path="exercises-6.html"><a href="exercises-6.html#exploring-the-count-tables"><i class="fa fa-check"></i><b>8.5.1</b> Exploring the count tables</a></li>
<li class="chapter" data-level="8.5.2" data-path="exercises-6.html"><a href="exercises-6.html#differential-expression-analysis-1"><i class="fa fa-check"></i><b>8.5.2</b> Differential expression analysis</a></li>
<li class="chapter" data-level="8.5.3" data-path="exercises-6.html"><a href="exercises-6.html#functional-enrichment-analysis-1"><i class="fa fa-check"></i><b>8.5.3</b> Functional enrichment analysis</a></li>
<li class="chapter" data-level="8.5.4" data-path="exercises-6.html"><a href="exercises-6.html#removing-unwanted-variation-from-the-expression-data"><i class="fa fa-check"></i><b>8.5.4</b> Removing unwanted variation from the expression data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chipseq.html"><a href="chipseq.html"><i class="fa fa-check"></i><b>9</b> ChIP-seq analysis</a><ul>
<li class="chapter" data-level="9.1" data-path="regulatory-protein-dna-interactions.html"><a href="regulatory-protein-dna-interactions.html"><i class="fa fa-check"></i><b>9.1</b> Regulatory protein-DNA interactions</a></li>
<li class="chapter" data-level="9.2" data-path="measuring-protein-dna-interactions-with-chip-seq.html"><a href="measuring-protein-dna-interactions-with-chip-seq.html"><i class="fa fa-check"></i><b>9.2</b> Measuring protein-DNA interactions with ChIP-seq</a></li>
<li class="chapter" data-level="9.3" data-path="factors-that-affect-chip-seq-experiment-and-analysis-quality.html"><a href="factors-that-affect-chip-seq-experiment-and-analysis-quality.html"><i class="fa fa-check"></i><b>9.3</b> Factors that affect ChIP-seq experiment and analysis quality</a><ul>
<li class="chapter" data-level="9.3.1" data-path="factors-that-affect-chip-seq-experiment-and-analysis-quality.html"><a href="factors-that-affect-chip-seq-experiment-and-analysis-quality.html#antibody-specificity"><i class="fa fa-check"></i><b>9.3.1</b> Antibody specificity</a></li>
<li class="chapter" data-level="9.3.2" data-path="factors-that-affect-chip-seq-experiment-and-analysis-quality.html"><a href="factors-that-affect-chip-seq-experiment-and-analysis-quality.html#sequencing-depth"><i class="fa fa-check"></i><b>9.3.2</b> Sequencing depth</a></li>
<li class="chapter" data-level="9.3.3" data-path="factors-that-affect-chip-seq-experiment-and-analysis-quality.html"><a href="factors-that-affect-chip-seq-experiment-and-analysis-quality.html#pcr-duplication"><i class="fa fa-check"></i><b>9.3.3</b> PCR duplication</a></li>
<li class="chapter" data-level="9.3.4" data-path="factors-that-affect-chip-seq-experiment-and-analysis-quality.html"><a href="factors-that-affect-chip-seq-experiment-and-analysis-quality.html#biological-replicates"><i class="fa fa-check"></i><b>9.3.4</b> Biological replicates</a></li>
<li class="chapter" data-level="9.3.5" data-path="factors-that-affect-chip-seq-experiment-and-analysis-quality.html"><a href="factors-that-affect-chip-seq-experiment-and-analysis-quality.html#control-experiments"><i class="fa fa-check"></i><b>9.3.5</b> Control experiments</a></li>
<li class="chapter" data-level="9.3.6" data-path="factors-that-affect-chip-seq-experiment-and-analysis-quality.html"><a href="factors-that-affect-chip-seq-experiment-and-analysis-quality.html#using-tagged-proteins"><i class="fa fa-check"></i><b>9.3.6</b> Using tagged proteins</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="pre-processing-chip-data.html"><a href="pre-processing-chip-data.html"><i class="fa fa-check"></i><b>9.4</b> Pre-processing ChIP data</a><ul>
<li class="chapter" data-level="9.4.1" data-path="pre-processing-chip-data.html"><a href="pre-processing-chip-data.html#mapping-of-chip-seq-data"><i class="fa fa-check"></i><b>9.4.1</b> Mapping of ChIP-seq data</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="chip-quality-control.html"><a href="chip-quality-control.html"><i class="fa fa-check"></i><b>9.5</b> ChIP quality control</a><ul>
<li class="chapter" data-level="9.5.1" data-path="chip-quality-control.html"><a href="chip-quality-control.html#the-data"><i class="fa fa-check"></i><b>9.5.1</b> The data</a></li>
<li class="chapter" data-level="9.5.2" data-path="chip-quality-control.html"><a href="chip-quality-control.html#sample-clustering"><i class="fa fa-check"></i><b>9.5.2</b> Sample clustering</a></li>
<li class="chapter" data-level="9.5.3" data-path="chip-quality-control.html"><a href="chip-quality-control.html#visualization-in-the-genome-browser"><i class="fa fa-check"></i><b>9.5.3</b> Visualization in the genome browser</a></li>
<li class="chapter" data-level="9.5.4" data-path="chip-quality-control.html"><a href="chip-quality-control.html#plus-and-minus-strand-cross-correlation"><i class="fa fa-check"></i><b>9.5.4</b> Plus and minus strand cross-correlation</a></li>
<li class="chapter" data-level="9.5.5" data-path="chip-quality-control.html"><a href="chip-quality-control.html#gc-bias-quantification"><i class="fa fa-check"></i><b>9.5.5</b> GC bias quantification</a></li>
<li class="chapter" data-level="9.5.6" data-path="chip-quality-control.html"><a href="chip-quality-control.html#sequence-read-genomic-distribution"><i class="fa fa-check"></i><b>9.5.6</b> Sequence read genomic distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="peak-calling.html"><a href="peak-calling.html"><i class="fa fa-check"></i><b>9.6</b> Peak calling</a><ul>
<li class="chapter" data-level="9.6.1" data-path="peak-calling.html"><a href="peak-calling.html#types-of-chip-seq-experiments"><i class="fa fa-check"></i><b>9.6.1</b> Types of ChIP-seq experiments</a></li>
<li class="chapter" data-level="9.6.2" data-path="peak-calling.html"><a href="peak-calling.html#peak-calling-sharp-peaks"><i class="fa fa-check"></i><b>9.6.2</b> Peak calling: Sharp peaks</a></li>
<li class="chapter" data-level="9.6.3" data-path="peak-calling.html"><a href="peak-calling.html#peak-calling-broad-regions"><i class="fa fa-check"></i><b>9.6.3</b> Peak calling: Broad regions</a></li>
<li class="chapter" data-level="9.6.4" data-path="peak-calling.html"><a href="peak-calling.html#peak-quality-control"><i class="fa fa-check"></i><b>9.6.4</b> Peak quality control</a></li>
<li class="chapter" data-level="9.6.5" data-path="peak-calling.html"><a href="peak-calling.html#peak-annotation"><i class="fa fa-check"></i><b>9.6.5</b> Peak annotation</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="motif-discovery.html"><a href="motif-discovery.html"><i class="fa fa-check"></i><b>9.7</b> Motif discovery</a><ul>
<li class="chapter" data-level="9.7.1" data-path="motif-discovery.html"><a href="motif-discovery.html#motif-comparison"><i class="fa fa-check"></i><b>9.7.1</b> Motif comparison</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="what-to-do-next.html"><a href="what-to-do-next.html"><i class="fa fa-check"></i><b>9.8</b> What to do next?</a></li>
<li class="chapter" data-level="9.9" data-path="exercises-7.html"><a href="exercises-7.html"><i class="fa fa-check"></i><b>9.9</b> Exercises</a><ul>
<li class="chapter" data-level="9.9.1" data-path="exercises-7.html"><a href="exercises-7.html#quality-control"><i class="fa fa-check"></i><b>9.9.1</b> Quality control</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bsseq.html"><a href="bsseq.html"><i class="fa fa-check"></i><b>10</b> DNA methylation analysis using bisulfite sequencing data</a><ul>
<li class="chapter" data-level="10.1" data-path="what-is-dna-methylation.html"><a href="what-is-dna-methylation.html"><i class="fa fa-check"></i><b>10.1</b> What is DNA methylation ?</a><ul>
<li class="chapter" data-level="10.1.1" data-path="what-is-dna-methylation.html"><a href="what-is-dna-methylation.html#how-dna-methylation-is-set"><i class="fa fa-check"></i><b>10.1.1</b> How DNA methylation is set ?</a></li>
<li class="chapter" data-level="10.1.2" data-path="what-is-dna-methylation.html"><a href="what-is-dna-methylation.html#how-to-measure-dna-methylation-with-bisulfite-sequencing"><i class="fa fa-check"></i><b>10.1.2</b> How to measure DNA methylation with bisulfite sequencing</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="analyzing-dna-methylation-data.html"><a href="analyzing-dna-methylation-data.html"><i class="fa fa-check"></i><b>10.2</b> Analyzing DNA methylation data</a></li>
<li class="chapter" data-level="10.3" data-path="processing-raw-data-and-getting-data-into-r.html"><a href="processing-raw-data-and-getting-data-into-r.html"><i class="fa fa-check"></i><b>10.3</b> Processing raw data and getting data into R</a></li>
<li class="chapter" data-level="10.4" data-path="data-filtering-and-exploratory-analysis.html"><a href="data-filtering-and-exploratory-analysis.html"><i class="fa fa-check"></i><b>10.4</b> Data filtering and exploratory analysis</a><ul>
<li class="chapter" data-level="10.4.1" data-path="data-filtering-and-exploratory-analysis.html"><a href="data-filtering-and-exploratory-analysis.html#reading-methylation-call-files"><i class="fa fa-check"></i><b>10.4.1</b> Reading methylation call files</a></li>
<li class="chapter" data-level="10.4.2" data-path="data-filtering-and-exploratory-analysis.html"><a href="data-filtering-and-exploratory-analysis.html#further-quality-check"><i class="fa fa-check"></i><b>10.4.2</b> Further quality check</a></li>
<li class="chapter" data-level="10.4.3" data-path="data-filtering-and-exploratory-analysis.html"><a href="data-filtering-and-exploratory-analysis.html#merging-samples-into-a-single-table"><i class="fa fa-check"></i><b>10.4.3</b> Merging samples into a single table</a></li>
<li class="chapter" data-level="10.4.4" data-path="data-filtering-and-exploratory-analysis.html"><a href="data-filtering-and-exploratory-analysis.html#filtering-cpgs"><i class="fa fa-check"></i><b>10.4.4</b> Filtering CpGs</a></li>
<li class="chapter" data-level="10.4.5" data-path="data-filtering-and-exploratory-analysis.html"><a href="data-filtering-and-exploratory-analysis.html#clustering-samples"><i class="fa fa-check"></i><b>10.4.5</b> Clustering samples</a></li>
<li class="chapter" data-level="10.4.6" data-path="data-filtering-and-exploratory-analysis.html"><a href="data-filtering-and-exploratory-analysis.html#principal-component-analysis-1"><i class="fa fa-check"></i><b>10.4.6</b> Principal component analysis</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="extracting-interesting-regions-differential-methylation-and-segmentation.html"><a href="extracting-interesting-regions-differential-methylation-and-segmentation.html"><i class="fa fa-check"></i><b>10.5</b> Extracting interesting regions: Differential methylation and segmentation</a><ul>
<li class="chapter" data-level="10.5.1" data-path="extracting-interesting-regions-differential-methylation-and-segmentation.html"><a href="extracting-interesting-regions-differential-methylation-and-segmentation.html#differential-methylation"><i class="fa fa-check"></i><b>10.5.1</b> Differential methylation</a></li>
<li class="chapter" data-level="10.5.2" data-path="extracting-interesting-regions-differential-methylation-and-segmentation.html"><a href="extracting-interesting-regions-differential-methylation-and-segmentation.html#methylation-segmentation"><i class="fa fa-check"></i><b>10.5.2</b> Methylation segmentation</a></li>
<li class="chapter" data-level="10.5.3" data-path="extracting-interesting-regions-differential-methylation-and-segmentation.html"><a href="extracting-interesting-regions-differential-methylation-and-segmentation.html#working-with-large-files"><i class="fa fa-check"></i><b>10.5.3</b> Working with large files</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="annotation-of-dmrsdmcs-and-segments.html"><a href="annotation-of-dmrsdmcs-and-segments.html"><i class="fa fa-check"></i><b>10.6</b> Annotation of DMRs/DMCs and segments</a><ul>
<li class="chapter" data-level="10.6.1" data-path="annotation-of-dmrsdmcs-and-segments.html"><a href="annotation-of-dmrsdmcs-and-segments.html#further-annotation-with-genes-or-gene-sets"><i class="fa fa-check"></i><b>10.6.1</b> Further annotation with genes or gene sets</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="other-r-packages-that-can-be-used-for-methylation-analysis.html"><a href="other-r-packages-that-can-be-used-for-methylation-analysis.html"><i class="fa fa-check"></i><b>10.7</b> Other R packages that can be used for methylation analysis</a></li>
<li class="chapter" data-level="10.8" data-path="exercises-8.html"><a href="exercises-8.html"><i class="fa fa-check"></i><b>10.8</b> Exercises</a><ul>
<li class="chapter" data-level="10.8.1" data-path="exercises-8.html"><a href="exercises-8.html#differential-methylation-1"><i class="fa fa-check"></i><b>10.8.1</b> Differential methylation</a></li>
<li class="chapter" data-level="10.8.2" data-path="exercises-8.html"><a href="exercises-8.html#methylome-segmentation"><i class="fa fa-check"></i><b>10.8.2</b> Methylome segmentation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="multiomics.html"><a href="multiomics.html"><i class="fa fa-check"></i><b>11</b> Multi-omics Analysis</a><ul>
<li class="chapter" data-level="11.1" data-path="use-case-multi-omics-data-from-colorectal-cancer.html"><a href="use-case-multi-omics-data-from-colorectal-cancer.html"><i class="fa fa-check"></i><b>11.1</b> Use case: Multi-omics data from colorectal cancer</a></li>
<li class="chapter" data-level="11.2" data-path="latent-variable-models-for-multi-omics-integration.html"><a href="latent-variable-models-for-multi-omics-integration.html"><i class="fa fa-check"></i><b>11.2</b> Latent variable models for multi-omics integration</a></li>
<li class="chapter" data-level="11.3" data-path="matrix-factorization-methods-for-unsupervised-multi-omics-data-integration.html"><a href="matrix-factorization-methods-for-unsupervised-multi-omics-data-integration.html"><i class="fa fa-check"></i><b>11.3</b> Matrix factorization methods for unsupervised multi-omics data integration</a><ul>
<li class="chapter" data-level="11.3.1" data-path="matrix-factorization-methods-for-unsupervised-multi-omics-data-integration.html"><a href="matrix-factorization-methods-for-unsupervised-multi-omics-data-integration.html#multiple-factor-analysis"><i class="fa fa-check"></i><b>11.3.1</b> Multiple factor analysis</a></li>
<li class="chapter" data-level="11.3.2" data-path="matrix-factorization-methods-for-unsupervised-multi-omics-data-integration.html"><a href="matrix-factorization-methods-for-unsupervised-multi-omics-data-integration.html#joint-non-negative-matrix-factorization"><i class="fa fa-check"></i><b>11.3.2</b> Joint non-negative matrix Factorization</a></li>
<li class="chapter" data-level="11.3.3" data-path="matrix-factorization-methods-for-unsupervised-multi-omics-data-integration.html"><a href="matrix-factorization-methods-for-unsupervised-multi-omics-data-integration.html#icluster"><i class="fa fa-check"></i><b>11.3.3</b> iCluster</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="clustering-using-latent-factors.html"><a href="clustering-using-latent-factors.html"><i class="fa fa-check"></i><b>11.4</b> Clustering using latent factors</a><ul>
<li class="chapter" data-level="11.4.1" data-path="clustering-using-latent-factors.html"><a href="clustering-using-latent-factors.html#one-hot-clustering"><i class="fa fa-check"></i><b>11.4.1</b> One-hot clustering</a></li>
<li class="chapter" data-level="11.4.2" data-path="clustering-using-latent-factors.html"><a href="clustering-using-latent-factors.html#k-means-clustering-1"><i class="fa fa-check"></i><b>11.4.2</b> K-means clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="biological-interpretation-of-latent-factors.html"><a href="biological-interpretation-of-latent-factors.html"><i class="fa fa-check"></i><b>11.5</b> Biological interpretation of latent factors</a><ul>
<li class="chapter" data-level="11.5.1" data-path="biological-interpretation-of-latent-factors.html"><a href="biological-interpretation-of-latent-factors.html#inspection-of-feature-weights-in-loading-vectors"><i class="fa fa-check"></i><b>11.5.1</b> Inspection of feature weights in loading vectors</a></li>
<li class="chapter" data-level="11.5.2" data-path="biological-interpretation-of-latent-factors.html"><a href="biological-interpretation-of-latent-factors.html#making-sense-of-factors-using-enrichment-analysis"><i class="fa fa-check"></i><b>11.5.2</b> Making sense of factors using enrichment analysis</a></li>
<li class="chapter" data-level="11.5.3" data-path="biological-interpretation-of-latent-factors.html"><a href="biological-interpretation-of-latent-factors.html#interpretation-using-additional-covariates"><i class="fa fa-check"></i><b>11.5.3</b> Interpretation using additional covariates</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="exercises-9.html"><a href="exercises-9.html"><i class="fa fa-check"></i><b>11.6</b> Exercises</a><ul>
<li class="chapter" data-level="11.6.1" data-path="exercises-9.html"><a href="exercises-9.html#matrix-factorization-methods"><i class="fa fa-check"></i><b>11.6.1</b> Matrix factorization methods</a></li>
<li class="chapter" data-level="11.6.2" data-path="exercises-9.html"><a href="exercises-9.html#clustering-using-latent-factors-1"><i class="fa fa-check"></i><b>11.6.2</b> Clustering using latent factors</a></li>
<li class="chapter" data-level="11.6.3" data-path="exercises-9.html"><a href="exercises-9.html#biological-interpretation-of-latent-factors-1"><i class="fa fa-check"></i><b>11.6.3</b> Biological interpretation of latent factors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Genomics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d" class="section level2">
<h2><span class="header-section-number">4.2</span> Dimensionality reduction techniques: Visualizing complex data sets in 2D</h2>
<p>In statistics, dimension reduction techniques are a set of processes for reducing the number of random variables by obtaining a set of principal variables. For example, in the context of a gene expression matrix across different patient samples, this might mean getting a set of new variables that cover the variation in sets of genes. This way samples can be represented by a couple of principal variables instead of thousands of genes. This is useful for visualization, clustering and predictive modeling.</p>
<div id="principal-component-analysis" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Principal component analysis</h3>
<p>Principal component analysis (PCA) is maybe the most popular technique to examine high-dimensional data. There are multiple interpretations of how PCA reduces dimensionality. We will first focus on geometrical interpretation, where this operation can be interpreted as rotating the original dimensions of the data. For this, we go back to our example gene expression data set. In this example, we will represent our patients with expression profiles of just two genes, CD33 (ENSG00000105383) and PYGL (ENSG00000100504). This way we can visualize them in a scatter plot (See Figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:scatterb4PCA">4.9</a>).</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb219-1"></a><span class="kw">plot</span>(mat[<span class="kw">rownames</span>(mat)<span class="op">==</span><span class="st">&quot;ENSG00000100504&quot;</span>,],</span>
<span id="cb219-2"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb219-2"></a>     mat[<span class="kw">rownames</span>(mat)<span class="op">==</span><span class="st">&quot;ENSG00000105383&quot;</span>,],<span class="dt">pch=</span><span class="dv">19</span>,</span>
<span id="cb219-3"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb219-3"></a>     <span class="dt">ylab=</span><span class="st">&quot;CD33 (ENSG00000105383)&quot;</span>,</span>
<span id="cb219-4"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb219-4"></a>     <span class="dt">xlab=</span><span class="st">&quot;PYGL (ENSG00000100504)&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:scatterb4PCA"></span>
<img src="04-unsupervisedLearning_files/figure-html/scatterb4PCA-1.png" alt="Gene expression values of CD33 and PYGL genes across leukemia patients." width="60%" />
<p class="caption">
FIGURE 4.9: Gene expression values of CD33 and PYGL genes across leukemia patients.
</p>
</div>
<p>PCA rotates the original data space such that the axes of the new coordinate system point to the directions of highest variance of the data. The axes or new variables are termed principal components (PCs) and are ordered by variance: The first component, PC 1, represents the direction of the highest variance of the data. The direction of the second component, PC 2, represents the highest of the remaining variance orthogonal to the first component. This can be naturally extended to obtain the required number of components, which together span a component space covering the desired amount of variance. In our toy example with only two genes, the principal components are drawn over the original scatter plot and in the next plot we show the new coordinate system based on the principal components. We will calculate the PCA with the <code>princomp()</code> function; this function returns the new coordinates as well. These new coordinates are simply a projection of data over the new coordinates. We will decorate the scatter plots with eigenvectors showing the direction of greatest variation. Then, we will plot the new coordinates (the resulting plot is shown in Figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:pcaRot">4.10</a>). These are automatically calculated by the <code>princomp()</code> function. Notice that we are using the <code>scale()</code> function when plotting coordinates and also before calculating the PCA. This function centers the data, meaning it subtracts the mean of each column vector from the elements in the vector. This essentially gives the columns a zero mean. It also divides the data by the standard deviation of the centered columns. These two operations help bring the data to a common scale, which is important for PCA not to be affected by different scales in the data.</p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-1"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb220-2"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-2"></a></span>
<span id="cb220-3"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-3"></a><span class="co"># create the subset of the data with two genes only</span></span>
<span id="cb220-4"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-4"></a><span class="co"># notice that we transpose the matrix so samples are </span></span>
<span id="cb220-5"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-5"></a><span class="co"># on the columns</span></span>
<span id="cb220-6"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-6"></a>sub.mat=<span class="kw">t</span>(mat[<span class="kw">rownames</span>(mat) <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;ENSG00000100504&quot;</span>,<span class="st">&quot;ENSG00000105383&quot;</span>),])</span>
<span id="cb220-7"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-7"></a></span>
<span id="cb220-8"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-8"></a><span class="co"># ploting our genes of interest as scatter plots</span></span>
<span id="cb220-9"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-9"></a><span class="kw">plot</span>(<span class="kw">scale</span>(mat[<span class="kw">rownames</span>(mat)<span class="op">==</span><span class="st">&quot;ENSG00000100504&quot;</span>,]),</span>
<span id="cb220-10"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-10"></a>     <span class="kw">scale</span>(mat[<span class="kw">rownames</span>(mat)<span class="op">==</span><span class="st">&quot;ENSG00000105383&quot;</span>,]),</span>
<span id="cb220-11"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-11"></a>     <span class="dt">pch=</span><span class="dv">19</span>,</span>
<span id="cb220-12"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-12"></a>     <span class="dt">ylab=</span><span class="st">&quot;CD33 (ENSG00000105383)&quot;</span>,</span>
<span id="cb220-13"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-13"></a>     <span class="dt">xlab=</span><span class="st">&quot;PYGL (ENSG00000100504)&quot;</span>,</span>
<span id="cb220-14"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-14"></a>     <span class="dt">col=</span><span class="kw">as.factor</span>(annotation_col<span class="op">$</span>LeukemiaType),</span>
<span id="cb220-15"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-15"></a>     <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>),<span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb220-16"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-16"></a></span>
<span id="cb220-17"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-17"></a><span class="co"># create the legend for the Leukemia types</span></span>
<span id="cb220-18"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-18"></a><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>,</span>
<span id="cb220-19"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-19"></a>       <span class="dt">legend=</span><span class="kw">unique</span>(annotation_col<span class="op">$</span>LeukemiaType),</span>
<span id="cb220-20"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-20"></a>       <span class="dt">fill =</span><span class="kw">palette</span>(<span class="st">&quot;default&quot;</span>),</span>
<span id="cb220-21"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-21"></a>       <span class="dt">border=</span><span class="ot">NA</span>,<span class="dt">box.col=</span><span class="ot">NA</span>)</span>
<span id="cb220-22"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-22"></a></span>
<span id="cb220-23"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-23"></a><span class="co"># calculate the PCA only for our genes and all the samples</span></span>
<span id="cb220-24"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-24"></a>pr=<span class="kw">princomp</span>(<span class="kw">scale</span>(sub.mat))</span>
<span id="cb220-25"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-25"></a></span>
<span id="cb220-26"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-26"></a></span>
<span id="cb220-27"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-27"></a><span class="co"># plot the direction of eigenvectors</span></span>
<span id="cb220-28"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-28"></a><span class="co"># pr$loadings returned by princomp has the eigenvectors</span></span>
<span id="cb220-29"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-29"></a><span class="kw">arrows</span>(<span class="dt">x0=</span><span class="dv">0</span>, <span class="dt">y0=</span><span class="dv">0</span>, <span class="dt">x1 =</span> pr<span class="op">$</span>loadings[<span class="dv">1</span>,<span class="dv">1</span>], </span>
<span id="cb220-30"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-30"></a>         <span class="dt">y1 =</span> pr<span class="op">$</span>loadings[<span class="dv">2</span>,<span class="dv">1</span>],<span class="dt">col=</span><span class="st">&quot;pink&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb220-31"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-31"></a><span class="kw">arrows</span>(<span class="dt">x0=</span><span class="dv">0</span>, <span class="dt">y0=</span><span class="dv">0</span>, <span class="dt">x1 =</span> pr<span class="op">$</span>loadings[<span class="dv">1</span>,<span class="dv">2</span>], </span>
<span id="cb220-32"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-32"></a>         <span class="dt">y1 =</span> pr<span class="op">$</span>loadings[<span class="dv">2</span>,<span class="dv">2</span>],<span class="dt">col=</span><span class="st">&quot;gray&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb220-33"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-33"></a></span>
<span id="cb220-34"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-34"></a></span>
<span id="cb220-35"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-35"></a><span class="co"># plot the samples in the new coordinate system</span></span>
<span id="cb220-36"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-36"></a><span class="kw">plot</span>(<span class="op">-</span>pr<span class="op">$</span>scores,<span class="dt">pch=</span><span class="dv">19</span>,</span>
<span id="cb220-37"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-37"></a>     <span class="dt">col=</span><span class="kw">as.factor</span>(annotation_col<span class="op">$</span>LeukemiaType),</span>
<span id="cb220-38"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-38"></a>     <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>),<span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>))</span>
<span id="cb220-39"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-39"></a></span>
<span id="cb220-40"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-40"></a><span class="co"># plot the new coordinate basis vectors</span></span>
<span id="cb220-41"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-41"></a><span class="kw">arrows</span>(<span class="dt">x0=</span><span class="dv">0</span>, <span class="dt">y0=</span><span class="dv">0</span>, <span class="dt">x1 =</span><span class="op">-</span><span class="dv">2</span>, </span>
<span id="cb220-42"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-42"></a>         <span class="dt">y1 =</span> <span class="dv">0</span>,<span class="dt">col=</span><span class="st">&quot;pink&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb220-43"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-43"></a><span class="kw">arrows</span>(<span class="dt">x0=</span><span class="dv">0</span>, <span class="dt">y0=</span><span class="dv">0</span>, <span class="dt">x1 =</span> <span class="dv">0</span>, </span>
<span id="cb220-44"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb220-44"></a>         <span class="dt">y1 =</span> <span class="dv">-1</span>,<span class="dt">col=</span><span class="st">&quot;gray&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:pcaRot"></span>
<img src="04-unsupervisedLearning_files/figure-html/pcaRot-1.png" alt="Geometric interpretation of PCA finding eigenvectors that point to the direction of highest variance. Eigenvectors can be used as a new coordinate system." width="60%" />
<p class="caption">
FIGURE 4.10: Geometric interpretation of PCA finding eigenvectors that point to the direction of highest variance. Eigenvectors can be used as a new coordinate system.
</p>
</div>
<p>As you can see, the new coordinate system is useful by itself. The X-axis, which represents the first component, separates the data along the lymphoblastic and myeloid leukemias.</p>
<p>PCA in this case, is obtained by calculating eigenvectors of the covariance matrix via an operation called eigen decomposition. The covariance matrix is obtained by covariance of pairwise variables of our expression matrix, which is simply <span class="math inline">\({ \operatorname{cov} (X,Y)={\frac {1}{n}}\sum _{i=1}^{n}(x_{i}-\mu_X)(y_{i}-\mu_Y)}\)</span>, where <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are expression values of genes in a sample in our example. This is a measure of how things vary together, if highly expressed genes in sample A are also highly expressed in sample B and lowly expressed in sample A are also lowly expressed in sample B, then sample A and B will have positive covariance. If the opposite is true, then they will have negative covariance. This quantity is related to correlation, and as we saw in the previous chapter, correlation is standardized covariance. Covariance of variables can be obtained with the <code>cov()</code> function, and eigen decomposition of such a matrix will produce a set of orthogonal vectors that span the directions of highest variation. In 2D, you can think of this operation as rotating two perpendicular lines together until they point to the directions where most of the variation in the data lies, similar to Figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:pcaRot">4.10</a>. An important intuition is that, after the rotation prescribed by eigenvectors is complete, the covariance between variables in this rotated dataset will be zero. There is a proper mathematical relationship between covariances of the rotated dataset and the original dataset. That’s why operating on the covariance matrix is related to the rotation of the original dataset.</p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb221-1"></a>cov.mat=<span class="kw">cov</span>(sub.mat) <span class="co"># calculate covariance matrix</span></span>
<span id="cb221-2"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb221-2"></a>cov.mat</span>
<span id="cb221-3"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb221-3"></a><span class="kw">eigen</span>(cov.mat) <span class="co"># obtain eigen decomposition for eigen values and vectors</span></span></code></pre></div>
<p>Eigenvectors and eigenvalues of the covariance matrix indicate the direction and the magnitude of variation of the data. In our visual example, the eigenvectors are so-called principal components. The eigenvector indicates the direction and the eigenvalues indicate the variation in that direction. Eigenvectors and values exist in pairs: every eigenvector has a corresponding eigenvalue and the eigenvectors are linearly independent from each other, which means they are orthogonal or uncorrelated as in our working example above. The eigenvectors are ranked by their corresponding eigenvalue, the higher the eigenvalue the more important the eigenvector is, because it explains more of the variation compared to the other eigenvectors. This feature of PCA makes the dimension reduction possible. We can sometimes display data sets that have many variables only in 2D or 3D because these top eigenvectors are sometimes enough to capture most of variation in the data. The <code>screeplot()</code> function takes the output of the <code>princomp()</code> or <code>prcomp()</code> functions as input and plots the variance explained by eigenvectors.</p>
<div id="singular-value-decomposition-and-principal-component-analysis" class="section level4">
<h4><span class="header-section-number">4.2.1.1</span> Singular value decomposition and principal component analysis</h4>
A more common way to calculate PCA is through something called singular value decomposition (SVD). This results in another interpretation of PCA, which is called “latent factor” or “latent component” interpretation. In a moment, it  will be clearer what we mean by “latent factors”. SVD is a matrix factorization or decomposition algorithm that decomposes an input matrix,<span class="math inline">\(X\)</span>, to three matrices as follows: <span class="math inline">\(\displaystyle \mathrm{X} = USV^T\)</span>. In essence, many matrices can be decomposed as a product of multiple matrices and we will come to other techniques later in this chapter. Singular value decomposition is shown in Figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:SVDcartoon">4.11</a>. <span class="math inline">\(U\)</span> is the matrix with eigenarrays on the columns and this has the same dimensions as the input matrix; you might see elsewhere the columns are called eigenassays. <span class="math inline">\(S\)</span> is the matrix that contains the singular values on the diagonal. The singular values are also known as eigenvalues and their square is proportional to explained variation by each eigenvector. Finally, the matrix <span class="math inline">\(V^T\)</span> contains the eigenvectors on its rows. Its interpretation is still the same. Geometrically, eigenvectors point to the direction of highest variance in the data. They are uncorrelated or geometrically orthogonal to each other. These interpretations are identical to the ones we made before. The slight difference is that the decomposition seems to output <span class="math inline">\(V^T\)</span>, which is just the transpose of the matrix <span class="math inline">\(V\)</span>. However, the SVD algorithms in R usually return the matrix <span class="math inline">\(V\)</span>. If you want the eigenvectors, you either simply use the columns of matrix <span class="math inline">\(V\)</span> or rows of <span class="math inline">\(V^T\)</span>.
<div class="figure" style="text-align: center"><span id="fig:SVDcartoon"></span>
<img src="images/SVDcartoon.png" alt="Singular value decomposition (SVD) explained in a diagram. " width="60%" />
<p class="caption">
FIGURE 4.11: Singular value decomposition (SVD) explained in a diagram.
</p>
</div>
<p>One thing that is new in Figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:SVDcartoon">4.11</a> is the concept of eigenarrays. The eigenarrays, sometimes called eigenassays, represent the sample space and can be used to plot the relationship between samples rather than genes. In this way, SVD offers additional information than the PCA using the covariance matrix. It offers us a way to summarize both genes and samples. As we can project the gene expression profiles over the top two eigengenes and get a 2D representation of genes, but with the SVD, we can also project the samples over the top two eigenarrays and get a representation of samples in 2D scatter plot. The eigenvector could represent independent expression programs across samples, such as cell-cycle, if we had time-based expression profiles. However, there is no guarantee that each eigenvector will be biologically meaningful. Similarly each eigenarray represents samples with specific expression characteristics. For example, the samples that have a particular pathway activated might be correlated to an eigenarray returned by SVD.</p>
<p>Previously, in order to map samples to the reduced 2D space we had to transpose the genes-by-samples matrix before using the <code>princomp()</code> function. We will now first use SVD on the genes-by-samples matrix to get eigenarrays and use that to plot samples on the reduced dimensions. We will project the columns in our original expression data on eigenarrays and use the first two dimensions in the scatter plot. If you look at the code you will see that for the projection we use <span class="math inline">\(U^T X\)</span> operation, which is just <span class="math inline">\(S V^T\)</span> if you follow the linear algebra. We will also perform the PCA this time with the <code>prcomp()</code> function on the transposed genes-by-samples matrix to get similar information, and plot the samples on the reduced coordinates.</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb222-1"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb222-2"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb222-2"></a>d=<span class="kw">svd</span>(<span class="kw">scale</span>(mat)) <span class="co"># apply SVD</span></span>
<span id="cb222-3"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb222-3"></a>assays=<span class="kw">t</span>(d<span class="op">$</span>u) <span class="op">%*%</span><span class="st"> </span><span class="kw">scale</span>(mat) <span class="co"># projection on eigenassays</span></span>
<span id="cb222-4"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb222-4"></a><span class="kw">plot</span>(assays[<span class="dv">1</span>,],assays[<span class="dv">2</span>,],<span class="dt">pch=</span><span class="dv">19</span>,</span>
<span id="cb222-5"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb222-5"></a>     <span class="dt">col=</span><span class="kw">as.factor</span>(annotation_col<span class="op">$</span>LeukemiaType))</span>
<span id="cb222-6"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb222-6"></a><span class="co">#plot(d$v[,1],d$v[,2],pch=19,</span></span>
<span id="cb222-7"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb222-7"></a><span class="co">#     col=annotation_col$LeukemiaType)</span></span>
<span id="cb222-8"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb222-8"></a>pr=<span class="kw">prcomp</span>(<span class="kw">t</span>(mat),<span class="dt">center=</span><span class="ot">TRUE</span>,<span class="dt">scale=</span><span class="ot">TRUE</span>) <span class="co"># apply PCA on transposed matrix</span></span>
<span id="cb222-9"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb222-9"></a></span>
<span id="cb222-10"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb222-10"></a><span class="co"># plot new coordinates from PCA, projections on eigenvectors</span></span>
<span id="cb222-11"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb222-11"></a><span class="co"># since the matrix is transposed eigenvectors represent </span></span>
<span id="cb222-12"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb222-12"></a><span class="kw">plot</span>(pr<span class="op">$</span>x[,<span class="dv">1</span>],pr<span class="op">$</span>x[,<span class="dv">2</span>],<span class="dt">col=</span><span class="kw">as.factor</span>(annotation_col<span class="op">$</span>LeukemiaType))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:svd"></span>
<img src="04-unsupervisedLearning_files/figure-html/svd-1.png" alt="SVD on the matrix and its transpose" width="65%" />
<p class="caption">
FIGURE 4.12: SVD on the matrix and its transpose
</p>
</div>
<p>As you can see in Figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:svd">4.12</a>, the two approaches yield separation of samples, although they are slightly different. The difference comes from the centering and scaling. In the first case, we scale and center columns and in the second case we scale and center rows since the matrix is transposed. If we do not do any scaling or centering we would get identical plots.</p>
<div id="eigenvectors-as-latent-factorsvariables" class="section level5">
<h5><span class="header-section-number">4.2.1.1.1</span> Eigenvectors as latent factors/variables</h5>
Finally, we can introduce the latent factor interpretation of PCA via SVD. As we have already mentioned, eigenvectors can also be interpreted as expression programs that are shared by several genes such as cell cycle expression program when measuring gene expression across samples taken in different time points. In this interpretation, linear combination of expression programs makes up the expression profile of the genes. Linear combination simply means multiplying the expression program with a weight and adding them up. Our <span class="math inline">\(USV^T\)</span> matrix multiplication can be rearranged to yield such an understanding, we can multiply eigenarrays <span class="math inline">\(U\)</span> with the diagonal eigenvalues <span class="math inline">\(S\)</span>, to produce an m-by-n weights matrix called <span class="math inline">\(W\)</span>, so <span class="math inline">\(W=US\)</span> and we can re-write the equation as just weights by eigenvectors matrix, <span class="math inline">\(X=WV^T\)</span> as shown in Figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:SVDasWeigths">4.13</a>.
<div class="figure" style="text-align: center"><span id="fig:SVDasWeigths"></span>
<img src="images/SVDasWeights.png" alt="Singular value decomposition (SVD) reorganized as multiplication of m-by-n weights matrix and eigenvectors " width="70%" />
<p class="caption">
FIGURE 4.13: Singular value decomposition (SVD) reorganized as multiplication of m-by-n weights matrix and eigenvectors
</p>
</div>
This simple transformation now makes it clear that indeed, if eigenvectors represents expression programs, their linear combination makes up individual gene expression profiles. As an example, we can show the linear combination of the first two eigenvectors can approximate the expression profile of an hypothetical gene in the gene expression matrix. Figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:SVDlatentExample">4.14</a> shows eigenvector 1 and eigenvector 2 combined with certain weights in <span class="math inline">\(W\)</span> matrix can approximate gene expression pattern our example gene.
<div class="figure" style="text-align: center"><span id="fig:SVDlatentExample"></span>
<img src="images/SVDlatentExample.png" alt="Gene expression of a gene can be regarded as a linear combination of eigenvectors. " width="55%" />
<p class="caption">
FIGURE 4.14: Gene expression of a gene can be regarded as a linear combination of eigenvectors.
</p>
</div>
<p>However, SVD does not care about biology. The eigenvectors are just obtained from the data with constraints of orthogonality and the direction of variation. There are examples of eigenvectors representing
real expression programs but that does not mean eigenvectors will always be biologically meaningful. Sometimes a combination of them might make more sense in biology than single eigenvectors. This is also the same for the other matrix factorization techniques we describe below.</p>
</div>
</div>
</div>
<div id="other-matrix-factorization-methods-for-dimensionality-reduction" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Other matrix factorization methods for dimensionality reduction</h3>
<p>We must mention a few other techniques that are similar to SVD in spirit. Remember, we mentioned that every matrix can be decomposed to other matrices where matrix multiplication operations reconstruct the original matrix, which is in general called “matrix factorization”. In the case of SVD/PCA, the constraint is that eigenvectors/arrays are orthogonal, however, there are other decomposition algorithms with other constraints.</p>
<div id="independent-component-analysis-ica" class="section level4">
<h4><span class="header-section-number">4.2.2.1</span> Independent component analysis (ICA)</h4>
We will first start with independent component analysis (ICA) which is an extension of PCA. ICA algorithm decomposes a given matrix <span class="math inline">\(X\)</span> as follows: <span class="math inline">\(X=SA\)</span> <span class="citation">(Hyvärinen <a href="#ref-hyvarinen2013independent" role="doc-biblioref">2013</a>)</span>. The rows of <span class="math inline">\(A\)</span> could be interpreted similar to the eigengenes and columns of <span class="math inline">\(S\)</span> could be interpreted as eigenarrays. These components are sometimes called metagenes and metasamples in the literature. Traditionally, <span class="math inline">\(S\)</span> is called the source matrix and <span class="math inline">\(A\)</span> is called mixing matrix. ICA is developed for a problem called “blind-source separation”. In this problem, multiple microphones record sound from multiple instruments, and the task is to disentangle sounds from original instruments since each microphone is recording a combination of sounds. In this respect, the matrix <span class="math inline">\(S\)</span> contains the original signals (sounds from different instruments) and their linear combinations identified by the weights in <span class="math inline">\(A\)</span>, and the product of <span class="math inline">\(A\)</span> and <span class="math inline">\(S\)</span> makes up the matrix <span class="math inline">\(X\)</span>, which is the observed signal from different microphones. With this interpretation in mind, if the interest is strictly expression patterns that represent the hidden expression programs, we see that the genes-by-samples matrix is transposed to a samples-by-genes matrix, so that the columns of <span class="math inline">\(S\)</span> represent these expression patterns, here referred to as “metagenes”, hopefully representing distinct expression programs (Figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:ICAcartoon">4.15</a> ). 
<div class="figure" style="text-align: center"><span id="fig:ICAcartoon"></span>
<img src="images/ICAcartoon.png" alt="Independent Component Analysis (ICA)" width="55%" />
<p class="caption">
FIGURE 4.15: Independent Component Analysis (ICA)
</p>
</div>
<p>ICA requires that the columns of the <span class="math inline">\(S\)</span> matrix, the “metagenes” in our example above, are statistically independent. This is a stronger constraint than uncorrelatedness. In this case, there should be no relationship between non-linear transformation of the data either. There are different ways of ensuring this statistical indepedence and this is the main constraint when finding the optimal <span class="math inline">\(A\)</span> and <span class="math inline">\(S\)</span> matrices. The various ICA algorithms use different proxies for statistical independence, and the definition of that proxy is the main difference between many ICA algorithms. The algorithm we are going to use requires that metagenes or sources in the <span class="math inline">\(S\)</span> matrix are non-Gaussian (non-normal) as possible. Non-Gaussianity is shown to be related to statistical independence <span class="citation">(Hyvärinen <a href="#ref-hyvarinen2013independent" role="doc-biblioref">2013</a>)</span>. Below, we are using the <code>fastICA::fastICA()</code> function to extract 2 components and plot the rows of matrix <span class="math inline">\(A\)</span> which represents metagenes shown in Figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:fastICAex">4.16</a>. This way, we can visualize samples in a 2D plot. If we wanted to plot the relationship between genes we would use the columns of matrix <span class="math inline">\(S\)</span>.</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb223-1"></a><span class="kw">library</span>(fastICA)</span>
<span id="cb223-2"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb223-2"></a>ica.res=<span class="kw">fastICA</span>(<span class="kw">t</span>(mat),<span class="dt">n.comp=</span><span class="dv">2</span>) <span class="co"># apply ICA</span></span>
<span id="cb223-3"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb223-3"></a></span>
<span id="cb223-4"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb223-4"></a><span class="co"># plot reduced dimensions</span></span>
<span id="cb223-5"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb223-5"></a><span class="kw">plot</span>(ica.res<span class="op">$</span>S[,<span class="dv">1</span>],ica.res<span class="op">$</span>S[,<span class="dv">2</span>],<span class="dt">col=</span><span class="kw">as.factor</span>(annotation_col<span class="op">$</span>LeukemiaType))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fastICAex"></span>
<img src="04-unsupervisedLearning_files/figure-html/fastICAex-1.png" alt="Leukemia gene expression values per patient on reduced dimensions by ICA." width="50%" />
<p class="caption">
FIGURE 4.16: Leukemia gene expression values per patient on reduced dimensions by ICA.
</p>
</div>
</div>
<div id="non-negative-matrix-factorization-nmf" class="section level4">
<h4><span class="header-section-number">4.2.2.2</span> Non-negative matrix factorization (NMF)</h4>
<p>Non-negative matrix factorization
algorithms are series of algorithms that aim to decompose the matrix <span class="math inline">\(X\)</span> into the product of matrices <span class="math inline">\(W\)</span> and <span class="math inline">\(H\)</span>, <span class="math inline">\(X=WH\)</span> (Figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:NMFcartoon">4.17</a>) <span class="citation">(Lee and Seung <a href="#ref-lee2001algorithms" role="doc-biblioref">2001</a>)</span>. The constraint is that <span class="math inline">\(W\)</span> and <span class="math inline">\(H\)</span> must contain non-negative values, so must <span class="math inline">\(X\)</span>. This is well suited for data sets that cannot contain negative values such as gene expression. This also implies additivity of components or latent factors. This is in line with the idea that expression pattern of a gene across samples is weighted sum of multiple metagenes. Unlike ICA and SVD/PCA, the metagenes can never be combined in a subtractive way. In this sense, expression programs potentially captured by metagenes are combined additively.</p>
<div class="figure" style="text-align: center"><span id="fig:NMFcartoon"></span>
<img src="images/NMFcartoon.png" alt="Non-negative matrix factorization summary" width="70%" />
<p class="caption">
FIGURE 4.17: Non-negative matrix factorization summary
</p>
</div>
<p>The algorithms that compute NMF try to minimize the cost function <span class="math inline">\(D(X,WH)\)</span>, which is the distance between <span class="math inline">\(X\)</span> and <span class="math inline">\(WH\)</span>. The early algorithms just use the Euclidean distance, which translates to <span class="math inline">\(\sum(X-WH)^2\)</span>; this is also known as the Frobenius norm and you will see in the literature it is written as :<span class="math inline">\(\||X-WH||_{F}\)</span>.
However, this is not the only distance metric; other distance metrics are also used in NMF algorithms. In addition, there could be other parameters to optimize that relates to sparseness of the <span class="math inline">\(W\)</span> and <span class="math inline">\(H\)</span> matrices. With sparse <span class="math inline">\(W\)</span> and <span class="math inline">\(H\)</span>, each entry in the <span class="math inline">\(X\)</span> matrix is expressed as the sum of a small number of components. This makes the interpretation easier, if the weights are <span class="math inline">\(0\)</span> then there is no contribution from the corresponding factors.</p>
<p>Below, we are plotting the values of metagenes (rows of <span class="math inline">\(H\)</span>) for components 1 and 3, shown in Figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:nmfCode">4.18</a>. In this context, these values can also be interpreted as the relationship between samples. If we wanted to plot the relationship between genes we would plot the columns of the <span class="math inline">\(W\)</span> matrix.</p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb224-1"></a><span class="kw">library</span>(NMF)</span>
<span id="cb224-2"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb224-2"></a>res=NMF<span class="op">::</span><span class="kw">nmf</span>(mat,<span class="dt">rank=</span><span class="dv">3</span>,<span class="dt">seed=</span><span class="st">&quot;nndsvd&quot;</span>) <span class="co"># nmf with 3 components/factors</span></span>
<span id="cb224-3"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb224-3"></a>w &lt;-<span class="st"> </span><span class="kw">basis</span>(res) <span class="co"># get W</span></span>
<span id="cb224-4"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb224-4"></a>h &lt;-<span class="st"> </span><span class="kw">coef</span>(res)  <span class="co"># get H</span></span>
<span id="cb224-5"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb224-5"></a></span>
<span id="cb224-6"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb224-6"></a><span class="co"># plot 1st factor against 3rd factor</span></span>
<span id="cb224-7"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb224-7"></a><span class="kw">plot</span>(h[<span class="dv">1</span>,],h[<span class="dv">3</span>,],<span class="dt">col=</span><span class="kw">as.factor</span>(annotation_col<span class="op">$</span>LeukemiaType),<span class="dt">pch=</span><span class="dv">19</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:nmfCode"></span>
<img src="04-unsupervisedLearning_files/figure-html/nmfCode-1.png" alt="Leukemia gene expression values per patient on reduced dimensions by NMF. Components 1 and 3 is used for the plot." width="60%" />
<p class="caption">
FIGURE 4.18: Leukemia gene expression values per patient on reduced dimensions by NMF. Components 1 and 3 is used for the plot.
</p>
</div>
<p>We should add the note that, due to random starting points of the optimization algorithm, NMF is usually run multiple times and a consensus clustering approach is used when clustering samples. This simply means that samples are clustered together if they cluster together in multiple runs of the NMF. The NMF package we used above has built-in ways to achieve this. In addition, NMF is a family of algorithms. The choice of cost function to optimize the difference between <span class="math inline">\(X\)</span> and <span class="math inline">\(WH\)</span>, and the methods used for optimization creates multiple variants of NMF. The “method” parameter in the above <code>nmf()</code> function controls the algorithm choice for NMF. </p>
</div>
<div id="chosing-the-number-of-components-and-ranking-components-in-importance" class="section level4">
<h4><span class="header-section-number">4.2.2.3</span> Chosing the number of components and ranking components in importance</h4>
<p>In both ICA and NMF, there is no well-defined way to rank components or to select the number of components. There are a couple of approaches that might suit both ICA and NMF for ranking components. One can use the norms of columns/rows in mixing matrices. This could simply mean take the sum of absolute values in mixing matrices.For our ICA example above, we would take the sum of the absolute values of the rows of <span class="math inline">\(A\)</span> since we transposed the input matrix <span class="math inline">\(X\)</span> before ICA. And for the NMF, we would use the columns of <span class="math inline">\(W\)</span>. These ideas assume that the larger coefficients in the weight or mixing matrices indicate more important components.</p>
<p>For selecting the optimal number of components, the NMF package provides different strategies. One way is to calculate the RSS for each <span class="math inline">\(k\)</span>, number of components, and take the <span class="math inline">\(k\)</span> where the RSS curve starts to stabilize. However, these strategies require that you run the algorithm with multiple possible component numbers. The <code>nmf</code> function will run these automatically when the <code>rank</code> argument is a vector of numbers. For ICA there is no straightforward way to choose the right number of components. A common strategy is to start with as many components as variables and try to rank them by their usefulness.</p>

<div class="rmdtip">
<p><strong>Want to know more ?</strong></p>
<p>The NMF package vignette has extensive information on how to run NMF to get stable results and an estimate of components: <a href="https://cran.r-project.org/web/packages/NMF/vignettes/NMF-vignette.pdf" class="uri">https://cran.r-project.org/web/packages/NMF/vignettes/NMF-vignette.pdf</a></p>
</div>

</div>
</div>
<div id="multi-dimensional-scaling" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Multi-dimensional scaling</h3>
<p>MDS is a set of data analysis techniques that displays the structure of distance data in a high-dimensional space into a lower dimensional space without much loss of information <span class="citation">(Cox and Cox <a href="#ref-cox2000multidimensional" role="doc-biblioref">2000</a>)</span>. The overall goal of MDS is to faithfully represent these distances with the lowest possible dimensions. The so-called “classical multi-dimensional scaling” algorithm, tries to minimize the following function:</p>
<p><span class="math inline">\({\displaystyle Stress_{D}(z_{1},z_{2},...,z_{N})={\Biggl (}{\frac {\sum _{i,j}{\bigl (}d_{ij}-\|z_{i}-z_{j}\|{\bigr )}^{2}}{\sum _{i,j}d_{ij}^{2}}}{\Biggr )}^{1/2}}\)</span></p>
<p>Here the function compares the new data points on the lower dimension <span class="math inline">\((z_{1},z_{2},...,z_{N})\)</span> to the input distances between data points or distance between samples in our gene expression example. It turns out, this problem can be efficiently solved with SVD/PCA on the scaled distance matrix, the projection on eigenvectors will be the most optimal solution for the equation above. Therefore, classical MDS is sometimes called Principal Coordinates Analysis in the literature. However, later variants improve on classical MDS by using this as a starting point and optimize a slightly different cost function that again measures how well the low-dimensional distances correspond to high-dimensional distances. This variant is called non-metric MDS and due to the nature of the cost function, it assumes a less stringent relationship between the low-dimensional distances $|z_{i}-z_{j}| and input distances <span class="math inline">\(d_{ij}\)</span>. Formally, this procedure tries to optimize the following function.</p>
<p><span class="math inline">\({\displaystyle Stress_{D}(z_{1},z_{2},...,z_{N})={\Biggl (}{\frac {\sum _{i,j}{\bigl (}\|z_{i}-z_{j}\|-\theta(d_{ij}){\bigr )}^{2}}{\sum _{i,j}\|z_{i}-z_{j}\|^{2}}}{\Biggr )}^{1/2}}\)</span></p>
<p>The core of a non-metric MDS algorithm is a two-fold optimization process. First the optimal monotonic transformation of the distances has to be found, which is shown in the above formula as <span class="math inline">\(\theta(d_{ij})\)</span>. Secondly, the points on a low dimension configuration have to be optimally arranged, so that their distances match the scaled distances as closely as possible. This two steps are repeated until some convergence criteria is reached. This usually means that the cost function does not improve much after certain number of iterations. The basic steps in a non-metric MDS algorithm are:</p>
<ol style="list-style-type: decimal">
<li>Find a random low dimensional configuration of points, or in the variant we will be using below we start with the configuration returned by classical MDS</li>
<li>Calculate the distances between the points in the low dimension <span class="math inline">\(\|z_{i}-z_{j}\|\)</span>, <span class="math inline">\(z_{i}\)</span> and <span class="math inline">\(z_{j}\)</span> are vector of positions for sample <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>.</li>
<li>Find the optimal monotonic transformation of the input distance, <span class="math inline">\({\textstyle \theta(d_{ij})}\)</span>, to approximate input distances to low-dimensional distances. This is achieved by isotonic regression, where a monotonically increasing free-form function is fit. This step practically ensures that ranking of low-dimensional distances are similar to rankings of input distances.</li>
<li>Minimize the stress function by re-configuring low-dimensional space and keeping <span class="math inline">\(\theta\)</span> function constant.</li>
<li>repeat from step 2 until convergence.</li>
</ol>
<p>We will now demonstrate both classical MDS and Kruskal’s isometric MDS.</p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb225-1"></a>mds=<span class="kw">cmdscale</span>(<span class="kw">dist</span>(<span class="kw">t</span>(mat)))</span>
<span id="cb225-2"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb225-2"></a>isomds=MASS<span class="op">::</span><span class="kw">isoMDS</span>(<span class="kw">dist</span>(<span class="kw">t</span>(mat)))</span></code></pre></div>
<pre><code>## initial  value 15.907414 
## final  value 13.462986 
## converged</code></pre>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb227-1"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb227-1"></a><span class="co"># plot the patients in the 2D space</span></span>
<span id="cb227-2"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb227-2"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb227-3"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb227-3"></a><span class="kw">plot</span>(mds,<span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">col=</span><span class="kw">as.factor</span>(annotation_col<span class="op">$</span>LeukemiaType),</span>
<span id="cb227-4"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb227-4"></a>     <span class="dt">main=</span><span class="st">&quot;classical MDS&quot;</span>)</span>
<span id="cb227-5"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb227-5"></a><span class="kw">plot</span>(isomds<span class="op">$</span>points,<span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">col=</span><span class="kw">as.factor</span>(annotation_col<span class="op">$</span>LeukemiaType),</span>
<span id="cb227-6"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb227-6"></a>     <span class="dt">main=</span><span class="st">&quot;isotonic MDS&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:mds2"></span>
<img src="04-unsupervisedLearning_files/figure-html/mds2-1.png" alt="Leukemia gene expression values per patient on reduced dimensions by classical MDS and isometric MDS." width="60%" />
<p class="caption">
FIGURE 4.19: Leukemia gene expression values per patient on reduced dimensions by classical MDS and isometric MDS.
</p>
</div>
<p>The resulting plot is shown in Figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:mds2">4.19</a>. In this example, there is not much difference between isotonic MDS and classical MDS. However, there might be cases where different MDS methods provide visible changes in the scatter plots.</p>
</div>
<div id="t-distributed-stochastic-neighbor-embedding-t-sne" class="section level3">
<h3><span class="header-section-number">4.2.4</span> t-Distributed Stochastic Neighbor Embedding (t-SNE)</h3>
<p>t-SNE maps the distances in high-dimensional space to lower dimensions and it is similar to the MDS method in this respect. But the benefit of this particular method is that it tries to preserve the local structure of the data so the distances and grouping of the points we observe in lower dimensions such as a 2D scatter plot is as close as possible to the distances we observe in the high-dimensional space <span class="citation">(Maaten and Hinton <a href="#ref-maaten2008visualizing" role="doc-biblioref">2008</a>)</span>. As with other dimension reduction methods, you can choose how many lower dimensions you need. The main difference of t-SNE, as mentiones above, is that it tries to preserve the local structure of the data. This kind of local structure embedding is missing in the MDS algorithm, which also has a similar goal. MDS tries to optimize the distances as a whole, whereas t-SNE optimizes the distances with the local structure in mind. This is defined by the “perplexity” parameter in the arguments. This parameter controls how much the local structure influences the distance calculation. The lower the value, the more the local structure is taken into account. Similar to MDS, the process is an optimization algorithm. Here, we also try to minimize the divergence between observed distances and lower dimension distances. However, in the case of t-SNE, the observed distances and lower dimensional distances are transformed using a probabilistic framework with their local variance in mind.</p>
<p>From here on, we will provide a bit more detail on how the algorithm works in case the conceptual description above is too shallow. In t-SNE the Euclidean distances between data points are transformed into a conditional similarity between points. This is done by assuming a normal distribution on each data point with a variance calculated ultimately by the use of the “perplexity” parameter. The perplexity parameter is, in a sense, a guess about the number of the closest neighbors each point has. Setting it to higher values gives more weight to global structure. Given <span class="math inline">\(d_{ij}\)</span> is the Euclidean distance between point <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, the similarity score <span class="math inline">\(p_{ij}\)</span> is calculated as shown below.</p>
<p><span class="math display">\[p_{j | i} = \frac{\exp(-\|d_{ij}\|^2 / 2 σ_i^2)}{∑_{k \neq i} \exp(-\|d_{ik}\|^2 / 2 σ_i^2)}\]</span></p>
<p>This distance is symmetrized by incorporating <span class="math inline">\(p_{i | j}\)</span> as shown below.</p>
<p><span class="math display">\[p_{i j}=\frac{p_{j|i} + p_{i|j}}{2n}\]</span></p>
<p>For the distances in the reduced dimension, we use t-distribution with one degree of freedom. In the formula below, <span class="math inline">\(| y_i-y_j\|^2\)</span> is Euclidean distance between points <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> in the reduced dimensions.</p>
<p><span class="math display">\[
q_{i j} = \frac{(1+ \| y_i-y_j\|^2)^{-1}}{(∑_{k \neq l} 1+ \| y_k-y_l\|^2)^{-1} }
\]</span></p>
<p>As most of the algorithms we have seen in this section, t-SNE is an optimization process in essence. In every iteration the points along lower dimensions are re-arranged to minimize the formulated difference between the observed joint probabilities (<span class="math inline">\(p_{i j}\)</span>) and low-dimensional joint probabilities (<span class="math inline">\(q_{i j}\)</span>). Here we are trying to compare probability distributions. In this case, this is done using a method called Kullback-Leibler divergence, or KL-divergence. In the formula below, since the <span class="math inline">\(p_{i j}\)</span> is pre-defined using original distances, the only way to optimize is to play with <span class="math inline">\(q_{i j}\)</span> because it depends on the configuration of points in the lower dimensional space. This configuration is optimized to minimize the KL-divergence between <span class="math inline">\(p_{i j}\)</span> and <span class="math inline">\(q_{i j}\)</span>.</p>
<p><span class="math display">\[
KL(P||Q) = \sum_{i, j} p_{ij} \, \log \frac{p_{ij}}{q_{ij}}.
\]</span>
Strictly speaking, KL-divergence measures how well the distribution <span class="math inline">\(P\)</span> which is observed using the original data points can be approximated by distribution <span class="math inline">\(Q\)</span>, which is modeled using points on the lower dimension. If the distributions are identical, KL-divergence would be <span class="math inline">\(0\)</span>. Naturally, the more divergent the distributions are, the higher the KL-divergence will be.</p>
<p>We will now show how to use t-SNE on our gene expression data set using the <code>Rtsne</code> package . We are setting the random seed because again, the t-SNE optimization algorithm has random starting points and this might create non-identical results in every run. After calculating the t-SNE lower dimension embeddings we plot the points in a 2D scatter plot, shown in Figure <a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#fig:tsne">4.20</a>.</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb228-1"></a><span class="kw">library</span>(<span class="st">&quot;Rtsne&quot;</span>)</span>
<span id="cb228-2"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb228-2"></a><span class="kw">set.seed</span>(<span class="dv">42</span>) <span class="co"># Set a seed if you want reproducible results</span></span>
<span id="cb228-3"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb228-3"></a>tsne_out &lt;-<span class="st"> </span><span class="kw">Rtsne</span>(<span class="kw">t</span>(mat),<span class="dt">perplexity =</span> <span class="dv">10</span>) <span class="co"># Run TSNE</span></span>
<span id="cb228-4"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb228-4"></a> <span class="co">#image(t(as.matrix(dist(tsne_out$Y))))</span></span>
<span id="cb228-5"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb228-5"></a><span class="co"># Show the objects in the 2D tsne representation</span></span>
<span id="cb228-6"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb228-6"></a><span class="kw">plot</span>(tsne_out<span class="op">$</span>Y,<span class="dt">col=</span><span class="kw">as.factor</span>(annotation_col<span class="op">$</span>LeukemiaType),</span>
<span id="cb228-7"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb228-7"></a>     <span class="dt">pch=</span><span class="dv">19</span>)</span>
<span id="cb228-8"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb228-8"></a></span>
<span id="cb228-9"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb228-9"></a><span class="co"># create the legend for the Leukemia types</span></span>
<span id="cb228-10"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb228-10"></a><span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>,</span>
<span id="cb228-11"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb228-11"></a>       <span class="dt">legend=</span><span class="kw">unique</span>(annotation_col<span class="op">$</span>LeukemiaType),</span>
<span id="cb228-12"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb228-12"></a>       <span class="dt">fill =</span><span class="kw">palette</span>(<span class="st">&quot;default&quot;</span>),</span>
<span id="cb228-13"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#cb228-13"></a>       <span class="dt">border=</span><span class="ot">NA</span>,<span class="dt">box.col=</span><span class="ot">NA</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:tsne"></span>
<img src="04-unsupervisedLearning_files/figure-html/tsne-1.png" alt="t-SNE of leukemia expression dataset" width="60%" />
<p class="caption">
FIGURE 4.20: t-SNE of leukemia expression dataset
</p>
</div>
<p>As you might have noticed, we set again a random seed with the <code>set.seed()</code> function. The optimization algorithm starts with random configuration of points in the lower dimension space, and in each iteration it tries to improve on the previous lower dimension conflagration, which is why starting points can result in different final outcomes.</p>

<div class="rmdtip">
<p><strong>Want to know more ?</strong></p>
<ul>
<li>How perplexity affects t-sne, interactive examples: <a href="https://distill.pub/2016/misread-tsne/" class="uri">https://distill.pub/2016/misread-tsne/</a></li>
<li>More on perplexity: <a href="https://blog.paperspace.com/dimension-reduction-with-t-sne/" class="uri">https://blog.paperspace.com/dimension-reduction-with-t-sne/</a></li>
<li>Intro to t-SNE: <a href="https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm" class="uri">https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm</a></li>
</ul>
</div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-cox2000multidimensional">
<p>Cox, and Cox. 2000. <em>Multidimensional Scaling, Second Edition</em>. Chapman &amp; Hall/Crc Monographs on Statistics &amp; Applied Probability. CRC Press.</p>
</div>
<div id="ref-hyvarinen2013independent">
<p>Hyvärinen. 2013. “Independent Component Analysis: Recent Advances.” <em>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</em> 371 (1984): 20110534.</p>
</div>
<div id="ref-lee2001algorithms">
<p>Lee, and Seung. 2001. “Algorithms for Non-Negative Matrix Factorization.” In <em>Advances in Neural Information Processing Systems</em>, 556–62.</p>
</div>
<div id="ref-maaten2008visualizing">
<p>Maaten, and Hinton. 2008. “Visualizing Data Using T-Sne.” <em>Journal of Machine Learning Research</em> 9 (Nov): 2579–2605.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="clustering-grouping-samples-based-on-their-similarity.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="exercises-2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/compgenomr/book/edit/master/04-unsupervisedLearning.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
