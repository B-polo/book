<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.14 Other supervised algorithms | Computational Genomics with R</title>
  <meta name="description" content="A guide to computationa genomics using R. The book covers fundemental topics with practical examples for an interdisciplinery audience" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="5.14 Other supervised algorithms | Computational Genomics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://compmgenomr.github.io/book/" />
  <meta property="og:image" content="https://compmgenomr.github.io/book/images/cover.jpg" />
  <meta property="og:description" content="A guide to computationa genomics using R. The book covers fundemental topics with practical examples for an interdisciplinery audience" />
  <meta name="github-repo" content="compgenomr/book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.14 Other supervised algorithms | Computational Genomics with R" />
  
  <meta name="twitter:description" content="A guide to computationa genomics using R. The book covers fundemental topics with practical examples for an interdisciplinery audience" />
  <meta name="twitter:image" content="https://compmgenomr.github.io/book/images/cover.jpg" />

<meta name="author" content="Altuna Akalin" />


<meta name="date" content="2020-09-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="logistic-regression-and-regularization.html"/>
<link rel="next" href="predicting-continuous-variables-regression-with-machine-learning.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-83786243-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-83786243-1');
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Computational Genomics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="who-is-this-book-for.html"><a href="who-is-this-book-for.html"><i class="fa fa-check"></i>Who is this book for?</a><ul>
<li class="chapter" data-level="" data-path="who-is-this-book-for.html"><a href="who-is-this-book-for.html#what-will-you-get-out-of-this"><i class="fa fa-check"></i>What will you get out of this?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="structure-of-the-book.html"><a href="structure-of-the-book.html"><i class="fa fa-check"></i>Structure of the book</a></li>
<li class="chapter" data-level="" data-path="software-information-and-conventions.html"><a href="software-information-and-conventions.html"><i class="fa fa-check"></i>Software information and conventions</a><ul>
<li class="chapter" data-level="" data-path="software-information-and-conventions.html"><a href="software-information-and-conventions.html#assignment-operator-convention"><i class="fa fa-check"></i>Assignment operator convention</a></li>
<li class="chapter" data-level="" data-path="software-information-and-conventions.html"><a href="software-information-and-conventions.html#packages-needed-to-run-the-book-code"><i class="fa fa-check"></i>Packages needed to run the book code</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="data-for-the-book.html"><a href="data-for-the-book.html"><i class="fa fa-check"></i>Data for the book</a></li>
<li class="chapter" data-level="" data-path="exercises-in-the-book.html"><a href="exercises-in-the-book.html"><i class="fa fa-check"></i>Exercises in the book</a></li>
<li class="chapter" data-level="" data-path="reproducibility-statement.html"><a href="reproducibility-statement.html"><i class="fa fa-check"></i>Reproducibility statement</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="how-to-contribute.html"><a href="how-to-contribute.html"><i class="fa fa-check"></i>How to contribute</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to Genomics</a><ul>
<li class="chapter" data-level="1.1" data-path="genes-dna-and-central-dogma.html"><a href="genes-dna-and-central-dogma.html"><i class="fa fa-check"></i><b>1.1</b> Genes, DNA and central dogma</a><ul>
<li class="chapter" data-level="1.1.1" data-path="genes-dna-and-central-dogma.html"><a href="genes-dna-and-central-dogma.html#what-is-a-genome"><i class="fa fa-check"></i><b>1.1.1</b> What is a genome?</a></li>
<li class="chapter" data-level="1.1.2" data-path="genes-dna-and-central-dogma.html"><a href="genes-dna-and-central-dogma.html#what-is-a-gene"><i class="fa fa-check"></i><b>1.1.2</b> What is a gene?</a></li>
<li class="chapter" data-level="1.1.3" data-path="genes-dna-and-central-dogma.html"><a href="genes-dna-and-central-dogma.html#how-are-genes-controlled-transcriptional-and-post-transcriptional-regulation"><i class="fa fa-check"></i><b>1.1.3</b> How are genes controlled? Transcriptional and post-transcriptional regulation</a></li>
<li class="chapter" data-level="1.1.4" data-path="genes-dna-and-central-dogma.html"><a href="genes-dna-and-central-dogma.html#what-does-a-gene-look-like"><i class="fa fa-check"></i><b>1.1.4</b> What does a gene look like?</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="elements-of-gene-regulation.html"><a href="elements-of-gene-regulation.html"><i class="fa fa-check"></i><b>1.2</b> Elements of gene regulation</a><ul>
<li class="chapter" data-level="1.2.1" data-path="elements-of-gene-regulation.html"><a href="elements-of-gene-regulation.html#transcriptional-regulation"><i class="fa fa-check"></i><b>1.2.1</b> Transcriptional regulation</a></li>
<li class="chapter" data-level="1.2.2" data-path="elements-of-gene-regulation.html"><a href="elements-of-gene-regulation.html#post-transcriptional-regulation"><i class="fa fa-check"></i><b>1.2.2</b> Post-transcriptional regulation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="shaping-the-genome-dna-mutation.html"><a href="shaping-the-genome-dna-mutation.html"><i class="fa fa-check"></i><b>1.3</b> Shaping the genome: DNA mutation</a></li>
<li class="chapter" data-level="1.4" data-path="high-throughput-experimental-methods-in-genomics.html"><a href="high-throughput-experimental-methods-in-genomics.html"><i class="fa fa-check"></i><b>1.4</b> High-throughput experimental methods in genomics</a><ul>
<li class="chapter" data-level="1.4.1" data-path="high-throughput-experimental-methods-in-genomics.html"><a href="high-throughput-experimental-methods-in-genomics.html#the-general-idea-behind-high-throughput-techniques"><i class="fa fa-check"></i><b>1.4.1</b> The general idea behind high-throughput techniques</a></li>
<li class="chapter" data-level="1.4.2" data-path="high-throughput-experimental-methods-in-genomics.html"><a href="high-throughput-experimental-methods-in-genomics.html#high-throughput-sequencing"><i class="fa fa-check"></i><b>1.4.2</b> High-throughput sequencing</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="visualization-and-data-repositories-for-genomics.html"><a href="visualization-and-data-repositories-for-genomics.html"><i class="fa fa-check"></i><b>1.5</b> Visualization and data repositories for genomics</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Rintro.html"><a href="Rintro.html"><i class="fa fa-check"></i><b>2</b> Introduction to R for Genomic Data Analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html"><i class="fa fa-check"></i><b>2.1</b> Steps of (genomic) data analysis</a><ul>
<li class="chapter" data-level="2.1.1" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html#data-collection"><i class="fa fa-check"></i><b>2.1.1</b> Data collection</a></li>
<li class="chapter" data-level="2.1.2" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html#data-quality-check-and-cleaning"><i class="fa fa-check"></i><b>2.1.2</b> Data quality check and cleaning</a></li>
<li class="chapter" data-level="2.1.3" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html#data-processing"><i class="fa fa-check"></i><b>2.1.3</b> Data processing</a></li>
<li class="chapter" data-level="2.1.4" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html#exploratory-data-analysis-and-modeling"><i class="fa fa-check"></i><b>2.1.4</b> Exploratory data analysis and modeling</a></li>
<li class="chapter" data-level="2.1.5" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html#visualization-and-reporting"><i class="fa fa-check"></i><b>2.1.5</b> Visualization and reporting</a></li>
<li class="chapter" data-level="2.1.6" data-path="steps-of-genomic-data-analysis.html"><a href="steps-of-genomic-data-analysis.html#why-use-r-for-genomics"><i class="fa fa-check"></i><b>2.1.6</b> Why use R for genomics ?</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html"><i class="fa fa-check"></i><b>2.2</b> Getting started with R</a><ul>
<li class="chapter" data-level="2.2.1" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#installing-packages"><i class="fa fa-check"></i><b>2.2.1</b> Installing packages</a></li>
<li class="chapter" data-level="2.2.2" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#installing-packages-in-custom-locations"><i class="fa fa-check"></i><b>2.2.2</b> Installing packages in custom locations</a></li>
<li class="chapter" data-level="2.2.3" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#getting-help-on-functions-and-packages"><i class="fa fa-check"></i><b>2.2.3</b> Getting help on functions and packages</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="computations-in-r.html"><a href="computations-in-r.html"><i class="fa fa-check"></i><b>2.3</b> Computations in R</a></li>
<li class="chapter" data-level="2.4" data-path="data-structures.html"><a href="data-structures.html"><i class="fa fa-check"></i><b>2.4</b> Data structures</a><ul>
<li class="chapter" data-level="2.4.1" data-path="data-structures.html"><a href="data-structures.html#vectors"><i class="fa fa-check"></i><b>2.4.1</b> Vectors</a></li>
<li class="chapter" data-level="2.4.2" data-path="data-structures.html"><a href="data-structures.html#matrices"><i class="fa fa-check"></i><b>2.4.2</b> Matrices</a></li>
<li class="chapter" data-level="2.4.3" data-path="data-structures.html"><a href="data-structures.html#data-frames"><i class="fa fa-check"></i><b>2.4.3</b> Data frames</a></li>
<li class="chapter" data-level="2.4.4" data-path="data-structures.html"><a href="data-structures.html#lists"><i class="fa fa-check"></i><b>2.4.4</b> Lists</a></li>
<li class="chapter" data-level="2.4.5" data-path="data-structures.html"><a href="data-structures.html#factors"><i class="fa fa-check"></i><b>2.4.5</b> Factors</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="data-types.html"><a href="data-types.html"><i class="fa fa-check"></i><b>2.5</b> Data types</a></li>
<li class="chapter" data-level="2.6" data-path="reading-and-writing-data.html"><a href="reading-and-writing-data.html"><i class="fa fa-check"></i><b>2.6</b> Reading and writing data</a><ul>
<li class="chapter" data-level="2.6.1" data-path="reading-and-writing-data.html"><a href="reading-and-writing-data.html#reading-large-files"><i class="fa fa-check"></i><b>2.6.1</b> Reading large files</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="plotting-in-r-with-base-graphics.html"><a href="plotting-in-r-with-base-graphics.html"><i class="fa fa-check"></i><b>2.7</b> Plotting in R with base graphics</a><ul>
<li class="chapter" data-level="2.7.1" data-path="plotting-in-r-with-base-graphics.html"><a href="plotting-in-r-with-base-graphics.html#combining-multiple-plots"><i class="fa fa-check"></i><b>2.7.1</b> Combining multiple plots</a></li>
<li class="chapter" data-level="2.7.2" data-path="plotting-in-r-with-base-graphics.html"><a href="plotting-in-r-with-base-graphics.html#saving-plots"><i class="fa fa-check"></i><b>2.7.2</b> Saving plots</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="plotting-in-r-with-ggplot2.html"><a href="plotting-in-r-with-ggplot2.html"><i class="fa fa-check"></i><b>2.8</b> Plotting in R with ggplot2</a><ul>
<li class="chapter" data-level="2.8.1" data-path="plotting-in-r-with-ggplot2.html"><a href="plotting-in-r-with-ggplot2.html#combining-multiple-plots-1"><i class="fa fa-check"></i><b>2.8.1</b> Combining multiple plots</a></li>
<li class="chapter" data-level="2.8.2" data-path="plotting-in-r-with-ggplot2.html"><a href="plotting-in-r-with-ggplot2.html#ggplot2-and-tidyverse"><i class="fa fa-check"></i><b>2.8.2</b> ggplot2 and tidyverse</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="functions-and-control-structures-for-ifelse-etc-.html"><a href="functions-and-control-structures-for-ifelse-etc-.html"><i class="fa fa-check"></i><b>2.9</b> Functions and control structures (for, if/else etc.)</a><ul>
<li class="chapter" data-level="2.9.1" data-path="functions-and-control-structures-for-ifelse-etc-.html"><a href="functions-and-control-structures-for-ifelse-etc-.html#user-defined-functions"><i class="fa fa-check"></i><b>2.9.1</b> User-defined functions</a></li>
<li class="chapter" data-level="2.9.2" data-path="functions-and-control-structures-for-ifelse-etc-.html"><a href="functions-and-control-structures-for-ifelse-etc-.html#loops-and-looping-structures-in-r"><i class="fa fa-check"></i><b>2.9.2</b> Loops and looping structures in R</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>2.10</b> Exercises</a><ul>
<li class="chapter" data-level="2.10.1" data-path="exercises.html"><a href="exercises.html#computations-in-r-1"><i class="fa fa-check"></i><b>2.10.1</b> Computations in R</a></li>
<li class="chapter" data-level="2.10.2" data-path="exercises.html"><a href="exercises.html#data-structures-in-r"><i class="fa fa-check"></i><b>2.10.2</b> Data structures in R</a></li>
<li class="chapter" data-level="2.10.3" data-path="exercises.html"><a href="exercises.html#reading-in-and-writing-data-out-in-r"><i class="fa fa-check"></i><b>2.10.3</b> Reading in and writing data out in R</a></li>
<li class="chapter" data-level="2.10.4" data-path="exercises.html"><a href="exercises.html#plotting-in-r"><i class="fa fa-check"></i><b>2.10.4</b> Plotting in R</a></li>
<li class="chapter" data-level="2.10.5" data-path="exercises.html"><a href="exercises.html#functions-and-control-structures-for-ifelse-etc.-1"><i class="fa fa-check"></i><b>2.10.5</b> Functions and control structures (for, if/else, etc.)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="stats.html"><a href="stats.html"><i class="fa fa-check"></i><b>3</b> Statistics for Genomics</a><ul>
<li class="chapter" data-level="3.1" data-path="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html"><a href="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html"><i class="fa fa-check"></i><b>3.1</b> How to summarize collection of data points: The idea behind statistical distributions</a><ul>
<li class="chapter" data-level="3.1.1" data-path="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html"><a href="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#describing-the-central-tendency-mean-and-median"><i class="fa fa-check"></i><b>3.1.1</b> Describing the central tendency: Mean and median</a></li>
<li class="chapter" data-level="3.1.2" data-path="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html"><a href="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#describing-the-spread-measurements-of-variation"><i class="fa fa-check"></i><b>3.1.2</b> Describing the spread: Measurements of variation</a></li>
<li class="chapter" data-level="3.1.3" data-path="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html"><a href="how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html#precision-of-estimates-confidence-intervals"><i class="fa fa-check"></i><b>3.1.3</b> Precision of estimates: Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="how-to-test-for-differences-between-samples.html"><a href="how-to-test-for-differences-between-samples.html"><i class="fa fa-check"></i><b>3.2</b> How to test for differences between samples</a><ul>
<li class="chapter" data-level="3.2.1" data-path="how-to-test-for-differences-between-samples.html"><a href="how-to-test-for-differences-between-samples.html#randomization-based-testing-for-difference-of-the-means"><i class="fa fa-check"></i><b>3.2.1</b> Randomization-based testing for difference of the means</a></li>
<li class="chapter" data-level="3.2.2" data-path="how-to-test-for-differences-between-samples.html"><a href="how-to-test-for-differences-between-samples.html#using-t-test-for-difference-of-the-means-between-two-samples"><i class="fa fa-check"></i><b>3.2.2</b> Using t-test for difference of the means between two samples</a></li>
<li class="chapter" data-level="3.2.3" data-path="how-to-test-for-differences-between-samples.html"><a href="how-to-test-for-differences-between-samples.html#multiple-testing-correction"><i class="fa fa-check"></i><b>3.2.3</b> Multiple testing correction</a></li>
<li class="chapter" data-level="3.2.4" data-path="how-to-test-for-differences-between-samples.html"><a href="how-to-test-for-differences-between-samples.html#moderated-t-tests-using-information-from-multiple-comparisons"><i class="fa fa-check"></i><b>3.2.4</b> Moderated t-tests: Using information from multiple comparisons</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="relationship-between-variables-linear-models-and-correlation.html"><a href="relationship-between-variables-linear-models-and-correlation.html"><i class="fa fa-check"></i><b>3.3</b> Relationship between variables: Linear models and correlation</a><ul>
<li class="chapter" data-level="3.3.1" data-path="relationship-between-variables-linear-models-and-correlation.html"><a href="relationship-between-variables-linear-models-and-correlation.html#how-to-fit-a-line"><i class="fa fa-check"></i><b>3.3.1</b> How to fit a line</a></li>
<li class="chapter" data-level="3.3.2" data-path="relationship-between-variables-linear-models-and-correlation.html"><a href="relationship-between-variables-linear-models-and-correlation.html#how-to-estimate-the-error-of-the-coefficients"><i class="fa fa-check"></i><b>3.3.2</b> How to estimate the error of the coefficients</a></li>
<li class="chapter" data-level="3.3.3" data-path="relationship-between-variables-linear-models-and-correlation.html"><a href="relationship-between-variables-linear-models-and-correlation.html#accuracy-of-the-model"><i class="fa fa-check"></i><b>3.3.3</b> Accuracy of the model</a></li>
<li class="chapter" data-level="3.3.4" data-path="relationship-between-variables-linear-models-and-correlation.html"><a href="relationship-between-variables-linear-models-and-correlation.html#regression-with-categorical-variables"><i class="fa fa-check"></i><b>3.3.4</b> Regression with categorical variables</a></li>
<li class="chapter" data-level="3.3.5" data-path="relationship-between-variables-linear-models-and-correlation.html"><a href="relationship-between-variables-linear-models-and-correlation.html#regression-pitfalls"><i class="fa fa-check"></i><b>3.3.5</b> Regression pitfalls</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>3.4</b> Exercises</a><ul>
<li class="chapter" data-level="3.4.1" data-path="exercises-1.html"><a href="exercises-1.html#how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions-1"><i class="fa fa-check"></i><b>3.4.1</b> How to summarize collection of data points: The idea behind statistical distributions</a></li>
<li class="chapter" data-level="3.4.2" data-path="exercises-1.html"><a href="exercises-1.html#how-to-test-for-differences-in-samples"><i class="fa fa-check"></i><b>3.4.2</b> How to test for differences in samples</a></li>
<li class="chapter" data-level="3.4.3" data-path="exercises-1.html"><a href="exercises-1.html#relationship-between-variables-linear-models-and-correlation-1"><i class="fa fa-check"></i><b>3.4.3</b> Relationship between variables: Linear models and correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="unsupervisedLearning.html"><a href="unsupervisedLearning.html"><i class="fa fa-check"></i><b>4</b> Exploratory Data Analysis with Unsupervised Machine Learning</a><ul>
<li class="chapter" data-level="4.1" data-path="clustering-grouping-samples-based-on-their-similarity.html"><a href="clustering-grouping-samples-based-on-their-similarity.html"><i class="fa fa-check"></i><b>4.1</b> Clustering: Grouping samples based on their similarity</a><ul>
<li class="chapter" data-level="4.1.1" data-path="clustering-grouping-samples-based-on-their-similarity.html"><a href="clustering-grouping-samples-based-on-their-similarity.html#distance-metrics"><i class="fa fa-check"></i><b>4.1.1</b> Distance metrics</a></li>
<li class="chapter" data-level="4.1.2" data-path="clustering-grouping-samples-based-on-their-similarity.html"><a href="clustering-grouping-samples-based-on-their-similarity.html#hiearchical-clustering"><i class="fa fa-check"></i><b>4.1.2</b> Hiearchical clustering</a></li>
<li class="chapter" data-level="4.1.3" data-path="clustering-grouping-samples-based-on-their-similarity.html"><a href="clustering-grouping-samples-based-on-their-similarity.html#k-means-clustering"><i class="fa fa-check"></i><b>4.1.3</b> K-means clustering</a></li>
<li class="chapter" data-level="4.1.4" data-path="clustering-grouping-samples-based-on-their-similarity.html"><a href="clustering-grouping-samples-based-on-their-similarity.html#how-to-choose-k-the-number-of-clusters"><i class="fa fa-check"></i><b>4.1.4</b> How to choose “k”, the number of clusters</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html"><i class="fa fa-check"></i><b>4.2</b> Dimensionality reduction techniques: Visualizing complex data sets in 2D</a><ul>
<li class="chapter" data-level="4.2.1" data-path="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#principal-component-analysis"><i class="fa fa-check"></i><b>4.2.1</b> Principal component analysis</a></li>
<li class="chapter" data-level="4.2.2" data-path="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#other-matrix-factorization-methods-for-dimensionality-reduction"><i class="fa fa-check"></i><b>4.2.2</b> Other matrix factorization methods for dimensionality reduction</a></li>
<li class="chapter" data-level="4.2.3" data-path="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#multi-dimensional-scaling"><i class="fa fa-check"></i><b>4.2.3</b> Multi-dimensional scaling</a></li>
<li class="chapter" data-level="4.2.4" data-path="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html"><a href="dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html#t-distributed-stochastic-neighbor-embedding-t-sne"><i class="fa fa-check"></i><b>4.2.4</b> t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.3</b> Exercises</a><ul>
<li class="chapter" data-level="4.3.1" data-path="exercises-2.html"><a href="exercises-2.html#clustering"><i class="fa fa-check"></i><b>4.3.1</b> Clustering</a></li>
<li class="chapter" data-level="4.3.2" data-path="exercises-2.html"><a href="exercises-2.html#dimension-reduction"><i class="fa fa-check"></i><b>4.3.2</b> Dimension reduction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="supervisedLearning.html"><a href="supervisedLearning.html"><i class="fa fa-check"></i><b>5</b> Predictive Modeling with Supervised Machine Learning</a><ul>
<li class="chapter" data-level="5.1" data-path="how-are-machine-learning-models-fit.html"><a href="how-are-machine-learning-models-fit.html"><i class="fa fa-check"></i><b>5.1</b> How are machine learning models fit?</a><ul>
<li class="chapter" data-level="5.1.1" data-path="how-are-machine-learning-models-fit.html"><a href="how-are-machine-learning-models-fit.html#machine-learning-vs.-statistics"><i class="fa fa-check"></i><b>5.1.1</b> Machine learning vs. statistics</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="steps-in-supervised-machine-learning.html"><a href="steps-in-supervised-machine-learning.html"><i class="fa fa-check"></i><b>5.2</b> Steps in supervised machine learning</a></li>
<li class="chapter" data-level="5.3" data-path="use-case-disease-subtype-from-genomics-data.html"><a href="use-case-disease-subtype-from-genomics-data.html"><i class="fa fa-check"></i><b>5.3</b> Use case: Disease subtype from genomics data</a></li>
<li class="chapter" data-level="5.4" data-path="data-preprocessing.html"><a href="data-preprocessing.html"><i class="fa fa-check"></i><b>5.4</b> Data preprocessing</a><ul>
<li class="chapter" data-level="5.4.1" data-path="data-preprocessing.html"><a href="data-preprocessing.html#data-transformation"><i class="fa fa-check"></i><b>5.4.1</b> Data transformation</a></li>
<li class="chapter" data-level="5.4.2" data-path="data-preprocessing.html"><a href="data-preprocessing.html#filtering-data-and-scaling"><i class="fa fa-check"></i><b>5.4.2</b> Filtering data and scaling</a></li>
<li class="chapter" data-level="5.4.3" data-path="data-preprocessing.html"><a href="data-preprocessing.html#dealing-with-missing-values"><i class="fa fa-check"></i><b>5.4.3</b> Dealing with missing values</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="splitting-the-data.html"><a href="splitting-the-data.html"><i class="fa fa-check"></i><b>5.5</b> Splitting the data</a><ul>
<li class="chapter" data-level="5.5.1" data-path="splitting-the-data.html"><a href="splitting-the-data.html#holdout-test-dataset"><i class="fa fa-check"></i><b>5.5.1</b> Holdout test dataset</a></li>
<li class="chapter" data-level="5.5.2" data-path="splitting-the-data.html"><a href="splitting-the-data.html#cross-validation"><i class="fa fa-check"></i><b>5.5.2</b> Cross-validation</a></li>
<li class="chapter" data-level="5.5.3" data-path="splitting-the-data.html"><a href="splitting-the-data.html#bootstrap-resampling"><i class="fa fa-check"></i><b>5.5.3</b> Bootstrap resampling</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="predicting-the-subtype-with-k-nearest-neighbors.html"><a href="predicting-the-subtype-with-k-nearest-neighbors.html"><i class="fa fa-check"></i><b>5.6</b> Predicting the subtype with k-nearest neighbors</a></li>
<li class="chapter" data-level="5.7" data-path="assessing-the-performance-of-our-model.html"><a href="assessing-the-performance-of-our-model.html"><i class="fa fa-check"></i><b>5.7</b> Assessing the performance of our model</a><ul>
<li class="chapter" data-level="5.7.1" data-path="assessing-the-performance-of-our-model.html"><a href="assessing-the-performance-of-our-model.html#receiver-operating-characteristic-roc-curves"><i class="fa fa-check"></i><b>5.7.1</b> Receiver Operating Characteristic (ROC) curves</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="model-tuning-and-avoiding-overfitting.html"><a href="model-tuning-and-avoiding-overfitting.html"><i class="fa fa-check"></i><b>5.8</b> Model tuning and avoiding overfitting</a><ul>
<li class="chapter" data-level="5.8.1" data-path="model-tuning-and-avoiding-overfitting.html"><a href="model-tuning-and-avoiding-overfitting.html#model-complexity-and-bias-variance-trade-off"><i class="fa fa-check"></i><b>5.8.1</b> Model complexity and bias variance trade-off</a></li>
<li class="chapter" data-level="5.8.2" data-path="model-tuning-and-avoiding-overfitting.html"><a href="model-tuning-and-avoiding-overfitting.html#data-split-strategies-for-model-tuning-and-testing"><i class="fa fa-check"></i><b>5.8.2</b> Data split strategies for model tuning and testing</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="variable-importance.html"><a href="variable-importance.html"><i class="fa fa-check"></i><b>5.9</b> Variable importance</a></li>
<li class="chapter" data-level="5.10" data-path="how-to-deal-with-class-imbalance.html"><a href="how-to-deal-with-class-imbalance.html"><i class="fa fa-check"></i><b>5.10</b> How to deal with class imbalance</a><ul>
<li class="chapter" data-level="5.10.1" data-path="how-to-deal-with-class-imbalance.html"><a href="how-to-deal-with-class-imbalance.html#sampling-for-class-balance"><i class="fa fa-check"></i><b>5.10.1</b> Sampling for class balance</a></li>
<li class="chapter" data-level="5.10.2" data-path="how-to-deal-with-class-imbalance.html"><a href="how-to-deal-with-class-imbalance.html#altering-case-weights"><i class="fa fa-check"></i><b>5.10.2</b> Altering case weights</a></li>
<li class="chapter" data-level="5.10.3" data-path="how-to-deal-with-class-imbalance.html"><a href="how-to-deal-with-class-imbalance.html#selecting-different-classification-score-cutoffs"><i class="fa fa-check"></i><b>5.10.3</b> Selecting different classification score cutoffs</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="dealing-with-correlated-predictors.html"><a href="dealing-with-correlated-predictors.html"><i class="fa fa-check"></i><b>5.11</b> Dealing with correlated predictors</a></li>
<li class="chapter" data-level="5.12" data-path="trees-and-forests-random-forests-in-action.html"><a href="trees-and-forests-random-forests-in-action.html"><i class="fa fa-check"></i><b>5.12</b> Trees and forests: Random forests in action</a><ul>
<li class="chapter" data-level="5.12.1" data-path="trees-and-forests-random-forests-in-action.html"><a href="trees-and-forests-random-forests-in-action.html#decision-trees"><i class="fa fa-check"></i><b>5.12.1</b> Decision trees</a></li>
<li class="chapter" data-level="5.12.2" data-path="trees-and-forests-random-forests-in-action.html"><a href="trees-and-forests-random-forests-in-action.html#trees-to-forests"><i class="fa fa-check"></i><b>5.12.2</b> Trees to forests</a></li>
<li class="chapter" data-level="5.12.3" data-path="trees-and-forests-random-forests-in-action.html"><a href="trees-and-forests-random-forests-in-action.html#variable-importance-1"><i class="fa fa-check"></i><b>5.12.3</b> Variable importance</a></li>
</ul></li>
<li class="chapter" data-level="5.13" data-path="logistic-regression-and-regularization.html"><a href="logistic-regression-and-regularization.html"><i class="fa fa-check"></i><b>5.13</b> Logistic regression and regularization</a><ul>
<li class="chapter" data-level="5.13.1" data-path="logistic-regression-and-regularization.html"><a href="logistic-regression-and-regularization.html#regularization-in-order-to-avoid-overfitting"><i class="fa fa-check"></i><b>5.13.1</b> Regularization in order to avoid overfitting</a></li>
<li class="chapter" data-level="5.13.2" data-path="logistic-regression-and-regularization.html"><a href="logistic-regression-and-regularization.html#variable-importance-2"><i class="fa fa-check"></i><b>5.13.2</b> Variable importance</a></li>
</ul></li>
<li class="chapter" data-level="5.14" data-path="other-supervised-algorithms.html"><a href="other-supervised-algorithms.html"><i class="fa fa-check"></i><b>5.14</b> Other supervised algorithms</a><ul>
<li class="chapter" data-level="5.14.1" data-path="other-supervised-algorithms.html"><a href="other-supervised-algorithms.html#gradient-boosting"><i class="fa fa-check"></i><b>5.14.1</b> Gradient boosting</a></li>
<li class="chapter" data-level="5.14.2" data-path="other-supervised-algorithms.html"><a href="other-supervised-algorithms.html#support-vector-machines-svm"><i class="fa fa-check"></i><b>5.14.2</b> Support Vector Machines (SVM)</a></li>
<li class="chapter" data-level="5.14.3" data-path="other-supervised-algorithms.html"><a href="other-supervised-algorithms.html#neural-networks-and-deep-versions-of-it"><i class="fa fa-check"></i><b>5.14.3</b> Neural networks and deep versions of it</a></li>
<li class="chapter" data-level="5.14.4" data-path="other-supervised-algorithms.html"><a href="other-supervised-algorithms.html#ensemble-learning"><i class="fa fa-check"></i><b>5.14.4</b> Ensemble learning</a></li>
</ul></li>
<li class="chapter" data-level="5.15" data-path="predicting-continuous-variables-regression-with-machine-learning.html"><a href="predicting-continuous-variables-regression-with-machine-learning.html"><i class="fa fa-check"></i><b>5.15</b> Predicting continuous variables: Regression with machine learning</a><ul>
<li class="chapter" data-level="5.15.1" data-path="predicting-continuous-variables-regression-with-machine-learning.html"><a href="predicting-continuous-variables-regression-with-machine-learning.html#use-case-predicting-age-from-dna-methylation"><i class="fa fa-check"></i><b>5.15.1</b> Use case: Predicting age from DNA methylation</a></li>
<li class="chapter" data-level="5.15.2" data-path="predicting-continuous-variables-regression-with-machine-learning.html"><a href="predicting-continuous-variables-regression-with-machine-learning.html#reading-and-processing-the-data"><i class="fa fa-check"></i><b>5.15.2</b> Reading and processing the data</a></li>
<li class="chapter" data-level="5.15.3" data-path="predicting-continuous-variables-regression-with-machine-learning.html"><a href="predicting-continuous-variables-regression-with-machine-learning.html#running-random-forest-regression"><i class="fa fa-check"></i><b>5.15.3</b> Running random forest regression</a></li>
</ul></li>
<li class="chapter" data-level="5.16" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>5.16</b> Exercises</a><ul>
<li class="chapter" data-level="5.16.1" data-path="exercises-3.html"><a href="exercises-3.html#classification"><i class="fa fa-check"></i><b>5.16.1</b> Classification</a></li>
<li class="chapter" data-level="5.16.2" data-path="exercises-3.html"><a href="exercises-3.html#regression"><i class="fa fa-check"></i><b>5.16.2</b> Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="genomicIntervals.html"><a href="genomicIntervals.html"><i class="fa fa-check"></i><b>6</b> Operations on Genomic Intervals and Genome Arithmetic</a><ul>
<li class="chapter" data-level="6.1" data-path="operations-on-genomic-intervals-with-genomicranges-package.html"><a href="operations-on-genomic-intervals-with-genomicranges-package.html"><i class="fa fa-check"></i><b>6.1</b> Operations on genomic intervals with <code>GenomicRanges</code> package</a><ul>
<li class="chapter" data-level="6.1.1" data-path="operations-on-genomic-intervals-with-genomicranges-package.html"><a href="operations-on-genomic-intervals-with-genomicranges-package.html#how-to-create-and-manipulate-a-granges-object"><i class="fa fa-check"></i><b>6.1.1</b> How to create and manipulate a GRanges object</a></li>
<li class="chapter" data-level="6.1.2" data-path="operations-on-genomic-intervals-with-genomicranges-package.html"><a href="operations-on-genomic-intervals-with-genomicranges-package.html#getting-genomic-regions-into-r-as-granges-objects"><i class="fa fa-check"></i><b>6.1.2</b> Getting genomic regions into R as GRanges objects</a></li>
<li class="chapter" data-level="6.1.3" data-path="operations-on-genomic-intervals-with-genomicranges-package.html"><a href="operations-on-genomic-intervals-with-genomicranges-package.html#finding-regions-that-dodo-not-overlap-with-another-set-of-regions"><i class="fa fa-check"></i><b>6.1.3</b> Finding regions that do/do not overlap with another set of regions</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="dealing-with-mapped-high-throughput-sequencing-reads.html"><a href="dealing-with-mapped-high-throughput-sequencing-reads.html"><i class="fa fa-check"></i><b>6.2</b> Dealing with mapped high-throughput sequencing reads</a><ul>
<li class="chapter" data-level="6.2.1" data-path="dealing-with-mapped-high-throughput-sequencing-reads.html"><a href="dealing-with-mapped-high-throughput-sequencing-reads.html#counting-mapped-reads-for-a-set-of-regions"><i class="fa fa-check"></i><b>6.2.1</b> Counting mapped reads for a set of regions</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="dealing-with-continuous-scores-over-the-genome.html"><a href="dealing-with-continuous-scores-over-the-genome.html"><i class="fa fa-check"></i><b>6.3</b> Dealing with continuous scores over the genome</a><ul>
<li class="chapter" data-level="6.3.1" data-path="dealing-with-continuous-scores-over-the-genome.html"><a href="dealing-with-continuous-scores-over-the-genome.html#extracting-subsections-of-rle-and-rlelist-objects"><i class="fa fa-check"></i><b>6.3.1</b> Extracting subsections of Rle and RleList objects</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="genomic-intervals-with-more-information-summarizedexperiment-class.html"><a href="genomic-intervals-with-more-information-summarizedexperiment-class.html"><i class="fa fa-check"></i><b>6.4</b> Genomic intervals with more information: SummarizedExperiment class</a><ul>
<li class="chapter" data-level="6.4.1" data-path="genomic-intervals-with-more-information-summarizedexperiment-class.html"><a href="genomic-intervals-with-more-information-summarizedexperiment-class.html#create-a-summarizedexperiment-object"><i class="fa fa-check"></i><b>6.4.1</b> Create a SummarizedExperiment object</a></li>
<li class="chapter" data-level="6.4.2" data-path="genomic-intervals-with-more-information-summarizedexperiment-class.html"><a href="genomic-intervals-with-more-information-summarizedexperiment-class.html#subset-and-manipulate-the-summarizedexperiment-object"><i class="fa fa-check"></i><b>6.4.2</b> Subset and manipulate the SummarizedExperiment object</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="visualizing-and-summarizing-genomic-intervals.html"><a href="visualizing-and-summarizing-genomic-intervals.html"><i class="fa fa-check"></i><b>6.5</b> Visualizing and summarizing genomic intervals</a><ul>
<li class="chapter" data-level="6.5.1" data-path="visualizing-and-summarizing-genomic-intervals.html"><a href="visualizing-and-summarizing-genomic-intervals.html#visualizing-intervals-on-a-locus-of-interest"><i class="fa fa-check"></i><b>6.5.1</b> Visualizing intervals on a locus of interest</a></li>
<li class="chapter" data-level="6.5.2" data-path="visualizing-and-summarizing-genomic-intervals.html"><a href="visualizing-and-summarizing-genomic-intervals.html#summaries-of-genomic-intervals-on-multiple-loci"><i class="fa fa-check"></i><b>6.5.2</b> Summaries of genomic intervals on multiple loci</a></li>
<li class="chapter" data-level="6.5.3" data-path="visualizing-and-summarizing-genomic-intervals.html"><a href="visualizing-and-summarizing-genomic-intervals.html#making-karyograms-and-circos-plots"><i class="fa fa-check"></i><b>6.5.3</b> Making karyograms and circos plots</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>6.6</b> Exercises</a><ul>
<li class="chapter" data-level="6.6.1" data-path="exercises-4.html"><a href="exercises-4.html#operations-on-genomic-intervals-with-the-genomicranges-package"><i class="fa fa-check"></i><b>6.6.1</b> Operations on genomic intervals with the <code>GenomicRanges</code> package</a></li>
<li class="chapter" data-level="6.6.2" data-path="exercises-4.html"><a href="exercises-4.html#dealing-with-mapped-high-throughput-sequencing-reads-1"><i class="fa fa-check"></i><b>6.6.2</b> Dealing with mapped high-throughput sequencing reads</a></li>
<li class="chapter" data-level="6.6.3" data-path="exercises-4.html"><a href="exercises-4.html#dealing-with-contiguous-scores-over-the-genome"><i class="fa fa-check"></i><b>6.6.3</b> Dealing with contiguous scores over the genome</a></li>
<li class="chapter" data-level="6.6.4" data-path="exercises-4.html"><a href="exercises-4.html#visualizing-and-summarizing-genomic-intervals-1"><i class="fa fa-check"></i><b>6.6.4</b> Visualizing and summarizing genomic intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="processingReads.html"><a href="processingReads.html"><i class="fa fa-check"></i><b>7</b> Quality Check, Processing and Alignment of High-throughput Sequencing Reads</a><ul>
<li class="chapter" data-level="7.1" data-path="fasta-and-fastq-formats.html"><a href="fasta-and-fastq-formats.html"><i class="fa fa-check"></i><b>7.1</b> FASTA and FASTQ formats</a></li>
<li class="chapter" data-level="7.2" data-path="quality-check-on-sequencing-reads.html"><a href="quality-check-on-sequencing-reads.html"><i class="fa fa-check"></i><b>7.2</b> Quality check on sequencing reads</a><ul>
<li class="chapter" data-level="7.2.1" data-path="quality-check-on-sequencing-reads.html"><a href="quality-check-on-sequencing-reads.html#sequence-quality-per-basecycle"><i class="fa fa-check"></i><b>7.2.1</b> Sequence quality per base/cycle</a></li>
<li class="chapter" data-level="7.2.2" data-path="quality-check-on-sequencing-reads.html"><a href="quality-check-on-sequencing-reads.html#sequence-content-per-basecycle"><i class="fa fa-check"></i><b>7.2.2</b> Sequence content per base/cycle</a></li>
<li class="chapter" data-level="7.2.3" data-path="quality-check-on-sequencing-reads.html"><a href="quality-check-on-sequencing-reads.html#read-frequency-plot"><i class="fa fa-check"></i><b>7.2.3</b> Read frequency plot</a></li>
<li class="chapter" data-level="7.2.4" data-path="quality-check-on-sequencing-reads.html"><a href="quality-check-on-sequencing-reads.html#other-quality-metrics-and-qc-tools"><i class="fa fa-check"></i><b>7.2.4</b> Other quality metrics and QC tools</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="filtering-and-trimming-reads.html"><a href="filtering-and-trimming-reads.html"><i class="fa fa-check"></i><b>7.3</b> Filtering and trimming reads</a></li>
<li class="chapter" data-level="7.4" data-path="mappingaligning-reads-to-the-genome.html"><a href="mappingaligning-reads-to-the-genome.html"><i class="fa fa-check"></i><b>7.4</b> Mapping/aligning reads to the genome</a></li>
<li class="chapter" data-level="7.5" data-path="further-processing-of-aligned-reads.html"><a href="further-processing-of-aligned-reads.html"><i class="fa fa-check"></i><b>7.5</b> Further processing of aligned reads</a></li>
<li class="chapter" data-level="7.6" data-path="exercises-5.html"><a href="exercises-5.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="rnaseqanalysis.html"><a href="rnaseqanalysis.html"><i class="fa fa-check"></i><b>8</b> RNA-seq Analysis</a><ul>
<li class="chapter" data-level="8.1" data-path="what-is-gene-expression.html"><a href="what-is-gene-expression.html"><i class="fa fa-check"></i><b>8.1</b> What is gene expression?</a></li>
<li class="chapter" data-level="8.2" data-path="methods-to-detect-gene-expression.html"><a href="methods-to-detect-gene-expression.html"><i class="fa fa-check"></i><b>8.2</b> Methods to detect gene expression</a></li>
<li class="chapter" data-level="8.3" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><i class="fa fa-check"></i><b>8.3</b> Gene expression analysis using high-throughput sequencing technologies</a><ul>
<li class="chapter" data-level="8.3.1" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#processing-raw-data"><i class="fa fa-check"></i><b>8.3.1</b> Processing raw data</a></li>
<li class="chapter" data-level="8.3.2" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#alignment"><i class="fa fa-check"></i><b>8.3.2</b> Alignment</a></li>
<li class="chapter" data-level="8.3.3" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#quantification"><i class="fa fa-check"></i><b>8.3.3</b> Quantification</a></li>
<li class="chapter" data-level="8.3.4" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#within-sample-normalization-of-the-read-counts"><i class="fa fa-check"></i><b>8.3.4</b> Within sample normalization of the read counts</a></li>
<li class="chapter" data-level="8.3.5" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#computing-different-normalization-schemes-in-r"><i class="fa fa-check"></i><b>8.3.5</b> Computing different normalization schemes in R</a></li>
<li class="chapter" data-level="8.3.6" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#exploratory-analysis-of-the-read-count-table"><i class="fa fa-check"></i><b>8.3.6</b> Exploratory analysis of the read count table</a></li>
<li class="chapter" data-level="8.3.7" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#differential-expression-analysis"><i class="fa fa-check"></i><b>8.3.7</b> Differential expression analysis</a></li>
<li class="chapter" data-level="8.3.8" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#functional-enrichment-analysis"><i class="fa fa-check"></i><b>8.3.8</b> Functional enrichment analysis</a></li>
<li class="chapter" data-level="8.3.9" data-path="gene-expression-analysis-using-high-throughput-sequencing-technologies.html"><a href="gene-expression-analysis-using-high-throughput-sequencing-technologies.html#accounting-for-additional-sources-of-variation"><i class="fa fa-check"></i><b>8.3.9</b> Accounting for additional sources of variation</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="other-applications-of-rna-seq.html"><a href="other-applications-of-rna-seq.html"><i class="fa fa-check"></i><b>8.4</b> Other applications of RNA-seq</a></li>
<li class="chapter" data-level="8.5" data-path="exercises-6.html"><a href="exercises-6.html"><i class="fa fa-check"></i><b>8.5</b> Exercises</a><ul>
<li class="chapter" data-level="8.5.1" data-path="exercises-6.html"><a href="exercises-6.html#exploring-the-count-tables"><i class="fa fa-check"></i><b>8.5.1</b> Exploring the count tables</a></li>
<li class="chapter" data-level="8.5.2" data-path="exercises-6.html"><a href="exercises-6.html#differential-expression-analysis-1"><i class="fa fa-check"></i><b>8.5.2</b> Differential expression analysis</a></li>
<li class="chapter" data-level="8.5.3" data-path="exercises-6.html"><a href="exercises-6.html#functional-enrichment-analysis-1"><i class="fa fa-check"></i><b>8.5.3</b> Functional enrichment analysis</a></li>
<li class="chapter" data-level="8.5.4" data-path="exercises-6.html"><a href="exercises-6.html#removing-unwanted-variation-from-the-expression-data"><i class="fa fa-check"></i><b>8.5.4</b> Removing unwanted variation from the expression data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chipseq.html"><a href="chipseq.html"><i class="fa fa-check"></i><b>9</b> ChIP-seq analysis</a><ul>
<li class="chapter" data-level="9.1" data-path="regulatory-protein-dna-interactions.html"><a href="regulatory-protein-dna-interactions.html"><i class="fa fa-check"></i><b>9.1</b> Regulatory protein-DNA interactions</a></li>
<li class="chapter" data-level="9.2" data-path="measuring-protein-dna-interactions-with-chip-seq.html"><a href="measuring-protein-dna-interactions-with-chip-seq.html"><i class="fa fa-check"></i><b>9.2</b> Measuring protein-DNA interactions with ChIP-seq</a></li>
<li class="chapter" data-level="9.3" data-path="factors-that-affect-chip-seq-experiment-and-analysis-quality.html"><a href="factors-that-affect-chip-seq-experiment-and-analysis-quality.html"><i class="fa fa-check"></i><b>9.3</b> Factors that affect ChIP-seq experiment and analysis quality</a><ul>
<li class="chapter" data-level="9.3.1" data-path="factors-that-affect-chip-seq-experiment-and-analysis-quality.html"><a href="factors-that-affect-chip-seq-experiment-and-analysis-quality.html#antibody-specificity"><i class="fa fa-check"></i><b>9.3.1</b> Antibody specificity</a></li>
<li class="chapter" data-level="9.3.2" data-path="factors-that-affect-chip-seq-experiment-and-analysis-quality.html"><a href="factors-that-affect-chip-seq-experiment-and-analysis-quality.html#sequencing-depth"><i class="fa fa-check"></i><b>9.3.2</b> Sequencing depth</a></li>
<li class="chapter" data-level="9.3.3" data-path="factors-that-affect-chip-seq-experiment-and-analysis-quality.html"><a href="factors-that-affect-chip-seq-experiment-and-analysis-quality.html#pcr-duplication"><i class="fa fa-check"></i><b>9.3.3</b> PCR duplication</a></li>
<li class="chapter" data-level="9.3.4" data-path="factors-that-affect-chip-seq-experiment-and-analysis-quality.html"><a href="factors-that-affect-chip-seq-experiment-and-analysis-quality.html#biological-replicates"><i class="fa fa-check"></i><b>9.3.4</b> Biological replicates</a></li>
<li class="chapter" data-level="9.3.5" data-path="factors-that-affect-chip-seq-experiment-and-analysis-quality.html"><a href="factors-that-affect-chip-seq-experiment-and-analysis-quality.html#control-experiments"><i class="fa fa-check"></i><b>9.3.5</b> Control experiments</a></li>
<li class="chapter" data-level="9.3.6" data-path="factors-that-affect-chip-seq-experiment-and-analysis-quality.html"><a href="factors-that-affect-chip-seq-experiment-and-analysis-quality.html#using-tagged-proteins"><i class="fa fa-check"></i><b>9.3.6</b> Using tagged proteins</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="pre-processing-chip-data.html"><a href="pre-processing-chip-data.html"><i class="fa fa-check"></i><b>9.4</b> Pre-processing ChIP data</a><ul>
<li class="chapter" data-level="9.4.1" data-path="pre-processing-chip-data.html"><a href="pre-processing-chip-data.html#mapping-of-chip-seq-data"><i class="fa fa-check"></i><b>9.4.1</b> Mapping of ChIP-seq data</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="chip-quality-control.html"><a href="chip-quality-control.html"><i class="fa fa-check"></i><b>9.5</b> ChIP quality control</a><ul>
<li class="chapter" data-level="9.5.1" data-path="chip-quality-control.html"><a href="chip-quality-control.html#the-data"><i class="fa fa-check"></i><b>9.5.1</b> The data</a></li>
<li class="chapter" data-level="9.5.2" data-path="chip-quality-control.html"><a href="chip-quality-control.html#sample-clustering"><i class="fa fa-check"></i><b>9.5.2</b> Sample clustering</a></li>
<li class="chapter" data-level="9.5.3" data-path="chip-quality-control.html"><a href="chip-quality-control.html#visualization-in-the-genome-browser"><i class="fa fa-check"></i><b>9.5.3</b> Visualization in the genome browser</a></li>
<li class="chapter" data-level="9.5.4" data-path="chip-quality-control.html"><a href="chip-quality-control.html#plus-and-minus-strand-cross-correlation"><i class="fa fa-check"></i><b>9.5.4</b> Plus and minus strand cross-correlation</a></li>
<li class="chapter" data-level="9.5.5" data-path="chip-quality-control.html"><a href="chip-quality-control.html#gc-bias-quantification"><i class="fa fa-check"></i><b>9.5.5</b> GC bias quantification</a></li>
<li class="chapter" data-level="9.5.6" data-path="chip-quality-control.html"><a href="chip-quality-control.html#sequence-read-genomic-distribution"><i class="fa fa-check"></i><b>9.5.6</b> Sequence read genomic distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="peak-calling.html"><a href="peak-calling.html"><i class="fa fa-check"></i><b>9.6</b> Peak calling</a><ul>
<li class="chapter" data-level="9.6.1" data-path="peak-calling.html"><a href="peak-calling.html#types-of-chip-seq-experiments"><i class="fa fa-check"></i><b>9.6.1</b> Types of ChIP-seq experiments</a></li>
<li class="chapter" data-level="9.6.2" data-path="peak-calling.html"><a href="peak-calling.html#peak-calling-sharp-peaks"><i class="fa fa-check"></i><b>9.6.2</b> Peak calling: Sharp peaks</a></li>
<li class="chapter" data-level="9.6.3" data-path="peak-calling.html"><a href="peak-calling.html#peak-calling-broad-regions"><i class="fa fa-check"></i><b>9.6.3</b> Peak calling: Broad regions</a></li>
<li class="chapter" data-level="9.6.4" data-path="peak-calling.html"><a href="peak-calling.html#peak-quality-control"><i class="fa fa-check"></i><b>9.6.4</b> Peak quality control</a></li>
<li class="chapter" data-level="9.6.5" data-path="peak-calling.html"><a href="peak-calling.html#peak-annotation"><i class="fa fa-check"></i><b>9.6.5</b> Peak annotation</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="motif-discovery.html"><a href="motif-discovery.html"><i class="fa fa-check"></i><b>9.7</b> Motif discovery</a><ul>
<li class="chapter" data-level="9.7.1" data-path="motif-discovery.html"><a href="motif-discovery.html#motif-comparison"><i class="fa fa-check"></i><b>9.7.1</b> Motif comparison</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="what-to-do-next.html"><a href="what-to-do-next.html"><i class="fa fa-check"></i><b>9.8</b> What to do next?</a></li>
<li class="chapter" data-level="9.9" data-path="exercises-7.html"><a href="exercises-7.html"><i class="fa fa-check"></i><b>9.9</b> Exercises</a><ul>
<li class="chapter" data-level="9.9.1" data-path="exercises-7.html"><a href="exercises-7.html#quality-control"><i class="fa fa-check"></i><b>9.9.1</b> Quality control</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bsseq.html"><a href="bsseq.html"><i class="fa fa-check"></i><b>10</b> DNA methylation analysis using bisulfite sequencing data</a><ul>
<li class="chapter" data-level="10.1" data-path="what-is-dna-methylation.html"><a href="what-is-dna-methylation.html"><i class="fa fa-check"></i><b>10.1</b> What is DNA methylation ?</a><ul>
<li class="chapter" data-level="10.1.1" data-path="what-is-dna-methylation.html"><a href="what-is-dna-methylation.html#how-dna-methylation-is-set"><i class="fa fa-check"></i><b>10.1.1</b> How DNA methylation is set ?</a></li>
<li class="chapter" data-level="10.1.2" data-path="what-is-dna-methylation.html"><a href="what-is-dna-methylation.html#how-to-measure-dna-methylation-with-bisulfite-sequencing"><i class="fa fa-check"></i><b>10.1.2</b> How to measure DNA methylation with bisulfite sequencing</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="analyzing-dna-methylation-data.html"><a href="analyzing-dna-methylation-data.html"><i class="fa fa-check"></i><b>10.2</b> Analyzing DNA methylation data</a></li>
<li class="chapter" data-level="10.3" data-path="processing-raw-data-and-getting-data-into-r.html"><a href="processing-raw-data-and-getting-data-into-r.html"><i class="fa fa-check"></i><b>10.3</b> Processing raw data and getting data into R</a></li>
<li class="chapter" data-level="10.4" data-path="data-filtering-and-exploratory-analysis.html"><a href="data-filtering-and-exploratory-analysis.html"><i class="fa fa-check"></i><b>10.4</b> Data filtering and exploratory analysis</a><ul>
<li class="chapter" data-level="10.4.1" data-path="data-filtering-and-exploratory-analysis.html"><a href="data-filtering-and-exploratory-analysis.html#reading-methylation-call-files"><i class="fa fa-check"></i><b>10.4.1</b> Reading methylation call files</a></li>
<li class="chapter" data-level="10.4.2" data-path="data-filtering-and-exploratory-analysis.html"><a href="data-filtering-and-exploratory-analysis.html#further-quality-check"><i class="fa fa-check"></i><b>10.4.2</b> Further quality check</a></li>
<li class="chapter" data-level="10.4.3" data-path="data-filtering-and-exploratory-analysis.html"><a href="data-filtering-and-exploratory-analysis.html#merging-samples-into-a-single-table"><i class="fa fa-check"></i><b>10.4.3</b> Merging samples into a single table</a></li>
<li class="chapter" data-level="10.4.4" data-path="data-filtering-and-exploratory-analysis.html"><a href="data-filtering-and-exploratory-analysis.html#filtering-cpgs"><i class="fa fa-check"></i><b>10.4.4</b> Filtering CpGs</a></li>
<li class="chapter" data-level="10.4.5" data-path="data-filtering-and-exploratory-analysis.html"><a href="data-filtering-and-exploratory-analysis.html#clustering-samples"><i class="fa fa-check"></i><b>10.4.5</b> Clustering samples</a></li>
<li class="chapter" data-level="10.4.6" data-path="data-filtering-and-exploratory-analysis.html"><a href="data-filtering-and-exploratory-analysis.html#principal-component-analysis-1"><i class="fa fa-check"></i><b>10.4.6</b> Principal component analysis</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="extracting-interesting-regions-differential-methylation-and-segmentation.html"><a href="extracting-interesting-regions-differential-methylation-and-segmentation.html"><i class="fa fa-check"></i><b>10.5</b> Extracting interesting regions: Differential methylation and segmentation</a><ul>
<li class="chapter" data-level="10.5.1" data-path="extracting-interesting-regions-differential-methylation-and-segmentation.html"><a href="extracting-interesting-regions-differential-methylation-and-segmentation.html#differential-methylation"><i class="fa fa-check"></i><b>10.5.1</b> Differential methylation</a></li>
<li class="chapter" data-level="10.5.2" data-path="extracting-interesting-regions-differential-methylation-and-segmentation.html"><a href="extracting-interesting-regions-differential-methylation-and-segmentation.html#methylation-segmentation"><i class="fa fa-check"></i><b>10.5.2</b> Methylation segmentation</a></li>
<li class="chapter" data-level="10.5.3" data-path="extracting-interesting-regions-differential-methylation-and-segmentation.html"><a href="extracting-interesting-regions-differential-methylation-and-segmentation.html#working-with-large-files"><i class="fa fa-check"></i><b>10.5.3</b> Working with large files</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="annotation-of-dmrsdmcs-and-segments.html"><a href="annotation-of-dmrsdmcs-and-segments.html"><i class="fa fa-check"></i><b>10.6</b> Annotation of DMRs/DMCs and segments</a><ul>
<li class="chapter" data-level="10.6.1" data-path="annotation-of-dmrsdmcs-and-segments.html"><a href="annotation-of-dmrsdmcs-and-segments.html#further-annotation-with-genes-or-gene-sets"><i class="fa fa-check"></i><b>10.6.1</b> Further annotation with genes or gene sets</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="other-r-packages-that-can-be-used-for-methylation-analysis.html"><a href="other-r-packages-that-can-be-used-for-methylation-analysis.html"><i class="fa fa-check"></i><b>10.7</b> Other R packages that can be used for methylation analysis</a></li>
<li class="chapter" data-level="10.8" data-path="exercises-8.html"><a href="exercises-8.html"><i class="fa fa-check"></i><b>10.8</b> Exercises</a><ul>
<li class="chapter" data-level="10.8.1" data-path="exercises-8.html"><a href="exercises-8.html#differential-methylation-1"><i class="fa fa-check"></i><b>10.8.1</b> Differential methylation</a></li>
<li class="chapter" data-level="10.8.2" data-path="exercises-8.html"><a href="exercises-8.html#methylome-segmentation"><i class="fa fa-check"></i><b>10.8.2</b> Methylome segmentation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="multiomics.html"><a href="multiomics.html"><i class="fa fa-check"></i><b>11</b> Multi-omics Analysis</a><ul>
<li class="chapter" data-level="11.1" data-path="use-case-multi-omics-data-from-colorectal-cancer.html"><a href="use-case-multi-omics-data-from-colorectal-cancer.html"><i class="fa fa-check"></i><b>11.1</b> Use case: Multi-omics data from colorectal cancer</a></li>
<li class="chapter" data-level="11.2" data-path="latent-variable-models-for-multi-omics-integration.html"><a href="latent-variable-models-for-multi-omics-integration.html"><i class="fa fa-check"></i><b>11.2</b> Latent variable models for multi-omics integration</a></li>
<li class="chapter" data-level="11.3" data-path="matrix-factorization-methods-for-unsupervised-multi-omics-data-integration.html"><a href="matrix-factorization-methods-for-unsupervised-multi-omics-data-integration.html"><i class="fa fa-check"></i><b>11.3</b> Matrix factorization methods for unsupervised multi-omics data integration</a><ul>
<li class="chapter" data-level="11.3.1" data-path="matrix-factorization-methods-for-unsupervised-multi-omics-data-integration.html"><a href="matrix-factorization-methods-for-unsupervised-multi-omics-data-integration.html#multiple-factor-analysis"><i class="fa fa-check"></i><b>11.3.1</b> Multiple factor analysis</a></li>
<li class="chapter" data-level="11.3.2" data-path="matrix-factorization-methods-for-unsupervised-multi-omics-data-integration.html"><a href="matrix-factorization-methods-for-unsupervised-multi-omics-data-integration.html#joint-non-negative-matrix-factorization"><i class="fa fa-check"></i><b>11.3.2</b> Joint non-negative matrix Factorization</a></li>
<li class="chapter" data-level="11.3.3" data-path="matrix-factorization-methods-for-unsupervised-multi-omics-data-integration.html"><a href="matrix-factorization-methods-for-unsupervised-multi-omics-data-integration.html#icluster"><i class="fa fa-check"></i><b>11.3.3</b> iCluster</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="clustering-using-latent-factors.html"><a href="clustering-using-latent-factors.html"><i class="fa fa-check"></i><b>11.4</b> Clustering using latent factors</a><ul>
<li class="chapter" data-level="11.4.1" data-path="clustering-using-latent-factors.html"><a href="clustering-using-latent-factors.html#one-hot-clustering"><i class="fa fa-check"></i><b>11.4.1</b> One-hot clustering</a></li>
<li class="chapter" data-level="11.4.2" data-path="clustering-using-latent-factors.html"><a href="clustering-using-latent-factors.html#k-means-clustering-1"><i class="fa fa-check"></i><b>11.4.2</b> K-means clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="biological-interpretation-of-latent-factors.html"><a href="biological-interpretation-of-latent-factors.html"><i class="fa fa-check"></i><b>11.5</b> Biological interpretation of latent factors</a><ul>
<li class="chapter" data-level="11.5.1" data-path="biological-interpretation-of-latent-factors.html"><a href="biological-interpretation-of-latent-factors.html#inspection-of-feature-weights-in-loading-vectors"><i class="fa fa-check"></i><b>11.5.1</b> Inspection of feature weights in loading vectors</a></li>
<li class="chapter" data-level="11.5.2" data-path="biological-interpretation-of-latent-factors.html"><a href="biological-interpretation-of-latent-factors.html#making-sense-of-factors-using-enrichment-analysis"><i class="fa fa-check"></i><b>11.5.2</b> Making sense of factors using enrichment analysis</a></li>
<li class="chapter" data-level="11.5.3" data-path="biological-interpretation-of-latent-factors.html"><a href="biological-interpretation-of-latent-factors.html#interpretation-using-additional-covariates"><i class="fa fa-check"></i><b>11.5.3</b> Interpretation using additional covariates</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="exercises-9.html"><a href="exercises-9.html"><i class="fa fa-check"></i><b>11.6</b> Exercises</a><ul>
<li class="chapter" data-level="11.6.1" data-path="exercises-9.html"><a href="exercises-9.html#matrix-factorization-methods"><i class="fa fa-check"></i><b>11.6.1</b> Matrix factorization methods</a></li>
<li class="chapter" data-level="11.6.2" data-path="exercises-9.html"><a href="exercises-9.html#clustering-using-latent-factors-1"><i class="fa fa-check"></i><b>11.6.2</b> Clustering using latent factors</a></li>
<li class="chapter" data-level="11.6.3" data-path="exercises-9.html"><a href="exercises-9.html#biological-interpretation-of-latent-factors-1"><i class="fa fa-check"></i><b>11.6.3</b> Biological interpretation of latent factors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Genomics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="other-supervised-algorithms" class="section level2">
<h2><span class="header-section-number">5.14</span> Other supervised algorithms</h2>
<p>We will next introduce a couple of other supervised algorithms for completeness but in less detail. These algorithms are also as popular as the others we introduced above and people who are interested in computational genomics see them used in the field for different problems. These algorithms also fit to the general framework of optimization of a cost/loss function. However, the approaches to the construction of the cost function and the cost function itself are different in each case.</p>
<div id="gradient-boosting" class="section level3">
<h3><span class="header-section-number">5.14.1</span> Gradient boosting</h3>
<p>Gradient boosting is a prediction model that uses an ensemble of decision trees similar to random forest. However, the decision trees are added sequentially, which is why these models are also called “Multiple Additive Regression Trees (MART)” <span class="citation">(Friedman and Meulman <a href="#ref-friedman2003mart" role="doc-biblioref">2003</a>)</span>. Apart from this, you will see similar methods called “Gradient boosting machines (GBM)”<span class="citation">(J. H. Friedman <a href="#ref-friedman2001gbm" role="doc-biblioref">2001</a>)</span> or “Boosted regression trees (BRT)” <span class="citation">(Elith, Leathwick, and Hastie <a href="#ref-elith2008brt" role="doc-biblioref">2008</a>)</span> in the literature.</p>
Generally, “boosting”  refers to an iterative learning approach where each new model tries to focus on data points where the previous ensemble of simple models did not predict well. Gradient boosting is an improvement over that, where each new model tries to focus on the residual errors (prediction error for the current ensemble of models) of the previous model. Specifically in gradient boosting, the simple models are trees. As in random forests, many trees are grown but in this case, trees are sequentially grown and each tree focuses on fixing the shortcomings of the previous trees. Figure <a href="other-supervised-algorithms.html#fig:GBMcartoon">5.13</a> shows this concept. One of the most widely used algorithms for gradient boosting is <code>XGboost</code> which stands for “extreme gradient boosting” <span class="citation">(Chen and Guestrin <a href="#ref-chen2016xgboost" role="doc-biblioref">2016</a>)</span>. Below we will demonstrate how to use this on our problem. <code>XGboost</code> as well as other gradient boosting methods has many parameters to regularize and optimize the complexity of the model. Finding the best parameters for your problem might take some time. However, this flexibility comes with benefits; methods depending on <code>XGboost</code> have won many machine learning competitions <span class="citation">(Chen and Guestrin <a href="#ref-chen2016xgboost" role="doc-biblioref">2016</a>)</span>.
<div class="figure" style="text-align: center"><span id="fig:GBMcartoon"></span>
<img src="images/ml-GBM-features.png" alt="Gradient boosting machines concept. Individual decision trees are built sequentially in order to fix the errors from the previous trees" width="70%" />
<p class="caption">
FIGURE 5.13: Gradient boosting machines concept. Individual decision trees are built sequentially in order to fix the errors from the previous trees
</p>
</div>
<p>The most important parameters are number of trees (<code>nrounds</code>), tree depth (<code>max_depth</code>), and learning rate or shrinkage (<code>eta</code>). Generally, the more trees we have, the better the algorithm will learn because each tree tries to fix classification errors that the previous tree ensemble could not perform. Having too many trees might cause overfitting. However, the learning rate parameter, eta, combats that by shrinking the contribution of each new tree. This can be set to lower values if you have many trees. You can either set a large number of trees and then tune the model with the learning rate parameter or set the learning rate low, say to <span class="math inline">\(0.01\)</span> or <span class="math inline">\(0.1\)</span> and tune the number of trees. Similarly, tree depth also controls for overfitting. The deeper the tree, the more usually it will overfit. This has to be tuned as well; the default is at 6. You can try to explore a range around the default. Apart from these, as in random forests, you can subsample the training data and/or the predictive variables. These strategies can also help you counter overfitting.</p>
<p>We are now going to use <code>XGboost</code> with the caret package on our cancer subtype classification problem. We are going to try different learning rate parameters. In this instance, we also subsample the dataset before we train each tree. The “subsample” parameter controls this and we set this to be 0.5, which means that before we train a tree we will sample 50% of the data and use only that portion to train the tree.</p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="other-supervised-algorithms.html#cb285-1"></a><span class="kw">library</span>(xgboost)</span>
<span id="cb285-2"><a href="other-supervised-algorithms.html#cb285-2"></a><span class="kw">set.seed</span>(<span class="dv">17</span>)</span>
<span id="cb285-3"><a href="other-supervised-algorithms.html#cb285-3"></a></span>
<span id="cb285-4"><a href="other-supervised-algorithms.html#cb285-4"></a><span class="co"># we will just set up 5-fold cross validation</span></span>
<span id="cb285-5"><a href="other-supervised-algorithms.html#cb285-5"></a>trctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,<span class="dt">number=</span><span class="dv">5</span>)</span>
<span id="cb285-6"><a href="other-supervised-algorithms.html#cb285-6"></a></span>
<span id="cb285-7"><a href="other-supervised-algorithms.html#cb285-7"></a><span class="co"># we will now train elastic net model</span></span>
<span id="cb285-8"><a href="other-supervised-algorithms.html#cb285-8"></a><span class="co"># it will try</span></span>
<span id="cb285-9"><a href="other-supervised-algorithms.html#cb285-9"></a>gbFit &lt;-<span class="st"> </span><span class="kw">train</span>(subtype<span class="op">~</span>., <span class="dt">data =</span> training, </span>
<span id="cb285-10"><a href="other-supervised-algorithms.html#cb285-10"></a>                 <span class="dt">method =</span> <span class="st">&quot;xgbTree&quot;</span>,</span>
<span id="cb285-11"><a href="other-supervised-algorithms.html#cb285-11"></a>                 <span class="dt">trControl=</span>trctrl,</span>
<span id="cb285-12"><a href="other-supervised-algorithms.html#cb285-12"></a>                 <span class="co"># paramters to try</span></span>
<span id="cb285-13"><a href="other-supervised-algorithms.html#cb285-13"></a>                 <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">nrounds=</span><span class="dv">200</span>,</span>
<span id="cb285-14"><a href="other-supervised-algorithms.html#cb285-14"></a>                                       <span class="dt">eta=</span><span class="kw">c</span>(<span class="fl">0.05</span>,<span class="fl">0.1</span>,<span class="fl">0.3</span>),</span>
<span id="cb285-15"><a href="other-supervised-algorithms.html#cb285-15"></a>                                       <span class="dt">max_depth=</span><span class="dv">4</span>,</span>
<span id="cb285-16"><a href="other-supervised-algorithms.html#cb285-16"></a>                                       <span class="dt">gamma=</span><span class="dv">0</span>,</span>
<span id="cb285-17"><a href="other-supervised-algorithms.html#cb285-17"></a>                                       <span class="dt">colsample_bytree=</span><span class="dv">1</span>,</span>
<span id="cb285-18"><a href="other-supervised-algorithms.html#cb285-18"></a>                                       <span class="dt">subsample=</span><span class="fl">0.5</span>,</span>
<span id="cb285-19"><a href="other-supervised-algorithms.html#cb285-19"></a>                                       <span class="dt">min_child_weight=</span><span class="dv">1</span>))</span>
<span id="cb285-20"><a href="other-supervised-algorithms.html#cb285-20"></a>                                       </span>
<span id="cb285-21"><a href="other-supervised-algorithms.html#cb285-21"></a></span>
<span id="cb285-22"><a href="other-supervised-algorithms.html#cb285-22"></a><span class="co"># best parameters by cross-validation accuracy</span></span>
<span id="cb285-23"><a href="other-supervised-algorithms.html#cb285-23"></a>gbFit<span class="op">$</span>bestTune</span></code></pre></div>
<pre><code>##   nrounds max_depth eta gamma colsample_bytree min_child_weight subsample
## 2     200         4 0.1     0                1                1       0.5</code></pre>
<p>Similar to random forests, we can estimate the variable importance for gradient boosting using the improvement in gini impurity or other performance-related metrics every time a variable is selected in a tree. Again, the <code>caret::varImp()</code> function can be used to plot the importance metrics.</p>

<div class="rmdtip">
<p><strong>Want to know more ?</strong></p>
<ul>
<li>More background on gradient boosting and XGboost:(<a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" class="uri">https://xgboost.readthedocs.io/en/latest/tutorials/model.html</a>). This explains the cost/loss function and regularization in more detail.</li>
<li>Lecture on Gradient boosting and random forests by Trevor Hastie: (<a href="https://youtu.be/wPqtzj5VZus" class="uri">https://youtu.be/wPqtzj5VZus</a>)</li>
</ul>
</div>

</div>
<div id="support-vector-machines-svm" class="section level3">
<h3><span class="header-section-number">5.14.2</span> Support Vector Machines (SVM)</h3>
<p>Support vector machines (SVM)  were popularized in the 90s due the efficiency and the performance of the algorithm <span class="citation">(Boser, Guyon, and Vapnik <a href="#ref-boser1992svm" role="doc-biblioref">1992</a>)</span>. The algorithm works by identifying the optimal decision boundary that separates the data points into different groups (or classes), and then predicts the class of new observations based on this separation boundary. Depending on the situation, the different groups might be separable by a linear straight line or by a non-linear boundary line or plane. If you review k-NN decision boundaries in Figure <a href="model-tuning-and-avoiding-overfitting.html#fig:kNNboundary">5.7</a>, you can see that the decision boundary is not linear. SVM can deal with linear or non-linear decision boundaries.</p>
First, SVM can map the data to higher dimensions where the decision boundary can be linear. This is achieved by applying certain mathematical functions, called “kernel functions”, to the predictor variable space. For example, a second-degree polynomial can be applied to predictor variables which creates new variables and in this new space the problem is linearly separable. Figure <a href="other-supervised-algorithms.html#fig:SVMcartoon">5.14</a> demonstrates this concept where points in feature space are mapped to quadratic space where linear separation is possible.
<div class="figure" style="text-align: center"><span id="fig:SVMcartoon"></span>
<img src="images/kernelSVM.png" alt="Support vector machine concept. With the help of a kernel function,points in feature space are mapped to higher dimensions where linear separation is possible." width="80%" />
<p class="caption">
FIGURE 5.14: Support vector machine concept. With the help of a kernel function,points in feature space are mapped to higher dimensions where linear separation is possible.
</p>
</div>
<p>Second, SVM not only tries to find a decision boundary, but tries to find the boundary with the largest buffer zone on the sides of the boundary. Having a boundary with a large buffer or “margin”, as it is formally called, will perform better for the new data points not used in the model training (margin is marked in Figure <a href="other-supervised-algorithms.html#fig:SVMcartoon">5.14</a> ). In addition, SVM calculates the decision boundary with some error toleration. As we have seen it may not always be possible to find a linear boundary that perfectly separates the classes. SVM tolerates some degree of error, as in data points on the wrong side of the decision boundary.</p>
<p>Another important feature of the algorithm is that SVM decides on the decision boundary by only relying on the “landmark” data points, formally known as “support vectors”. These are points that are closest to the decision boundary and harder to classify. By keeping track of such points only for decision boundary creation, the computational complexity of the algorithm is reduced. However, this depends on the margin or the buffer zone. If we have a large margin then there are many landmark points. The extent of the margin is also related to the variance-bias trade-off. If the allowed margin is small the classification will try to find a boundary that makes fewer errors in the training set therefore might overfit. If the margin is larger, it will tolerate more errors in the training set and might generalize better. Practically, this is controlled by the “C” or “Cost” parameter in the SVM example we will show below. Another important choice we will make is the kernel function. Below we use the radial basis kernel function. This function provides an extra predictor dimension where the problem is linearly separable. The model we will use has only one parameter, which is “C”. It is recommended that <span class="math inline">\(C\)</span> is in the form of <span class="math inline">\(2^k\)</span> where <span class="math inline">\(k\)</span> is in the range of -5 and 15 <span class="citation">(Hsu, Chang, Lin, et al. <a href="#ref-hsu2003practical" role="doc-biblioref">2003</a>)</span>. Another parameter that can be tuned is related to the radial basis function called “sigma”. A smaller sigma means less bias and more variance, while a larger sigma means less variance and more bias. Again, exponential sequences are recommended for tuning that <span class="citation">(Hsu, Chang, Lin, et al. <a href="#ref-hsu2003practical" role="doc-biblioref">2003</a>)</span>. We will set it to 1 for demonstration purposes below.</p>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="other-supervised-algorithms.html#cb287-1"></a><span class="co">#svm code here</span></span>
<span id="cb287-2"><a href="other-supervised-algorithms.html#cb287-2"></a><span class="kw">library</span>(kernlab)</span>
<span id="cb287-3"><a href="other-supervised-algorithms.html#cb287-3"></a><span class="kw">set.seed</span>(<span class="dv">17</span>)</span>
<span id="cb287-4"><a href="other-supervised-algorithms.html#cb287-4"></a></span>
<span id="cb287-5"><a href="other-supervised-algorithms.html#cb287-5"></a><span class="co"># we will just set up 5-fold cross validation</span></span>
<span id="cb287-6"><a href="other-supervised-algorithms.html#cb287-6"></a>trctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,<span class="dt">number=</span><span class="dv">5</span>)</span>
<span id="cb287-7"><a href="other-supervised-algorithms.html#cb287-7"></a></span>
<span id="cb287-8"><a href="other-supervised-algorithms.html#cb287-8"></a><span class="co"># we will now train elastic net model</span></span>
<span id="cb287-9"><a href="other-supervised-algorithms.html#cb287-9"></a><span class="co"># it will try</span></span>
<span id="cb287-10"><a href="other-supervised-algorithms.html#cb287-10"></a>svmFit &lt;-<span class="st"> </span><span class="kw">train</span>(subtype<span class="op">~</span>., <span class="dt">data =</span> training, </span>
<span id="cb287-11"><a href="other-supervised-algorithms.html#cb287-11"></a>                <span class="co"># this SVM used radial basis function</span></span>
<span id="cb287-12"><a href="other-supervised-algorithms.html#cb287-12"></a>                 <span class="dt">method =</span> <span class="st">&quot;svmRadial&quot;</span>, </span>
<span id="cb287-13"><a href="other-supervised-algorithms.html#cb287-13"></a>                 <span class="dt">trControl=</span>trctrl,</span>
<span id="cb287-14"><a href="other-supervised-algorithms.html#cb287-14"></a>                <span class="dt">tuneGrid=</span><span class="kw">data.frame</span>(<span class="dt">C=</span><span class="kw">c</span>(<span class="fl">0.25</span>,<span class="fl">0.5</span>,<span class="dv">1</span>),</span>
<span id="cb287-15"><a href="other-supervised-algorithms.html#cb287-15"></a>                                    <span class="dt">sigma=</span><span class="dv">1</span>))</span></code></pre></div>

<div class="rmdtip">
<p><strong>Want to know more ?</strong></p>
<ul>
<li>MIT lecture by Patrick Winston on SVM: <a href="https://youtu.be/_PwhiWxHK8o" class="uri">https://youtu.be/_PwhiWxHK8o</a>. This lecture explains the concept with some mathematical background. It is not hard to follow. You should be able to follow this if you know what vectors are and if you have some knowledge on derivatives and basic algebra.</li>
<li>Online demo for SVM: (<a href="https://cs.stanford.edu/people/karpathy/svmjs/demo/" class="uri">https://cs.stanford.edu/people/karpathy/svmjs/demo/</a>). You can play with sigma and C parameters for radial basis SVM and see how they affect the decision boundary.</li>
</ul>
</div>

</div>
<div id="neural-networks-and-deep-versions-of-it" class="section level3">
<h3><span class="header-section-number">5.14.3</span> Neural networks and deep versions of it</h3>
<p>Neural networks  are another popular machine learning method which is recently regaining popularity. The earlier versions of the algorithm were popularized in the 80s and 90s. The advantage of neural networks is like SVM, they can model non-linear decision boundaries. The basic idea of neural networks is to combine the predictor variables in order to model the response variable as a non-linear function. In a neural network, input variables pass through several layers that combine the variables and transform those combinations and recombine outputs depending on how many layers the network has. In the conceptual example in Figure <a href="other-supervised-algorithms.html#fig:neuralNetDiagram">5.15</a> the input nodes receive predictor variables and make linear combinations of them in the form of <span class="math inline">\(\sum ( w_ixi +b)\)</span>. Simply put, the variables are multiplied with weights and summed up. This is what we call “linear combination”. These quantities are further fed into another layer called the hidden layer where an activation function is applied on the sums. And these results are further fed into an output node which outputs class probabilities assuming we are working on a classification algorithm. There could be many more hidden layers that will even further combine the output from hidden layers before them. The algorithm in the end also has a cost function similar to the logistic regression cost function, but it now has to estimate all the weight parameters: <span class="math inline">\(w_i\)</span>. This is a more complicated problem than logistic regression because of the number of parameters to be estimated but neural networks are able to fit complex functions due their parameter space flexibility as well.</p>
<div class="figure" style="text-align: center"><span id="fig:neuralNetDiagram"></span>
<img src="images/neuralNetDiagram.png" alt="Diagram for a simple neural network, their combinations pass through hidden layers and are combined again for the output. Predictor variables are fed to the network and weights are adjusted to optimize the cost function" width="80%" />
<p class="caption">
FIGURE 5.15: Diagram for a simple neural network, their combinations pass through hidden layers and are combined again for the output. Predictor variables are fed to the network and weights are adjusted to optimize the cost function
</p>
</div>
<p>In a practical sense, the number of nodes in the hidden layer (size) and some regularization on the weights can be applied to control for overfitting. This is called the calculated (decay) parameter controls for overfitting.</p>
<p>We will train a simple neural network on our cancer data set. In this simple example, the network architecture is somewhat fixed. We can only the choose number of nodes (denoted by “size”) in the hidden layer and a regularization parameter (denoted by “decay”). Increasing the number of nodes in hidden layer or in other implementations increasing the number of the hidden layers, will help model non-linear relationships but can overfit. One way to combat that is to limit the number of nodes in the hidden layer; another way is to regularize the weights. The decay parameter does just that, it penalizes the loss function by <span class="math inline">\(decay(weigths^2)\)</span>. In the example below, we try 1 or 2 nodes in the hidden layer in the interest of simplicity and run-time. In addition, we set <code>decay=0</code>, which will correspond to not doing any regularization.</p>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="other-supervised-algorithms.html#cb288-1"></a><span class="co">#svm code here</span></span>
<span id="cb288-2"><a href="other-supervised-algorithms.html#cb288-2"></a><span class="kw">library</span>(nnet)</span>
<span id="cb288-3"><a href="other-supervised-algorithms.html#cb288-3"></a><span class="kw">set.seed</span>(<span class="dv">17</span>)</span>
<span id="cb288-4"><a href="other-supervised-algorithms.html#cb288-4"></a></span>
<span id="cb288-5"><a href="other-supervised-algorithms.html#cb288-5"></a><span class="co"># we will just set up 5-fold cross validation</span></span>
<span id="cb288-6"><a href="other-supervised-algorithms.html#cb288-6"></a>trctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,<span class="dt">number=</span><span class="dv">5</span>)</span>
<span id="cb288-7"><a href="other-supervised-algorithms.html#cb288-7"></a></span>
<span id="cb288-8"><a href="other-supervised-algorithms.html#cb288-8"></a><span class="co"># we will now train neural net model</span></span>
<span id="cb288-9"><a href="other-supervised-algorithms.html#cb288-9"></a><span class="co"># it will try</span></span>
<span id="cb288-10"><a href="other-supervised-algorithms.html#cb288-10"></a>nnetFit &lt;-<span class="st"> </span><span class="kw">train</span>(subtype<span class="op">~</span>., <span class="dt">data =</span> training, </span>
<span id="cb288-11"><a href="other-supervised-algorithms.html#cb288-11"></a>                 <span class="dt">method =</span> <span class="st">&quot;nnet&quot;</span>,</span>
<span id="cb288-12"><a href="other-supervised-algorithms.html#cb288-12"></a>                 <span class="dt">trControl=</span>trctrl,</span>
<span id="cb288-13"><a href="other-supervised-algorithms.html#cb288-13"></a>                 <span class="dt">tuneGrid=</span><span class="kw">data.frame</span>(<span class="dt">size=</span><span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,<span class="dt">decay=</span><span class="dv">0</span></span>
<span id="cb288-14"><a href="other-supervised-algorithms.html#cb288-14"></a>                                      ),</span>
<span id="cb288-15"><a href="other-supervised-algorithms.html#cb288-15"></a>                 <span class="co"># this is maximum number of weights</span></span>
<span id="cb288-16"><a href="other-supervised-algorithms.html#cb288-16"></a>                 <span class="co"># needed for the nnet method</span></span>
<span id="cb288-17"><a href="other-supervised-algorithms.html#cb288-17"></a>                 <span class="dt">MaxNWts=</span><span class="dv">2000</span>) </span></code></pre></div>
<p>The example we used above is a bit outdated. The modern “deep” neural networks provide much more flexibility in the number of nodes, number of layers and regularization options. In many areas, especially computer vision deep neural networks are the state-of-the-art <span class="citation">(LeCun, Bengio, and Hinton <a href="#ref-lecun2015deep" role="doc-biblioref">2015</a>)</span>. These modern implementations of neural networks are available in R via the <code>keras</code> package and can also be trained via the <code>caret</code> package with the similar interface we have shown until now.</p>

<div class="rmdtip">
<p><strong>Want to know more ?</strong></p>
<ul>
<li>Deep neural networks in R: (<a href="https://keras.rstudio.com/" class="uri">https://keras.rstudio.com/</a>). There are examples and background information on deep neural networks.</li>
<li>Online demo for neural networks: (<a href="https://cs.stanford.edu/~karpathy/svmjs/demo/demonn.html" class="uri">https://cs.stanford.edu/~karpathy/svmjs/demo/demonn.html</a>). You can see the effect of the number of hidden layers and number of nodes on the decision boundary.</li>
</ul>
</div>

</div>
<div id="ensemble-learning" class="section level3">
<h3><span class="header-section-number">5.14.4</span> Ensemble learning</h3>
<p>Ensemble learning models are simply combinations of different machine learning models. By now, we already introduced the concept of ensemble learning in random forests and gradient boosting. However, this concept can be generalized to combining all kinds of different models. “Random forests” is an ensemble of the same type of models, decision trees. We can also have ensembles of different types of models. For example, we can combine random forest, k-NN and elastic net models, and make class predictions based on the votes from those different models. Below, we are showing how to do this. We are going to get predictions for three different models on the test set, use majority voting to decide on the class label, and then check performance using <code>caret::confusionMatrix()</code>.</p>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="other-supervised-algorithms.html#cb289-1"></a><span class="co"># predict with k-NN model</span></span>
<span id="cb289-2"><a href="other-supervised-algorithms.html#cb289-2"></a>knnPred=<span class="kw">as.character</span>(<span class="kw">predict</span>(knnFit,testing[,<span class="op">-</span><span class="dv">1</span>],<span class="dt">type=</span><span class="st">&quot;class&quot;</span>))</span>
<span id="cb289-3"><a href="other-supervised-algorithms.html#cb289-3"></a><span class="co"># predict with elastic Net model</span></span>
<span id="cb289-4"><a href="other-supervised-algorithms.html#cb289-4"></a>enetPred=<span class="kw">as.character</span>(<span class="kw">predict</span>(enetFit,testing[,<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb289-5"><a href="other-supervised-algorithms.html#cb289-5"></a><span class="co"># predict with random forest model</span></span>
<span id="cb289-6"><a href="other-supervised-algorithms.html#cb289-6"></a>rfPred=<span class="kw">as.character</span>(<span class="kw">predict</span>(rfFit,testing[,<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb289-7"><a href="other-supervised-algorithms.html#cb289-7"></a></span>
<span id="cb289-8"><a href="other-supervised-algorithms.html#cb289-8"></a><span class="co"># do voting for class labels</span></span>
<span id="cb289-9"><a href="other-supervised-algorithms.html#cb289-9"></a><span class="co"># code finds the most frequent class label per row</span></span>
<span id="cb289-10"><a href="other-supervised-algorithms.html#cb289-10"></a>votingPred=<span class="kw">apply</span>(<span class="kw">cbind</span>(knnPred,enetPred,rfPred),<span class="dv">1</span>,</span>
<span id="cb289-11"><a href="other-supervised-algorithms.html#cb289-11"></a>                 <span class="cf">function</span>(x) <span class="kw">names</span>(<span class="kw">which.max</span>(<span class="kw">table</span>(x))))</span>
<span id="cb289-12"><a href="other-supervised-algorithms.html#cb289-12"></a></span>
<span id="cb289-13"><a href="other-supervised-algorithms.html#cb289-13"></a><span class="co"># check accuracy</span></span>
<span id="cb289-14"><a href="other-supervised-algorithms.html#cb289-14"></a><span class="kw">confusionMatrix</span>(<span class="dt">data=</span>testing[,<span class="dv">1</span>],</span>
<span id="cb289-15"><a href="other-supervised-algorithms.html#cb289-15"></a>                <span class="dt">reference=</span><span class="kw">as.factor</span>(votingPred))<span class="op">$</span>overall[<span class="dv">1</span>]</span></code></pre></div>
<pre><code>##  Accuracy 
## 0.9814815</code></pre>
<p>In the test set, we were able to obtain perfect accuracy after voting. More complicated and accurate ways to build ensembles exist. We could also use the mean of class probabilities instead of voting for final class predictions. We can even combine models in a regression based scheme to assign weights to the votes or to the predicted class probabilities of each model. In these cases, the prediction performance of the ensembles can also be tested with sampling techniques such as cross-validation. You can think of this as another layer of optimization or modeling for combining results from different models. We will not pursue this further in this chapter but packages such as <a href="https://cran.r-project.org/web/packages/caretEnsemble/"><code>caretEnsemble</code></a>, <a href="https://cran.r-project.org/web/packages/SuperLearner/index.html"><code>SuperLearner</code></a> or <a href="https://mlr.mlr-org.com/"><code>mlr</code></a> can combine models in various ways described above. 

</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-boser1992svm">
<p>Boser, Guyon, and Vapnik. 1992. “A Training Algorithm for Optimal Margin Classifiers.” In <em>Proceedings of the Fifth Annual Workshop on Computational Learning Theory</em>, 144–52. ACM.</p>
</div>
<div id="ref-chen2016xgboost">
<p>Chen, and Guestrin. 2016. “Xgboost: A Scalable Tree Boosting System.” In <em>Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining</em>, 785–94. ACM.</p>
</div>
<div id="ref-elith2008brt">
<p>Elith, Leathwick, and Hastie. 2008. “A Working Guide to Boosted Regression Trees.” <em>Journal of Animal Ecology</em> 77 (4): 802–13.</p>
</div>
<div id="ref-friedman2001gbm">
<p>Friedman. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” <em>Annals of Statistics</em>, 1189–1232.</p>
</div>
<div id="ref-friedman2003mart">
<p>Friedman, and Meulman. 2003. “Multiple Additive Regression Trees with Application in Epidemiology.” <em>Statistics in Medicine</em> 22 (9): 1365–81.</p>
</div>
<div id="ref-hsu2003practical">
<p>Hsu, Chang, Lin, and others. 2003. “A Practical Guide to Support Vector Classification.”</p>
</div>
<div id="ref-lecun2015deep">
<p>LeCun, Bengio, and Hinton. 2015. “Deep Learning.” <em>Nature</em> 521 (7553): 436.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="logistic-regression-and-regularization.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="predicting-continuous-variables-regression-with-machine-learning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/compgenomr/book/edit/master/05-supervisedLearning.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
