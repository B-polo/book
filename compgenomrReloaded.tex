% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
  12pt,
  numberinsequence,krantz2]{krantz}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
  \setmonofont[Scale=0.7]{Source Code Pro}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Computational Genomics with R},
  pdfauthor={Altuna Akalin},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=Blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[bf,singlelinecheck=off]{caption}
\usepackage{graphicx}

\setmainfont[UprightFeatures={SmallCapsFont=AlegreyaSC-Regular}]{Alegreya}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\ifxetex
  \usepackage{letltxmacro}
  \setlength{\XeTeXLinkMargin}{1pt}
  \LetLtxMacro\SavedIncludeGraphics\includegraphics
  \def\includegraphics#1#{% #1 catches optional stuff (star/opt. arg.)
    \IncludeGraphicsAux{#1}%
  }%
  \newcommand*{\IncludeGraphicsAux}[2]{%
    \XeTeXLinkBox{%
      \SavedIncludeGraphics#1{#2}%
    }%
  }%
\fi

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\makeatletter
\@ifundefined{Shaded}{
}{\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}}
\makeatother

\newenvironment{rmdblock}[1]
  {
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.7\height}[0pt][0pt]{
      {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/#1}}
    }
  }
  \setlength{\fboxsep}{1em}
  \begin{kframe}
  \item
  }
  {
  \end{kframe}
  \end{itemize}
  }
\newenvironment{rmdnote}
  {\begin{rmdblock}{note}}
  {\end{rmdblock}}
\newenvironment{rmdcaution}
  {\begin{rmdblock}{caution}}
  {\end{rmdblock}}
\newenvironment{rmdimportant}
  {\begin{rmdblock}{important}}
  {\end{rmdblock}}
\newenvironment{rmdtip}
  {\begin{rmdblock}{tip}}
  {\end{rmdblock}}
\newenvironment{rmdwarning}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\frontmatter
\usepackage[]{natbib}
\bibliographystyle{apalike.bst}

\title{Computational Genomics with R}
\author{Altuna Akalin}
\date{2020-09-24}

\begin{document}
\maketitle

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}
\begin{center}
\includegraphics{images/dedication.pdf}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}


The aim of this book is to provide the fundamentals for data analysis for genomics. We developed this book based on the computational genomics courses we are giving every year. We have had invariably an interdisciplinary audience with backgrounds from physics, biology, medicine, math, computer science or other quantitative fields. We want this book to be a starting point for computational genomics students and a guide for further data analysis in more specific topics in genomics. This is why we tried to cover a large variety of topics from programming to basic genome biology. As the field is interdisciplinary, it requires different starting points for people with different backgrounds. A biologist might skip sections on basic genome biology and start with R programming, whereas a computer scientist might want to start with genome biology. In the same manner, a more experienced person might want to refer to this book when s/he needs to do a certain type of analysis which s/he does not have prior experience.

\includegraphics{images/by-nc-sa.png}\\
The online version of this book is licensed under the \href{http://creativecommons.org/licenses/by-nc-sa/4.0/}{Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License}.

\hypertarget{who-is-this-book-for}{%
\section*{Who is this book for?}\label{who-is-this-book-for}}


The book contains practical and theoretical aspects of computational genomics. Biology and
medicine generate more data than ever before. Therefore, we need to educate more people with data
analysis skills and understanding of computational genomics.
Since computational genomics is interdisciplinary, this book aims to be accessible for
biologists, medical scientists, computer scientists and people from other quantitative backgrounds. We wrote this book for the following audiences:

\begin{itemize}
\tightlist
\item
  Biologists and medical scientists who generate the data and are keen on analyzing it themselves.
\item
  Students and researchers who are formally starting to do research on or using computational genomics do not have extensive domain-specific knowledge, but have at least a beginner-level understanding in a quantitative field, for example, math, stats.
\item
  Experienced researchers looking for recipes or quick how-tos to get started in specific data analysis tasks related to computational genomics.
\end{itemize}

\hypertarget{what-will-you-get-out-of-this}{%
\subsection*{What will you get out of this?}\label{what-will-you-get-out-of-this}}


This resource describes the skills and provides how-tos that will help readers
analyze their own genomics data.

After reading:

\begin{itemize}
\tightlist
\item
  If you are not familiar with R, you will get the basics of R and dive right in to specialized uses of R for computational genomics.
\item
  You will understand genomic intervals and operations on them, such as overlap.
\item
  You will be able to use R and its vast package library to do sequence analysis, such as calculating GC content for given segments of a genome or find transcription factor binding sites.
\item
  You will be familiar with visualization techniques used in genomics, such as heatmaps, meta-gene plots, and genomic track visualization.
\item
  You will be familiar with supervised and unsupervised learning techniques which are important in data modeling and exploratory analysis of high-dimensional data.
\item
  You will be familiar with analysis of different high-throughput sequencing datasets (RNA-seq, ChIP-seq, BS-seq and multi-omics integration) mostly using R-based tools.
\end{itemize}

\hypertarget{structure-of-the-book}{%
\section*{Structure of the book}\label{structure-of-the-book}}


The book is designed with the idea that practical and conceptual
understanding of data analysis methods is as important, if not more important, than the theoretical understanding, such as detailed derivation of equations in statistics or machine learning. That is why we first try to give a conceptual explanation of the concepts then we try to give essential parts of the mathematical formulas for more detailed understanding. In this spirit, we always show the code and
explain the code for a particular data analysis task. We also give additional references such as books, websites , video lectures and scientific papers for readers who desire to gain deeper theoretical understanding of data-analysis-related methods or concepts.

Chapter \ref{intro}: ``Introduction to Genomics'' introduces the basic concepts in genome biology and genomics. Understanding these concepts is important for computational genomics.

Chapter \ref{Rintro}: ``Introduction to R for Genomic Data Analysis'' provides the basic R skills necessary to follow the book in addition to common data analysis paradigms we observe in genomic data analysis. Chapter \ref{stats}: ``Statistics for Genomics'', Chapter \ref{unsupervisedLearning}: ``Exploratory Data Analysis with Unsupervised Machine Learning'' and Chapter \ref{supervisedLearning}: ``Predictive Modeling with Supervised Machine Learning'' introduce the necessary quantitative skills that one will need when analyzing high-dimensional genomics data.

Chapter \ref{genomicIntervals}: ``Operations on Genomic Intervals and Genome Arithmetic'' introduces the fundamental tools for dealing with genomic intervals and their relationship to each other over the genome. In addition, the chapter introduces a variety of genomic data visualization methods. The skills introduced in this chapter are key skills that are needed to work with processed genomic data which are available through public databases such as Ensembl and the UCSC browser.

The next chapters deal with specific analysis of high-throughput sequencing data and integrating different kinds of datasets. Chapter \ref{processingReads}: ``Quality Check, Processing and Alignment of High-throughput Sequencing Reads'' introduces quality checks that need to be done on sequencing reads and different ways to process them further. Chapters \ref{rnaseqanalysis}, \ref{chipseq} and \ref{bsseq} deal with RNA-seq analysis, ChIP-seq analysis and BS-seq analysis. The last chapter, Chapter \ref{multiomics}:``Multi-omics Analysis'' deals with methods for integrating multiple omics datasets.

Most chapters have exercises that reinforce some of the important points introduced in the chapters. The exercises are classified into Beginner, Intermediate and Advanced categories. If you are well versed in a certain subject you might want to skip Beginner-level exercises.

To sum it up, this book is a comprehensive guide for computational genomics. Some sections are there for the sake of the wide interdisciplinary audience and completeness, and not all sections will be equally useful to all readers of this broad audience.

\hypertarget{software-information-and-conventions}{%
\section*{Software information and conventions}\label{software-information-and-conventions}}


Package names and inline code and file names are formatted in a typewriter font (e.g.~\texttt{methylKit}). Function names are followed by parentheses (e.g.~\texttt{genomation::ScoreMatrix()}). The double-colon operator \texttt{::} means accessing an object from a package.

\hypertarget{assignment-operator-convention}{%
\subsection*{Assignment operator convention}\label{assignment-operator-convention}}


Traditionally, \texttt{\textless{}-} is the preferred assignment operator. However, throughout the book we use \texttt{=} and \texttt{\textless{}-} as the assignment operator interchangeably.

\hypertarget{packages-needed-to-run-the-book-code}{%
\subsection*{Packages needed to run the book code}\label{packages-needed-to-run-the-book-code}}


This book is primarily about using R packages to analyze genomics data, therefore if you want to reproduce the analysis in this book you need to install the relevant packages in each chapter using \texttt{install.packages} or \texttt{BiocManager::install} functions. In each chapter, we load the necessary packages with the \texttt{library()} or \texttt{require()} function when we use the needed functions from respective packages. By looking at calls, you can see which packages are needed for that code chunk or chapter. If you need to install all the package dependencies for the book, you can run the following command and have a cup of tea while waiting.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{requireNamespace}\NormalTok{(}\StringTok{"BiocManager"}\NormalTok{, }\DataTypeTok{quietly =} \OtherTok{TRUE}\NormalTok{))}
    \KeywordTok{install.packages}\NormalTok{(}\StringTok{"BiocManager"}\NormalTok{)}
\NormalTok{BiocManager}\OperatorTok{::}\KeywordTok{install}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{'qvalue'}\NormalTok{,}\StringTok{'plot3D'}\NormalTok{,}\StringTok{'ggplot2'}\NormalTok{,}\StringTok{'pheatmap'}\NormalTok{,}\StringTok{'cowplot'}\NormalTok{,}
                      \StringTok{'cluster'}\NormalTok{, }\StringTok{'NbClust'}\NormalTok{, }\StringTok{'fastICA'}\NormalTok{, }\StringTok{'NMF'}\NormalTok{,}\StringTok{'matrixStats'}\NormalTok{,}
                      \StringTok{'Rtsne'}\NormalTok{, }\StringTok{'mosaic'}\NormalTok{, }\StringTok{'knitr'}\NormalTok{, }\StringTok{'genomation'}\NormalTok{,}
                      \StringTok{'ggbio'}\NormalTok{, }\StringTok{'Gviz'}\NormalTok{, }\StringTok{'DESeq2'}\NormalTok{, }\StringTok{'RUVSeq'}\NormalTok{,}
                      \StringTok{'gProfileR'}\NormalTok{, }\StringTok{'ggfortify'}\NormalTok{, }\StringTok{'corrplot'}\NormalTok{,}
                      \StringTok{'gage'}\NormalTok{, }\StringTok{'EDASeq'}\NormalTok{, }\StringTok{'citr'}\NormalTok{, }\StringTok{'formatR'}\NormalTok{,}
                      \StringTok{'svglite'}\NormalTok{, }\StringTok{'Rqc'}\NormalTok{, }\StringTok{'ShortRead'}\NormalTok{, }\StringTok{'QuasR'}\NormalTok{,}
                      \StringTok{'methylKit'}\NormalTok{,}\StringTok{'FactoMineR'}\NormalTok{, }\StringTok{'iClusterPlus'}\NormalTok{,}
                      \StringTok{'enrichR'}\NormalTok{,}\StringTok{'caret'}\NormalTok{,}\StringTok{'xgboost'}\NormalTok{,}\StringTok{'glmnet'}\NormalTok{,}
                      \StringTok{'DALEX'}\NormalTok{,}\StringTok{'kernlab'}\NormalTok{,}\StringTok{'pROC'}\NormalTok{,}\StringTok{'nnet'}\NormalTok{,}\StringTok{'RANN'}\NormalTok{,}
                      \StringTok{'ranger'}\NormalTok{,}\StringTok{'GenomeInfoDb'}\NormalTok{, }\StringTok{'GenomicRanges'}\NormalTok{,}
                      \StringTok{'GenomicAlignments'}\NormalTok{, }\StringTok{'ComplexHeatmap'}\NormalTok{, }\StringTok{'circlize'}\NormalTok{, }
                      \StringTok{'rtracklayer'}\NormalTok{, }\StringTok{'BSgenome.Hsapiens.UCSC.hg38'}\NormalTok{,}
                      \StringTok{'BSgenome.Hsapiens.UCSC.hg19'}\NormalTok{,}\StringTok{'tidyr'}\NormalTok{,}
                      \StringTok{'AnnotationHub'}\NormalTok{, }\StringTok{'GenomicFeatures'}\NormalTok{, }\StringTok{'normr'}\NormalTok{,}
                      \StringTok{'MotifDb'}\NormalTok{, }\StringTok{'TFBSTools'}\NormalTok{, }\StringTok{'rGADEM'}\NormalTok{, }\StringTok{'JASPAR2018'}
\NormalTok{                     ))}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-for-the-book}{%
\section*{Data for the book}\label{data-for-the-book}}


We rely on data from different R and Bioconductor packages throughout the book. For the datasets that do not ship with those packages, we created our own package \href{https://github.com/compgenomr/compGenomRData}{\textbf{compGenomRData}}. You can install this package via \texttt{devtools::install\_github("compgenomr/compGenomRData")}. We use the \texttt{system.file()} function to get the path to the files. We noticed many inexperienced users are confused about this function. This function just outputs the full path to the file that is installed with the data package.

\hypertarget{exercises-in-the-book}{%
\section*{Exercises in the book}\label{exercises-in-the-book}}


There is a set of exercises at the end of each chapter. The exercises are
separated in thematic sections that follow the major sections in the chapter.
In addition, each exercise is classified based on its difficulty as ``Beginner'',
``Intermediate'' and ``Advanced''. Beginner-level exercises can usually be done
by refactoring the code in the chapter. Advanced-level exercises usually require
a combination of code from different sections or chapters. The intermediate level
is somewhere in between. The solutions to the exercises are available at
\url{https://github.com/compgenomr/exercises}.

\hypertarget{reproducibility-statement}{%
\section*{Reproducibility statement}\label{reproducibility-statement}}


This book is compiled with R 4.0.0 and the following packages. We only list the main packages and their versions but not their dependencies.

\begin{verbatim}
## qvalue_2.20.0 | plot3D_1.3 | ggplot2_3.3.1 | pheatmap_1.0.12
## cowplot_1.0.0 | cluster_2.1.0 | NbClust_3.0 | fastICA_1.2.2
## NMF_0.23.0 | matrixStats_0.56.0 | Rtsne_0.15 | mosaic_1.7.0
## knitr_1.28 | genomation_1.20.0 | ggbio_1.36.0 | Gviz_1.32.0
## DESeq2_1.28.1 | RUVSeq_1.22.0 | gProfileR_0.7.0 | ggfortify_0.4.10
## corrplot_0.84 | gage_2.37.0 | EDASeq_2.22.0 | citr_0.3.2
## formatR_1.7 | svglite_1.2.3 | Rqc_1.22.0 | ShortRead_1.46.0
## QuasR_1.28.0 | methylKit_1.14.2 | FactoMineR_2.3 | iClusterPlus_1.24.0
## enrichR_2.1 | caret_6.0.86 | xgboost_1.0.0.2 | glmnet_4.0
## DALEX_1.2.1 | kernlab_0.9.29 | pROC_1.16.2 | nnet_7.3.14
## RANN_2.6.1 | ranger_0.12.1 | GenomeInfoDb_1.24.0 | GenomicRanges_1.40.0
## GenomicAlignments_1.24.0 | ComplexHeatmap_2.4.2 | circlize_0.4.9 | rtracklayer_1.48.0
## tidyr_1.1.0 | AnnotationHub_2.20.0 | GenomicFeatures_1.40.0 | normr_1.14.0
## MotifDb_1.30.0 | TFBSTools_1.26.0 | rGADEM_2.36.0 | JASPAR2018_1.1.1
## BSgenome.Hsapiens.UCSC.hg38_1.4.3 | BSgenome.Hsapiens.UCSC.hg19_1.4.3
\end{verbatim}

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}


I wish to thank the R and Bioconductor community for developing and maintaining libraries for genomic data analysis. Without their constant work and dedication, writing such a book would not be possible.

I also wish to thank all my past and present mentors, colleagues and employers.
The interaction with them provided the motivation to write such a book, and organize and teach hands-on courses on computational genomics.

I wish to thank John Kimmel, the editor from Chapman \& Hall/CRC, who helped me publish this book. It was a pleasure to work with him. He generously agreed to let me keep the online version of this book, so I can continue updating it after it is printed.

This has been a long journey for me. I started writing parts of this book as early as 2013. If it wasn't for Vedran Franke, Bora Uyar and Jonathan Ronen, it would have taken even longer. They kindly agreed to contribute the missing chapters and they did a great job. I am thankful for their contributions.

The following people kindly contributed fixes for typos and code, and various suggestions: Thomas Schalch, Alex Gosdschan, Rodrigo Ogava, Fei Zhao, Jonathan Kitt, Janani Ravi, Christian Schudoma, Samuel Sledzieski and Dania Hamo, Sarvesh Nikumbh.

\begin{flushright}
Altuna Akalin\\
Berlin, Germany
\end{flushright}

\hypertarget{about-the-authors}{%
\chapter*{About the Authors}\label{about-the-authors}}


\href{https://github.com/al2na}{\emph{Dr.~Altuna Akalin}} organized the book structure, wrote most of the book and edited the rest. Altuna is a bioinformatics scientist and the head of Bioinformatics and Omics Data Science Platform at the Berlin Institute of Medical Systems Biology, Max Delbrück Center in Berlin. He has been developing computational methods for analyzing and integrating large-scale genomics data sets since 2002. He is interested in using machine learning and statistics to uncover patterns related to important biological variables such as disease state and type. He lived in the USA, Norway, Turkey, Japan, and Switzerland in order to pursue research work and education related to computational genomics. The underlying aim of his current work is utilizing complex molecular signatures to provide decision support systems for disease diagnostics and biomarker discovery. In addition to the research efforts and managing a scientific lab, since 2015, he has been organizing and teaching at computational genomics courses in Berlin with participants from across the world. This book is mostly a result of material developed for those and previous teaching efforts at Weill Cornell Medical College in New York and Friedrich Miescher Institute in Basel, Switzerland.

Altuna Akalin and the following contributing authors have decades of combined experience in data analysis for genomics. They are developers of Bioconductor packages such as \href{https://bioconductor.org/packages/release/bioc/html/methylKit.html}{\textbf{methylKit}}, \href{https://bioconductor.org/packages/release/bioc/html/genomation.html}{\textbf{genomation}}, \href{https://bioconductor.org/packages/release/bioc/html/RCAS.html}{\textbf{RCAS}} and \href{https://bioconductor.org/packages/release/bioc/html/netSmooth.html}{\textbf{netSmooth}}. In addition, they have played key roles in developing end-to-end genomics data analysis pipelines for RNA-seq, ChIP-seq, Bisulfite-seq, and single cell RNA-seq called \href{http://bioinformatics.mdc-berlin.de/pigx/}{PiGx}.

\textbf{Contributing authors}

\href{https://github.com/borauyar}{\emph{Dr.~Bora Uyar}} contributed Chapter 8, ``RNA-seq Analysis''. Bora started his bioinformatics training in Sabanci University (Istanbul/Turkey), from which he got his undergraduate degree. Later, he obtained an MSc from Simon Fraser University (Vancouver/Canada), then a PhD from the European Molecular Biology Laboratory in Heidelberg/Germany. Since 2015, he has been working as a bioinformatics scientist at the Bioinformatics Platform and Omics Data Science Platform at the Berlin Institute for Medical Systems Biology. He has been contributing to the bioinformatics platform through research, collaborations, services and data analysis method development. His current primary research interest is the integration of multiple types of omics datasets to discover prognostic/diagnostic biomarkers of cancers.

\href{https://github.com/frenkiboy}{\emph{Dr.~Vedran Franke}} contributed Chapter 9, ``ChIP-seq Analysis''. Vedran Franke received his PhD from the University of Zagreb. His work focused on the biogenesis and function of small RNA molecules during early embryogenesis, and establishment of pluripotency. Prior to his PhD, he worked as a scientific researcher under Boris
Lenhard at the University of Bergen, Norway, focusing on principles of gene enhancer functions. He continues his research in the
Bioinformatics and Omics Data Science Platform at the Berlin Institute for Medical System Biology. He develops tools for multi-omics data integration, focusing on single-cell RNA sequencing, and epigenomics. His integrated knowledge of cellular physiology along with his proficiency in data analysis enables him to find creative solutions to difficult biological problems.

\href{https://github.com/jonathanronen}{\emph{Dr.~Jonathan Ronen}} contributed Chapter 11, ``Multi-omics Analysis''. Jonathan got his MSc in control engineering from the Norwegian University of Science and Technology in 2010. He then worked as a software developer in Oslo, Brussels, and Munich. During that time, he was also on the founding team of www.holderdeord.no, a website that links votes in the Norwegian parliament to pledges made in party manifestos. In 2014--2015, Jonathan worked as a data scientist in New York University's Social Media and Political Participation lab. During that time, he also launched www.lahadam.co.il, a website which tracked Israeli politicians' facebook posts. Jonathan obtained a PhD in computational biology in 2020, where he has published tools for imputation for single cell RNA-seq using priors, and integrative analysis of multi-omics data using deep learning.

\mainmatter

\hypertarget{intro}{%
\chapter{Introduction to Genomics}\label{intro}}

The aim of this chapter is to provide the reader with some of the fundamentals
required for
understanding genome biology. By no means, is this a complete overview of the
subject, but just a summary that will help the non-biologist reader understand
the recurring biological concepts in computational genomics. Readers that are
well-versed in genome biology and modern genome-wide quantitative assays should
feel
free to skip this chapter or skim it through.

\hypertarget{genes-dna-and-central-dogma}{%
\section{Genes, DNA and central dogma}\label{genes-dna-and-central-dogma}}

A central concept that will come up again and again is ``the gene''.
Before we can explain that, we need to
introduce a few other concepts that are important to understand the gene concept.
The human body is made up of billions of cells. These cells specialize in different
tasks. For example, in the liver there are cells that help produce enzymes
to break toxins. In the heart, there are specialized muscle cells that make
the heart beat. Yet, all these different kinds of cells come from a single-celled
embryo. All the instructions to make different kinds of cells are contained
within that single cell and with every division of that cell,
those instructions
are transmitted to new cells. These instructions can be coded into a string -- a
molecule of DNA, a polymer made of recurring units called nucleotides. The four
nucleotides in DNA molecules, Adenine, Guanine, Cytosine and Thymine (coded as
four letters: A, C, G, and T) in a specific sequence, store the information for
life. DNA is organized in a double-helix form where two complementary polymers
interlace with each other and twist into the familiar helical shape.

\hypertarget{what-is-a-genome}{%
\subsection{What is a genome?}\label{what-is-a-genome}}

The full DNA sequence of an organism, which contains all the
hereditary information, is called a genome. The genome contains all the information
to build and maintain an organism. Genomes come in different sizes and structures.
Our genome is not only a naked stretch of DNA.
In eukaryotic cells, DNA is wrapped around proteins (histones) \index{histone} forming higher-order structures like
nucleosomes which make up chromatins \index{chromatin} and chromosomes (see Figure \ref{fig:chromatinChr}).

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/chromatinChr} 

}

\caption{Chromosome structure in animals.}\label{fig:chromatinChr}
\end{figure}

There might be several chromosomes \index{chromosome}
depending on the organism. However, in some species (such as most prokaryotes)
DNA is stored in a circular form. The size of the genome between species differs too.
The human genome has 46 chromosomes and over 3 billion base-pairs, whereas the wheat genome
has 42 chromosomes and 17 billion base-pairs; both genome size and chromosome numbers
are variable between different organisms. Genome sequences of organisms are
obtained using sequencing technology. With this technology, fragments of the
DNA sequence from the genome, called reads, are obtained.
Larger chunks of the genome sequence
are later obtained by stitching the
initial fragments to larger ones by using the overlapping reads. The latest
sequencing technologies made genome sequencing cheaper and faster. These
technologies output more reads, longer reads and more accurate reads.

The estimated cost
of the first human genome was \$300 million in 1999--2000; today a high-quality human genome
can be obtained for \$1500. Since the costs are going down, researchers and clinicians
can generate more data. This drives up the costs for data storage and also drives
up the demand for qualified people to analyze genomic data. This was one of the
motivations behind writing this book.

\hypertarget{what-is-a-gene}{%
\subsection{What is a gene?}\label{what-is-a-gene}}

In the genome, there are specific regions containing the precise information that \index{gene}
encodes for physical products of genetic information. A region in the genome
with this information is traditionally called a ``gene''. However, the
precise definition of the gene is still developing. According to the classical
textbooks in molecular biology, a gene is a segment of a DNA sequence
corresponding to a single protein or to a single catalytic and structural RNA
molecule \citep{Albe_2002_book}. A modern definition is: ``A region (or regions) that includes all
of the sequence elements necessary to encode a functional transcript'' \citep{eilbeck2005sequence}. No
matter how variable the definitions are, all agree on the fact that genes are
basic units of heredity in all living organisms.

All cells use their hereditary
information in the same way most of the time; the DNA is replicated to
transfer the information to new cells. If activated, the genes are transcribed into
messenger RNAs (mRNAs) \index{mRNA} in the nucleus (in eukaryotes), followed by mRNAs (if the
gene is protein coding) getting translated into proteins in the cytoplasm. This is
essentially a process of information transfer between information-carrying
polymers; DNA, RNA and proteins, known as the ``central dogma'' \index{central dogma} of molecular
biology (see Figure \ref{fig:CentDog} for a summary).
Proteins are essential elements for life.
The growth and repair, functioning and structure of all living cells depend on them.
This is why the gene is a central
concept in genome biology, because a gene can encode information for proteins and other
functional molecules. How genes are controlled and activated dictates
everything about an organism. From the identity of a cell to response to
an infection, how cells develop and behave against certain stimuli is governed
by the activity of the genes and the functional molecules they encode. The liver cell becomes a liver cell because certain
genes are activated and their functional products are produced to help the liver
cell achieve its tasks.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/centDogma} 

}

\caption{Central Dogma: replication, transcription, translation}\label{fig:CentDog}
\end{figure}

\hypertarget{how-are-genes-controlled-transcriptional-and-post-transcriptional-regulation}{%
\subsection{How are genes controlled? Transcriptional and post-transcriptional regulation}\label{how-are-genes-controlled-transcriptional-and-post-transcriptional-regulation}}

In order to answer this question, we have to dig a little deeper into the
transcription concept we introduced via the central dogma.
The first step in a process of information transfer - the production of an RNA \index{gene regulation}
copy of a part of the DNA sequence - is called transcription. This task is
carried out by the RNA polymerase enzyme. RNA polymerase-dependent initiation of
transcription is enabled by the existence of a specific region in the sequence
of DNA - a core promoter. Core promoters are regions of DNA that promote
transcription and are found upstream from the start site of transcription. In
eukaryotes, several proteins, called general transcription factors, recognize and
bind to core promoters and form a pre-initiation complex. RNA polymerases
recognize these complexes and initiate synthesis of RNAs, the
polymerase travels along the template DNA and makes an RNA
copy \citep{hager2009transcription}. After mRNA is
produced it is often spliced by spliceosome. The sections, called `introns', are removed and
sections called `exons' left in. Then, the remaining mRNA is translated into proteins. Which exons
will be part of the final mature transcript can also be regulated and creates
diversity in protein structure and function (See Figure \ref{fig:TransSplice}).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/TransSplice} 

}

\caption{Transcription can be followed by splicing, which creates different transcript isoforms. This will in return create different protein isoforms since the information required to produce the protein is encoded in the transcripts. Differences in transcripts of the same gene can give rise to different protein isoforms.}\label{fig:TransSplice}
\end{figure}

Contrary to protein coding genes, non-coding RNA (ncRNAs)
genes are processed and assume their functional structures after transcription
and without going into translation, hence the name: non-coding RNAs. Certain
ncRNAs can also be spliced but still not translated. ncRNAs and
other RNAs in general can form complementary base-pairs within the RNA molecule
which gives them additional complexity. This self-complementarity-based
structure, termed the RNA secondary structure, is often necessary for functions of many
ncRNA species.

In summary, the set of processes, from transcription initiation to production
of the functional product, is referred to as gene expression.
Gene expression quantification and regulation is a fundamental topic in
genome biology.

\hypertarget{what-does-a-gene-look-like}{%
\subsection{What does a gene look like?}\label{what-does-a-gene-look-like}}

Before we move forward, it will be good to discuss how we can visualize genes. \index{gene}
As someone interested in computational genomics, you will frequently encounter
a gene on a computer screen, and how it is represented on the computer will be
equivalent to what you imagine when you hear the word ``gene''.
In the online databases, the genes will appear as a sequence of letters
or as a series of connected boxes showing exon-intron structure, which may
include the direction of transcription as well (see Figure \ref{fig:RealGene}). You will encounter more with the latter, so this is likely what will
pop into your mind when you think of genes.

As we have mentioned, DNA has two strands. A gene can be located
on either of them, and the direction of transcription will depend on that. In the
Figure \ref{fig:RealGene}, you can see arrows on introns (lines connecting boxes) indicating the
direction of the gene.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/RealGene} 

}

\caption{A) Representation of a gene in the UCSC browser. Boxes indicate exons, and lines indicate introns. B) Partial sequence of FATE1 gene as shown in the NCBI GenBank database.}\label{fig:RealGene}
\end{figure}

\hypertarget{elements-of-gene-regulation}{%
\section{Elements of gene regulation}\label{elements-of-gene-regulation}}

The mechanisms regulating gene expression \index{gene regulation} are essential for
all living organisms as they dictate where and how much of a gene product (it may
be protein or ncRNA) should be manufactured. This regulation could occur
at the pre- and co-transcriptional level by controlling how many transcripts should be produced and/or which version of the transcript should be produced by regulating
splicing. The same gene could encode for different versions of the same protein via
splicing regulation.This process defines which parts of the gene will go into the final
mRNA that will code for the protein variant.
In addition, gene products can be regulated post-transcriptionally where certain
molecules bind to RNA and mark them for degradation even before they can be used
in protein production.

Gene regulation drives cellular differentiation; a
process during which different tissues and cell types are produced. It also
helps cells maintain differentiated states of cells/tissues. As a result of
this process, at the final stage of differentiation, different kinds of cells
maintain different expression profiles, although they contain the same genetic
material. As mentioned above, there are two main types of regulation and next we
will provide information on those.

\hypertarget{transcriptional-regulation}{%
\subsection{Transcriptional regulation}\label{transcriptional-regulation}}

The rate of transcription initiation is the primary regulatory element in gene
expression regulation. The rate is controlled by core promoter elements as well as
distant-acting regulatory elements such as enhancers. On top of that, processes
like histone modifications and/or DNA methylation have a crucial regulatory
impact on transcription. If a region is not accessible for the transcriptional
machinery, e.g.~in the case where the chromatin structure is compacted due to the
presence of specific histone modifications, or if the promoter DNA is
methylated, transcription may not start at all. Last but not least, gene
activity is also controlled post-transcriptionally by ncRNAs such as microRNAs
(miRNAs), as well as by cell signaling, resulting in protein modification or
altered protein-protein interactions.

\hypertarget{regulation-by-transcription-factors-through-regulatory-regions}{%
\subsubsection{Regulation by transcription factors through regulatory regions}\label{regulation-by-transcription-factors-through-regulatory-regions}}

Transcription factors are proteins that \index{transcription factors (TFs)}
recognize a specific DNA motif to bind on a regulatory region and regulate the transcription rate of \index{DNA motif}
the gene associated with that regulatory region (see Figure \ref{fig:regSummary}
for an illustration). These factors bind to a variety of regulatory regions summarized in
Figure \ref{fig:regSummary}, and their concerted action
controls the transcription rate. Apart from their binding preference, their
concentration, and the availability of synergistic or competing transcription
factors will also affect the transcription rate.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/regulationSummary} 

}

\caption{Representation of regulatory regions in animal genomes}\label{fig:regSummary}
\end{figure}

\hypertarget{core-and-proximal-promoters}{%
\paragraph{Core and proximal promoters}\label{core-and-proximal-promoters}}

Core promoters are the immediate neighboring regions around \index{promoter}
the transcription start site (TSS) \index{transcription start site (TSS)} that serve as a docking site for the
transcriptional machinery and pre-initiation complex (PIC) assembly. The
textbook model for transcription initiation is as follows: The core promoter has
a TATA motif (referred as TATA-box) 30 bp upstream of an initiator sequence
(Inr), which also contains TSS. Firstly, transcription factor TFIID binds to the
TATA-box. Next, general transcription factors are recruited and transcription is
initiated on the initiator sequence. Apart from the
TATA-box and Inr, there are a number of sequence elements on the animal core
promoters that are associated with transcription initiation and PIC assembly,
such as downstream promoter elements (DPEs), the BRE elements and CpG islands.
DPEs are found 28-32 bp downstream of the TSS in TATA-less promoters of
\emph{Drosophila melanogaster}. They generally co-occur with the Inr element, and are
thought to have a similar function to the TATA-box. The BRE element is
recognized by the TFIIB protein and lie upstream of the TATA-box. CpG islands
are CG dinucleotide-enriched segments of vertebrate genomes, despite the general
depletion of CG dinucleotides in those genomes. 50 to 70\% of promoters in the
human genome are associated with CpG islands.

Proximal promoter elements are typically right upstream
of the core promoters, usually contain binding sites for activator
transcription factors, and
provide additional control over gene expression.

\hypertarget{enhancers}{%
\paragraph{Enhancers}\label{enhancers}}

Proximal regulation is not the only\index{enhancer} or the most important mode
of gene regulation. Most of the transcription factor binding sites in
the human genome are found in intergenic regions or in introns.
This indicates the widespread usage of distal regulatory elements in animal
genomes. On a molecular
function level, enhancers are similar to proximal promoters; they contain binding
sites for the same transcriptional activators and they basically enhance the
gene expression. However, they are often highly modular and several of them
can affect the same promoter at the same time or in different time-points or
tissues. In addition, their activity is independent of their
orientation and their distance to the promoter they interact with. A
number of studies showed that enhancers can act upon their target genes over
several kilobases away. According to a popular
model, enhancers achieve this by looping the DNA and coming into contact with
their target genes.

\hypertarget{silencers}{%
\paragraph{Silencers}\label{silencers}}

Silencers are similar to enhancers; however their effect is
opposite of enhancers on the transcription of the target gene, and results in
decreasing their level of transcription. They contain binding sites for
repressive transcription factors. Repressor transcription factors can either
block the binding of an activator , directly compete for the same binding site,
or induce a repressive chromatin state in which no activator binding is
possible. Silencer effects, similar to those of enhancers, are independent of
orientation and distance to target genes. In contradiction to this general view,
in \emph{Drosophila} there are two types of silencers, long-range and short-range.
Short-range silencers are close to promoters and long-range silencers can
silence multiple promoters or enhancers over kilobases away. Like
enhancers, silencers bound by repressors may also induce changes in DNA
structure by looping and creating higher-order structures. One class of
such repressor proteins, which is thought to initiate higher-order structures by
looping, is Polycomb group proteins (PcGs).

\hypertarget{insulators}{%
\paragraph{Insulators}\label{insulators}}

Insulator regions limit the effect of other regulatory elements
to certain chromosomal boundaries; in other words, they create regulatory
domains untainted by the regulatory elements in regions outside that domain.
Insulators can block enhancer-promoter communication and/or prevent spreading of
repressive chromatin domains. In vertebrates and insects, some of the
well-studied insulators are bound by CTCF (CCCTC-binding factor).
Genome-wide studies from different mammalian tissues confirm that CTCF binding
is largely invariant of cell type, and CTCF \index{CTCF protein} motif locations are conserved in
vertebrates. At present, there are two models that explain the insulator
function; the most prevalent model claims insulators create physically separate
domains by modifying chromosome structure. This is thought to be achieved by
CTCF-driven chromatin looping and recent evidence shows that CTCF can induce a
higher-order chromosome structure through creating loops of chromatins.
According to the second model, an insulator-bound activator cannot bind an
enhancer; thus enhancer-blocking activity is achieved and insulators can also
recruit an active histone domain, creating an active domain for enhancers to
function.

\hypertarget{locus-control-regions}{%
\paragraph{Locus control regions}\label{locus-control-regions}}

Locus control regions (LCRs) are clusters of
different regulatory elements that control an entire set of genes on a locus. LCRs
help genes achieve their temporal and/or tissue-specific expression programs.
LCRs may be composed of multiple cis-regulatory elements, such as insulators and
enhancers, and they act upon their targets even from long distances. However,
LCRs function in an orientation-dependent manner, for example the activity of
beta-globin LCR is lost if inverted. The mechanism of LCR function otherwise
seems similar to other long-range regulators described above. The evidence is
mounting in the direction of a model where DNA-looping creates a chromosomal
structure in which target genes are clustered together, which seems to be
essential for maintaining an open chromatin domain.

\hypertarget{epigenetic-regulation}{%
\subsubsection{Epigenetic regulation}\label{epigenetic-regulation}}

Epigenetics in biology usually refers to \index{gene regulation}
constructions (chromatin structure, DNA methylation, etc.) other than DNA \index{epigenetics}
sequence that influence gene regulation. In essence, epigenetic regulation is
the regulation of DNA packing and structure, the consequence of which is gene
expression regulation. A typical example is that DNA packing inside the nucleus can
directly influence gene expression by creating accessible regions for transcription
factors to bind.
There are two main mechanisms in
epigenetic regulation: i) DNA modifications and ii) histone modifications. Below,
we will introduce these two mechanisms.

\hypertarget{dna-modifications-such-as-methylation}{%
\paragraph{DNA modifications such as methylation}\label{dna-modifications-such-as-methylation}}

DNA methylation is usually associated with gene silencing. \index{DNA methylation}
DNA methyltransferase enzyme catalyzes the addition of a methyl group to cytosine of
CpG dinucleotides (while in mammals the addition of methyl group is largely
restricted to CpG dinucleotides, methylation can occur in other bases as well). This covalent modification either interferes with transcription factor
binding on the region, or methyl-CpG binding proteins induce the spread of
repressive chromatin domains, thus the gene is silenced if its promoter has
methylated CG dinucleotides. DNA methylation usually occurs in repeat
sequences to repress transposable elements. These elements, when active, can
jump around and insert them to random parts of the genome, potentially disrupting
the genomic functions.\index{CpG island}

DNA methylation is also related to a
key core and proximal promoter element: CpG islands. CpG islands are usually
unmethylated, however, for some genes, CpG island methylation accompanies their
silenced expression. For example, during X-chromosome inactivation, many CpG
islands are heavily methylated and the associated genes are silenced. In
addition, in embryonic stem cell differentiation, pluripotency-associated genes
are silenced due to DNA methylation. Apart from methylation, there are other
kinds of DNA modifications present in mammalian genomes, such as hydroxy-methylation and
formylcytosine. These are other modifications under current research that are either
intermediate or stable modifications with distinct functional associations. There
are at least a dozen distinct DNA modifications observed when we look across
all studied species \citep{sood2019dnamod}.

\hypertarget{histone-modifications}{%
\paragraph{Histone modifications}\label{histone-modifications}}

Histones are proteins that constitute a \index{histone} nucleosome. In
eukaryotes, eight histone proteins are wrapped by DNA and make up the
nucleosome. They help super-coiling of DNA and inducing high-order structure
called chromatin. In chromatin, DNA is either densely packed (called
heterochromatin or closed chromatin), or it is loosely packed (called
euchromatin or open chromatin). Heterochromatin is thought to harbor
inactive genes since DNA is densely packed and transcriptional machinery cannot
access it. On the other hand, euchromatin is more accessible for transcriptional
machinery and might therefore harbor active genes. Histones have long and
unstructured N-terminal tails which can be covalently modified. The most studied
modifications include acetylation, methylation and phosphorylation \citep{strahl2000language}. Using
their tails, histones interact with neighboring nucleosomes and the
modifications on the tail affect the nucleosomes' affinity to bind DNA and
therefore influence DNA packaging around nucleosomes. Different modifications on
histones are used in different combinations to program the activity of the genes
during differentiation. Histone modifications have a distinct nomenclature, for \index{histone modification}
example: H3K4me3 means the lysine (K) on the 4th position of histone H3 is
tri-methylated.

\begin{longtable}[]{@{}ll@{}}
\caption{\label{tab:histoneMod} Histone modifications and their effects. If more than one histone modification has the same effect, they are separated by commas.}\tabularnewline
\toprule
Modifications & Effect\tabularnewline
\midrule
\endfirsthead
\toprule
Modifications & Effect\tabularnewline
\midrule
\endhead
H3K9ac & Active promoters and enhancers\tabularnewline
H3K14ac & Active transcription\tabularnewline
H3K4me3/me2/me1 & Active promoters and enhancers,\tabularnewline
& H3K4me1 and H3K27ac is enhancer-specific\tabularnewline
H3K27ac & H3K27ac is enhancer-specific\tabularnewline
H3K36me3 & Active transcribed regions\tabularnewline
H3K27me3/me2/me1 & Silent promoters\tabularnewline
H3K9me3/me2/me1 & Silent promoters\tabularnewline
\bottomrule
\end{longtable}

Histone modifications are associated with a number of different
transcription-related conditions; some of them are summarized in Table \ref{tab:histoneMod}.
Histone modifications can indicate where the regulatory regions are and they can
also indicate activity of the genes. From a gene regulatory perspective, maybe
the most important modifications are the ones associated with enhancers and
promoters.

Furthermore, certain proteins can influence chromatin structure by interacting
with histones. Some of these proteins, like those of the Polycomb Group (PcG)
and CTCF, are discussed above in the insulators and silencer sections. In
vertebrates and insects, PcGs are responsible for maintaining the silent state
of developmental genes, and trithorax group proteins (trxG) for maintaining
their active state \citetext{\citealp[ ]{henikoff2008nucleosome}; \citealp{schwartz2007polycomb}}. PcGs and trxGs induce repressed or active states by
catalyzing histone modifications or DNA methylation. Both the proteins bind PREs
that can be on promoters or several kilobases away. Another protein
that induces histone modifications is CTCF. CTCF is associated with boundaries between active and repressive histone marks \index{CTCF protein} \citep{phillips2009ctcf}. This is due to the role of CTCF in regulating the 3D genome structure. Two CTCF binding sites that are far away from each other in linear distance can bind together in 3D space thus forming chromatin loops.

\begin{rmdtip}
\begin{rmdtip}

\textbf{Want to know more?}

\begin{itemize}
\item
  Transcriptional regulatory elements in the human genome: \url{http://www.ncbi.nlm.nih.gov/pubmed/16719718}
\item
  On metazoan promoters: Types and transcriptional properties:
  \url{http://www.ncbi.nlm.nih.gov/pubmed/22392219}
\item
  General principles of regulatory sequence function:
  \url{http://www.nature.com/nrg/journal/v15/n7/abs/nrg3684.html}
\item
  DNA methylation: Roles in mammalian development:
  \url{http://www.nature.com/doifinder/10.1038/nrg3354}
\item
  Histone modifications and organization of the genome:
  \url{http://www.nature.com/nrg/journal/v12/n1/full/nrg2905.html}
\item
  DNA methylation and histone modifications are linked:
  \url{http://www.nature.com/nrg/journal/v10/n5/abs/nrg2540.html}
\end{itemize}

\end{rmdtip}
\end{rmdtip}

\hypertarget{post-transcriptional-regulation}{%
\subsection{Post-transcriptional regulation}\label{post-transcriptional-regulation}}

\hypertarget{regulation-by-non-coding-rnas}{%
\subsubsection{Regulation by non-coding RNAs}\label{regulation-by-non-coding-rnas}}

Recent years have witnessed an explosion in non-coding \index{gene regulation}
RNA (ncRNA)-related research\index{non-coding RNA (ncRNA)}. Many publications implicated ncRNAs as important
regulatory elements. Plants and animals produce many
different types of ncRNAs such as long non-coding RNAs (lncRNAs),
small interferring RNAs (siRNAs), microRNAs (miRNAs), promoter-associated RNAs
(PARs) and small nucleolar RNAs (snoRNAs) \citep{morris2014rise}. lncRNAs are typically \textgreater200-bp long,
they are involved in epigenetic regulation by interacting with chromatin
remodeling factors and they function in gene regulation. siRNAs are short
double-stranded RNAs which are involved in gene regulation and transposon
control; they silence their target genes by cooperating with Argonaute proteins. miRNAs are short single-stranded RNA molecules that interact with their
target genes by using their complementary sequence and mark them for quicker
degradation. PARs may regulate gene expression as well: they are approximately
18-to -200-bp-long ncRNAs originating from promoters of coding genes \citep{morris2014rise}.
snoRNAs are also shown
to play roles in gene regulation, although they are mostly believed to guide
ribosomal RNA modifications \citep{morris2014rise}.

\hypertarget{splicing-regulation}{%
\subsubsection{Splicing regulation}\label{splicing-regulation}}

Splicing is regulated by regulatory elements on the pre-mRNA and proteins \index{gene splicing}
binding to those elements. Regulatory elements are categorized
as splicing
enhancers and repressors. They can be located either in exons
or introns. Depending on their activity and their locations there are four types of regulatory elements for splicing:

\begin{itemize}
\tightlist
\item
  exonic splicing enhancers (ESEs)
\item
  exonic splicing silencers (ESSs)
\item
  intronic splicing enhancers (ISEs)
\item
  intronic splicing silencers (ISSs).
\end{itemize}

The majority of splicing repressors are heterogeneous nuclear ribonucleoproteins (hnRNPs). If splicing repressor protein bind
silencer elements, they reduce the chance of a nearby site being
used as a splice junction. On the contrary, splicing enhancers are sites to which splicing activator proteins bind and binding
on that region increases the probability that a nearby site will be used as a splice junction \citep{wang2008splicing}. Most of the activator proteins that bind to splicing enhancers are members of the SR protein family. Such proteins can recognize specific RNA recognition motifs. By regulating splicing exons can be skipped or included,
which creates protein diversity \citep{wang2008splicing}.

\begin{rmdtip}
\begin{rmdtip}

\textbf{Want to know more?}

\begin{itemize}
\item
  On miRNAs, their genesis, and modes of regulation \citep{bartel2004micrornas}:
  \url{http://www.sciencedirect.com/science/article/pii/S0092867404000455}
\item
  Functions of non-coding RNAs \citep{morris2014rise}:
  \url{http://www.nature.com/nrg/journal/v15/n6/abs/nrg3722.html}
\item
  On splicing and its regulation \citep{wang2008splicing}: \url{https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/18369186/}
\end{itemize}

\end{rmdtip}
\end{rmdtip}

\hypertarget{shaping-the-genome-dna-mutation}{%
\section{Shaping the genome: DNA mutation}\label{shaping-the-genome-dna-mutation}}

Human and chimpanzee genomes are 98.8\% similar. The 1.2\% difference is what separates \index{mutation}
us from chimpanzees. The further you move away from human species in terms of evolutionary
distance, the higher the difference gets. However, even between the members of
the same species, differences in genome sequences exist. These differences are
due to a process called mutation which drives differences between individuals
but also provides the fuel for evolution as the source of the genetic
variation. Individuals with beneficial mutations can adapt to their surroundings
better than others and in time, these mutations, which are beneficial for survival,
spread in the population due to a process called ``natural selection''. Selection
acts upon individuals with beneficial features, which gives them an edge for
survival in a given environment. Genetic variation created by the mutations
in individuals provides the material on which selection can act. If the
selection process goes for a long time in a relatively isolated environment
that requires adaptation, this population can evolve into a different species
given enough time. This is the basic idea behind evolution in a nutshell,
and without mutations providing the genetic variation, there would be no evolution.

Mutations in the genome occur due to multiple reasons. First, DNA replication
is not an error-free process. Before a cell division, the DNA is replicated with
1 mistake per 10\^{}8 to 10\^{}10 base-pairs. Second, mutagens such as UV light
can induce mutations on the genome. The third factor that contributes to mutation
is imperfect DNA repair. Every day, any human cell suffers multiple instances of DNA damage.
DNA repair enzymes are there to cope with this damage but they are also not
error-free, depending on which DNA repair mechanism is used (there are multiple),
mistakes will be made at varying rates.

Mutations are classified by how many bases they affect, their effect on
DNA structure and gene function. By their effect on DNA structure
the mutations \index{mutation} are classified as follows:

\begin{itemize}
\tightlist
\item
  \textbf{Base substitution}: A base is changed with another.
\item
  \textbf{Deletion}: One or more bases is deleted.
\item
  \textbf{Insertion}: New base or bases inserted into the genome.
\item
  \textbf{Microsatellite mutation}: Small insertions or deletions of small tandemly
  repeating DNA segments.
\item
  \textbf{Inversion}: A DNA fragment changes its orientation 180 degrees.
\item
  \textbf{Translocation}: A DNA fragment moves to another location in the genome.
\end{itemize}

Mutations can also be classified by their size as follows:

\begin{itemize}
\tightlist
\item
  \textbf{Point mutations}: Mutations that involve one base. Substitutions, deletions and
  insertions are point mutations. They are also termed as single nucleotide
  polymorphisms (\textbf{SNPs}).\index{SNP}
\item
  \textbf{Small-scale mutations}: Mutations that involve several bases.
\item
  \textbf{Large-scale mutations}: Mutations which involve larger chromosomal regions.
  Transposable element insertions (where a segment of the genome
  jumps to another region in the genome) and segmental duplications (a large
  region is copied multiple times in tandem) are typical large scale mutations.
\item
  \textbf{Aneuploidies}: Insertions or deletions of whole chromosomes.
\item
  \textbf{Whole-genome polyploidies}: Duplications involving whole genome.
\end{itemize}

Mutations can be classified by their effect on gene function as follows:

\begin{itemize}
\tightlist
\item
  \textbf{Gain-of-function mutations}: A type of mutation in which the altered gene
  product possesses a new molecular function or a new pattern of gene
  expression.
\item
  \textbf{Loss-of-function mutations}: A mutation that results in reduced or abolished
  protein function. This is the more common type of mutation.
\end{itemize}

\begin{rmdtip}
\begin{rmdtip}

\textbf{Want to know more?}

\begin{itemize}
\item
  Interactive tutorial on mutations: \url{http://www.dnaftb.org/27/}
\item
  Tutorial on mutation and health maintained by \url{NIH:https://ghr.nlm.nih.gov/primer\#mutationsanddisorders}
\item
  Visualizations for different types of mutations: \url{https://www.yourgenome.org/facts/what-types-of-mutation-are-there}
\item
  Review article on mutations regarding human disease: \url{https://www.nature.com/articles/nrg3241}
\end{itemize}

\end{rmdtip}
\end{rmdtip}

\hypertarget{high-throughput-experimental-methods-in-genomics}{%
\section{High-throughput experimental methods in genomics}\label{high-throughput-experimental-methods-in-genomics}}

Most of the biological phenomena described above relating to transcription, gene regulation or DNA mutation can be measured over the entire genome using
high-throughput experimental techniques, which are quickly becoming
the standard for studying genome biology. In addition, their
applications in the clinic are also gaining momentum as there are already diagnostic
tests that are based on these techniques.

Some of the things that can be measured by high-throughput assays are as follows:

\begin{itemize}
\tightlist
\item
  \emph{Which genes are expressed and how much?}
\item
  \emph{Where does a transcription factor bind?}
\item
  \emph{Which bases are methylated in the genome?}
\item
  \emph{Which transcripts are translated?}
\item
  \emph{Where does RNA-binding proteins bind?}
\item
  \emph{Which microRNAs are expressed?}
\item
  \emph{Which parts of the genome are in contact with each other?}
\item
  \emph{Where are the mutations in the genome located?}
\item
  \emph{Which parts of the genome are nucleosome-free?}
\end{itemize}

There are many more questions one can answer using modern genome-wide techniques
and every other day a new variant of the existing techniques comes along
to answer a new question.
However, one has to keep in mind that these methods are at varying degrees
of maturity and they all come with technical limitations
and are not noise-free. Despite this, they are extremely useful for research and clinical
purposes. And, thanks to these methods, we are able to sequence and annotate
genomes on a massive scale.

\hypertarget{the-general-idea-behind-high-throughput-techniques}{%
\subsection{The general idea behind high-throughput techniques}\label{the-general-idea-behind-high-throughput-techniques}}

High-throughput methods aim to quantify or locate all or most of the genome that harbors
the biological feature (expressed genes, binding sites, etc.) of interest.
Most of the methods rely on some sort of enrichment of the targeted
biological feature. For example, if you want to measure expression of protein coding
genes you need to be able to extract mRNA molecules with special post-transcriptional
alterations that protein-coding genes acquire, as done in many RNA sequencing (RNA-seq) experiments\index{RNA-seq}. If you are looking for transcription factor
binding, you need to enrich
for the DNA fragments that are bound by the protein of interest, as it is done in ChIP-seq experiments. \index{ChIP-seq}This part depends on available molecular biology and chemistry techniques, and the final
product of this part is RNA or DNA fragments.

Next, you need to be able to
tell where these fragments are coming from in the genome and how many of them
there are. Microarrays \index{microarray} were the standard tool for the quantification step
until the spread of sequencing techniques. In microarrays, one had to design complementary bases, called ``oligos'' or ``probes'', to the genetic material enriched via the experimental protocol.
If the enriched material is complementary to the oligos,
a light signal will
be produced and the intensity of the signal will be proportional to the amount of the
genetic material pairing with that oligo. There will be more probes available for
hybridization (process of complementary bases forming bonds), so the more fragments
available, stronger the signal. For this to be able to work, you need to know
at least part of your genome sequence, and design probes. If you want to measure
gene expression, your probes should overlap with genes and should be unique
enough to not to bind sequences from other genes. This technology is now being
replaced with sequencing technology, where you directly sequence your genetic \index{high-throughput sequencing}
material. If you have the sequence of your fragments, you can align them back
to the genome, see where they are coming from, and count them. This is a better
technology where the quantification is based on the real identity of fragments
rather than based on hybridization to designed probes.

In summary, HT techniques have the following steps, and this also summarized in
Figure \ref{fig:HTassays}:

\begin{itemize}
\tightlist
\item
  Extraction: This is the step where you extract the genetic
  material of interest, RNA or DNA.
\item
  Enrichment: In this step, you enrich for the event you are interested
  in. For example, protein binding sites. In some cases such as whole-genome DNA
  sequencing, there is no need for enrichment step. You just get fragments of
  genomic DNA and sequence them.
\item
  Quantification: This is where you quantify your enriched material. Depending
  on the protocol you may need to quantify a control set as well, where you
  should see no enrichment or only background enrichment.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/HTassays} 

}

\caption{Common steps of high-throughput assays in genome biology.}\label{fig:HTassays}
\end{figure}

\hypertarget{high-throughput-sequencing}{%
\subsection{High-throughput sequencing}\label{high-throughput-sequencing}}

High-throughput sequencing, or massively parallel sequencing, is a
collection of methods and technologies that can sequence DNA thousands/millions \index{high-throughput sequencing}
of fragments at a time. This is in contrast to older technologies that can
produce a limited
number of fragments at a time. Here, throughput refers to the number
of sequenced bases per hour. The older low-throughput sequencing methods have
\textasciitilde100 times less throughput compared to modern high-throughput methods. The
increased throughput gives the ability to measure biological features on a
genome-wide scale in a shorter time frame.

Similar to other high-throughput methods, sequencing-based methods also require
an enrichment step. This step enriches for the features we are interested in.
The main difference of the sequencing-based methods is the quantification step.
In high-throughput sequencing,
enriched fragments are put through the sequencer which outputs the sequences
for the fragments. Due to limitations in current leading technologies, only
a limited number of bases can be sequenced from the input fragments. However,
the length is usually enough to uniquely map the reads to the genome and quantify
the input fragments.

\hypertarget{high-throughput-sequencing-data}{%
\subsubsection{High-throughput sequencing data}\label{high-throughput-sequencing-data}}

If there is a genome available, the reads are aligned to the genome and based
on the library preparation protocol, different strategies are applied for analysis. A sequencing library is composed of fragments of RNA or DNA ready to be sequenced. The library preparation primarily depends on the experiment of interest. There are a number of library preparation protocols aimed at quantifying different signals from the genome. Some of the potential analysis strategies for different library-prep protocols and processed output of read alignments
are depicted in Figure \ref{fig:HTseq}.\index{high-throughput sequencing}
For example, we may be interested in quantifying the gene expression.
The experimental protocol, called RNA sequencing,
RNA-seq, enriches for fragments of RNA that are coming from protein coding genes.\index{RNA-seq}
Upon alignment, we can calculate the coverage profile which gives us a read count
per base along the genome. This information can be stored in a text file or
specialized file formats to be used in subsequent analysis or visualization.
We can also just count how many reads overlap with exons of each gene and record
read counts per gene for further analysis. This essentially produces a table
with gene names and read counts for different samples. As we will see in later
chapters, this is an essential information for statistical models for
RNA-seq data. Furthermore, we can stack up the reads and count how many times a base position in a read mismatches the base in the genome. Read aligners
allow for mismatches, and for this reason we can see reads with mismatches.
This information can be used to identify SNPs, and can be stored again in
a tabular format with the information of position and mismatch type and
number of reads supporting the mismatch. The original algorithms are a bit
more complicated than just counting mismatches but the general idea is the
same; what they are doing differently is trying to minimize false positive
rates by using filters, so that not every mismatch is recorded as a SNP.\index{SNP}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/HTseq} 

}

\caption{High-throughput sequencing summary}\label{fig:HTseq}
\end{figure}

\hypertarget{future-of-high-throughput-sequencing}{%
\subsubsection{Future of high-throughput sequencing}\label{future-of-high-throughput-sequencing}}

The sequencing technology is still evolving. Obtaining
longer single-molecule reads, and preferably, being able to call base modifications \index{high-throughput sequencing}
on the fly is the next frontier.
With longer reads, the genome assembly will be easier for the regions
that have high repeat content. With single-molecule sequencing, we will be able to
tell how many transcripts are present in a given cell population without
relying on fragment amplification methods which can introduce biases.

Another recent development is single-cell sequencing. Current technologies usually
work on genetic material from thousands to millions of cells. This means that the
results you receive represent the population of cells that were used in the
experiment. However, there is a lot of variation between the same type of cells, but
this variation is not observed at all. Newer sequencing techniques can work
on single cells and give quantitative information on each cell.

\begin{rmdtip}
\begin{rmdtip}

\textbf{Want to know more?}

\begin{itemize}
\item
  Current and the future high-throughput sequencing technologies: \url{http://www.sciencedirect.com/science/article/pii/S1097276515003408}
\item
  Illumina repository for different library preparation protocols for sequencing: \url{http://www.illumina.com/techniques/sequencing/ngs-library-prep/library-prep-methods.html}
\end{itemize}

\end{rmdtip}
\end{rmdtip}

\hypertarget{visualization-and-data-repositories-for-genomics}{%
\section{Visualization and data repositories for genomics}\label{visualization-and-data-repositories-for-genomics}}

There are \textasciitilde100 animal genomes sequenced as of 2016. On top these, there are many
research projects from either individual labs or consortia that
produce petabytes of auxiliary genomics data, such as ChIP-seq, RNA-seq, etc. \index{ChIP-seq} \index{RNA-seq}

There are two requirements to be able to visualize genomes and their associated
data: 1) you need to be able to work with a species
that has a sequenced genome and 2) you want to have annotation on that genome,
meaning, at the very least, you want to know where the genes are. Most
genomes after sequencing are quickly annotated with gene-predictions or
known gene sequences are mapped on to them, and you can also
have conservation to other species to filter functional elements. If you
are working with a model organism or human, you will also have a lot of
auxiliary information to help demarcate the functional regions such
as regulatory regions, ncRNAs, and SNPs that are common in the population.
Or you might have disease- or tissue-specific data available.
The more the organism is worked on, the more auxiliary data you will have.

\hypertarget{accessing-genome-sequences-and-annotations-via-genome-browsers}{%
\subsubsection{Accessing genome sequences and annotations via genome browsers}\label{accessing-genome-sequences-and-annotations-via-genome-browsers}}

As someone who intends to work with genomics, you will need to visualize a
large amount of data to make biological inferences or simply check regions of
interest in the genome visually. Looking at the genome case by case with all
the additional datasets is a necessary step to develop a hypothesis and understand
the data.

Many genomes and their associated data are
available through genome browsers. A genome browser
is a website or an application
that helps you visualize the genome and all the available data associated
with it. Via genome browsers\index{genome browser}, you will be able to see where genes are in
relation to each other and other functional elements. You will be
able to see gene structure. You will be able to see auxiliary data such as
conservation, repeat content and SNPs. Here we review some of the popular
browsers.

\textbf{UCSC genome browser:} This is an online browser hosted by University of
California, Santa Cruz at \url{http://genome.ucsc.edu/}.
This is an interactive website that contains genomes \index{UCSC Genome Browser}
and annotations for many species. You can search for genes or genome coordinates
for the species of your interest. It is usually very responsive and allows
you to visualize large amounts of data. In addition, it has multiple other tools
that can be used in connection with the browser. One of the most useful tools
is the \emph{UCSC Table Browser}, which lets you download all the data you see on the
browser, including sequence data, in multiple formats.
Users can upload data or provide links to the data
to visualize user-specific data.

\textbf{Ensembl:} This is another online browser maintained by the European Bioinformatics Institute and the Wellcome Trust Sanger Institute in
the UK, \url{http://www.ensembl.org}.
Similar to the UCSC browser, users can visualize genes or genomic coordinates
from multiple species and it also comes with auxiliary data. Ensembl is
associated with the \emph{Biomart} \index{Biomart} tool which is similar to UCSC Table browser, and can
download genome data including all the auxiliary data set in multiple formats.\index{Ensembl Genome Browser}

\textbf{IGV:} Integrated genomics viewer (IGV) is a desktop application developed
by Broad institute (\url{https://www.broadinstitute.org/igv/}). It is
developed to deal with large amounts of high-throughput sequencing data, which
is harder to view in online browsers. IGV can integrate your local sequencing
results with online annotation on your desktop machine. This is useful when
viewing sequencing data, especially alignments. Other browsers mentioned above
have similar features, however you will need to make your large
sequencing data available online somewhere before it can be viewed by browsers. \index{IGV Browser}

\hypertarget{data-repositories-for-high-throughput-assays}{%
\subsubsection{Data repositories for high-throughput assays}\label{data-repositories-for-high-throughput-assays}}

Genome browsers contain lots of auxiliary high-throughput data. However, there are
many more public high-throughput data sets available and they are certainly
not available through genome browsers. Normally, every high-throughput dataset
associated with a publication should be deposited in public archives. There
are two major public archives we use to deposit data. One of them is
\emph{Gene Expression Omnibus (GEO)} hosted at \url{http://www.ncbi.nlm.nih.gov/geo/},
and the other one is \emph{European Nucleotide Archive (ENA)} hosted at
\url{http://www.ebi.ac.uk/ena}. These repositories accept high-throughput datasets
and users can freely download and use these public data sets for their
own research. Many data sets in these repositories are in their raw format,
for example, the format the sequencer provides mostly. Some data sets will also
have processed data but that is not a norm.

Apart from these repositories, there are multiple multi-national consortia dedicated to certain genome biology or disease-related problems and
they maintain their own databases and provide access to processed and raw data.
Some of these consortia are mentioned below.

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.32\columnwidth}\raggedright
Consortium\strut
\end{minipage} & \begin{minipage}[b]{0.62\columnwidth}\raggedright
What is it for?\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.32\columnwidth}\raggedright
\href{https://www.encodeproject.org/}{ENCODE}\strut
\end{minipage} & \begin{minipage}[t]{0.62\columnwidth}\raggedright
Transcription factor binding sites, gene expression and epigenomics data for cell lines\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\raggedright
\href{http://www.roadmapepigenomics.org/}{Epigenomics Roadmap}\strut
\end{minipage} & \begin{minipage}[t]{0.62\columnwidth}\raggedright
Epigenomics data for multiple cell types\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\raggedright
\href{http://cancergenome.nih.gov/}{The Cancer Genome Atlas (TCGA)}\strut
\end{minipage} & \begin{minipage}[t]{0.62\columnwidth}\raggedright
Expression, mutation and epigenomics data for multiple cancer types\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\raggedright
\href{http://www.1000genomes.org/}{1000 genomes project}\strut
\end{minipage} & \begin{minipage}[t]{0.62\columnwidth}\raggedright
Human genetic variation data obtained by sequencing 1000s of individuals\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\index{ENCODE}
\index{Epigenomics Roadmap}
\index{The Cancer Genome Atlas (TCGA)}

\hypertarget{Rintro}{%
\chapter{Introduction to R for Genomic Data Analysis}\label{Rintro}}

The aim of computational genomics is to provide biological interpretation and insights from
high-dimensional genomics data. Generally speaking, it is similar to any other kind
of data analysis endeavor but oftentimes doing computational genomics will require domain-specific knowledge and tools.

As new high-throughput experimental techniques are on the rise, data analysis
capabilities are sought-after features for researchers. The aim of this chapter is to first familiarize readers with data analysis steps and then provide basics of R programming within the context of genomic data analysis. R is a free statistical programming language that is popular among researchers and data miners to build software and analyze data. Although
basic R programming tutorials are easily accessible, we are aiming to introduce
the subject with the genomic context in the background. The examples and
narrative will always be from real-life situations when you try to analyze
genomic data with R. We believe tailoring material to the context of genomics
makes a difference when learning this programming language for the sake of analyzing
genomic data.

\hypertarget{steps-of-genomic-data-analysis}{%
\section{Steps of (genomic) data analysis}\label{steps-of-genomic-data-analysis}}

Regardless of the analysis type, data analysis has a common pattern. We will
discuss this general pattern and how it applies to genomics problems. The data analysis steps typically include data collection, quality check and cleaning, processing, modeling, visualization, and reporting. Although one expects to go through these steps in a linear fashion, it is normal to go back and repeat the steps with different parameters or tools. In practice, data analysis requires going through the same steps over and over again in order to be able to do a combination of the following: a) answer other related questions, b) deal with data quality issues that are later realized, and c) include new data sets to the analysis.

We will now go through a brief explanation of the steps within the context of genomic data analysis.

\hypertarget{data-collection}{%
\subsection{Data collection}\label{data-collection}}

Data collection refers to any source, experiment or survey that provides data for the data analysis question you have. In genomics, data collection is done by high-throughput assays, introduced in Chapter \ref{intro}. One can also use publicly available data sets and specialized databases, also mentioned in Chapter \ref{intro}. How much data and what type of data you should collect depends on the question you are trying to answer and the technical and biological variability of the system you are studying.

\hypertarget{data-quality-check-and-cleaning}{%
\subsection{Data quality check and cleaning}\label{data-quality-check-and-cleaning}}

In general, data analysis almost always deals with imperfect data. It is
common to have missing values or measurements that are noisy. Data quality check
and cleaning aims to identify any data quality issue and clean it from the dataset.

High-throughput genomics data is produced by technologies that could embed
technical biases into the data. If we were to give an example from sequencing,
the sequenced reads do not have the same quality of bases called. Towards the
ends of the reads, you could have bases that might be called incorrectly. Identifying those low-quality bases and removing them will improve the read mapping step.

\hypertarget{data-processing}{%
\subsection{Data processing}\label{data-processing}}

This step refers to processing the data into a format that is suitable for
exploratory analysis and modeling. Oftentimes, the data will not come in a ready-to-analyze
format. You may need to convert it to other formats by transforming
data points (such as log transforming, normalizing, etc.), or subset the data set
with some arbitrary or pre-defined condition. In terms of genomics, processing
includes multiple steps. Following the sequencing analysis example above,
processing will include aligning reads to the genome and quantification over genes or regions of interest. This is simply counting how many reads are covering your regions of interest. This quantity can give you ideas about how much a gene is expressed if your experimental protocol was RNA sequencing\index{RNA-seq}. This can be followed by some normalization to aid the next step.

\hypertarget{exploratory-data-analysis-and-modeling}{%
\subsection{Exploratory data analysis and modeling}\label{exploratory-data-analysis-and-modeling}}

This phase usually takes in the processed or semi-processed data and applies machine learning or statistical methods to explore the data. Typically, one needs to see a relationship between variables measured, and a relationship between samples based on the variables measured. At this point, we might be looking to see if the samples are grouped as expected by the experimental design, or are there outliers or any other anomalies? After this step you might want to do additional cleanup or re-processing to deal with anomalies.

Another related step is modeling. This generally refers to modeling your variable of interest based on other variables you measured. In the context of genomics, it could be that you are trying to predict disease status of the patients from expression of genes you measured from their tissue samples. Then your variable of interest is the disease status. This kind of approach is generally called ``predictive modeling'', and could be solved with regression-based machine learning methods.

Statistical modeling would also be a part of this modeling step. This can cover predictive modeling as well, where we use statistical methods such as linear regression. Other analyses such as hypothesis testing, where we have an expectation and we are trying to confirm that expectation, is also related to statistical modeling. A good example of this in genomics is the differential gene expression analysis. This can be formulated as comparing two data sets, in this case expression values from condition A and condition B, with the expectation that condition A and condition B have similar expression values. You will see more on this in Chapter \ref{stats}.

\hypertarget{visualization-and-reporting}{%
\subsection{Visualization and reporting}\label{visualization-and-reporting}}

Visualization is necessary for all the previous steps more or less. But in the final phase, we need final figures, tables, and text that describe the outcome of your analysis. This will be your report. In genomics, we use common data visualization methods as well as specific visualization methods developed or popularized by genomic data analysis. You will see many popular visualization methods in Chapters \ref{stats} and \ref{genomicIntervals}.

\hypertarget{why-use-r-for-genomics}{%
\subsection{Why use R for genomics ?}\label{why-use-r-for-genomics}}

R, with its statistical analysis
heritage, plotting features, and rich user-contributed packages is one of the
best languages for the task of analyzing genomic data.
High-dimensional genomics datasets are usually suitable to
be analyzed with core R packages and functions. On top of that, Bioconductor and CRAN have an
array of specialized tools for doing genomics-specific analysis. Here is a list of computational genomics tasks that can be completed using R.

\hypertarget{data-cleanup-and-processing}{%
\subsubsection{Data cleanup and processing}\label{data-cleanup-and-processing}}

Most of general data cleanup, such as removing incomplete columns and values, reorganizing and transforming data, can be achieved using R. In addition, with the help of packages, R can connect to databases in various formats such as mySQL, mongoDB, etc., and query and get the data into the R environment using database specific tools.

On top of these, genomic data-specific processing and quality check can be achieved via R/Bioconductor packages. For example, sequencing read quality checks and even \index{high-throughput sequencing} HT-read alignments \index{read alignment} can be achieved via R packages.

\hypertarget{general-data-analysis-and-exploration}{%
\subsubsection{General data analysis and exploration}\label{general-data-analysis-and-exploration}}

Most genomics data sets are suitable for application of general data analysis tools. In some cases, you may need to preprocess the data to get it to a state that is suitable for application of such tools. Here is a non-exhaustive list of what kind of things can be done via R. You will see popular data analysis methods in Chapters \ref{stats}, \ref{unsupervisedLearning} and \ref{supervisedLearning}.

\begin{itemize}
\tightlist
\item
  Unsupervised data analysis: clustering (k-means, hierarchical), matrix factorization
  (PCA, ICA, etc.)
\item
  Supervised data analysis: generalized linear models, support vector machines, random forests
\end{itemize}

\hypertarget{genomics-specific-data-analysis-methods}{%
\subsubsection{Genomics-specific data analysis methods}\label{genomics-specific-data-analysis-methods}}

R/Bioconductor gives you access to a multitude of other bioinformatics-specific algorithms. Here are some of the things you can do. We will touch upon many of the following methods in Chapter \ref{genomicIntervals} and onwards.

\begin{itemize}
\tightlist
\item
  Sequence analysis: TF binding motifs, GC content and CpG counts of a given DNA sequence
\item
  Differential expression (or arrays and sequencing-based measurements)
\item
  Gene set/pathway analysis: What kind of genes are enriched in my gene set?
\item
  Genomic interval operations such as overlapping CpG islands with transcription start sites, and filtering based on overlaps
\item
  Overlapping aligned reads with exons and counting aligned reads per gene
\end{itemize}

\hypertarget{visualization}{%
\subsubsection{Visualization}\label{visualization}}

Visualization is an important part of all data analysis techniques including computational genomics. Again, you can use core visualization techniques in R and also genomics-specific ones with the help of specific packages. Here are some of the things you can do with R.

\begin{itemize}
\tightlist
\item
  Basic plots: Histograms, scatter plots, bar plots, box plots, heatmaps
\item
  Ideograms and circos plots for genomics provide visualization of different features over the whole genome.
\item
  Meta-profiles of genomic features, such as read enrichment over all promoters
\item
  Visualization of quantitative assays for given locus in the genome
\end{itemize}

\hypertarget{getting-started-with-r}{%
\section{Getting started with R}\label{getting-started-with-r}}

Download and install R (\url{http://cran.r-project.org/}) and RStudio (\url{http://www.rstudio.com/}) if you do not have them already. Rstudio is optional but it is a great tool if you are just starting to learn R.
You will need specific data sets to run the code snippets in this book; we have explained how to install and use the data in the \protect\hyperlink{data-for-the-book}{Data for the book} section in the \protect\hyperlink{preface}{Preface}. If you have not used Rstudio before, we recommend running it and familiarizing yourself with it first. To put it simply, this interface combines multiple features you will need while analyzing data. You can see your code, how it is executed, the plots you make, and your data all in one interface.

\hypertarget{installing-packages}{%
\subsection{Installing packages}\label{installing-packages}}

R packages are add-ons to base R that help you achieve additional tasks that are not directly supported by base R. It is by the action of these extra functionality that R excels as a tool for computational genomics. The Bioconductor project (\url{http://bioconductor.org/}) is a dedicated package repository for computational biology-related packages. However main package repository of R, called CRAN, also has computational biology related packages. In addition, R-Forge (\url{http://r-forge.r-project.org/}), GitHub (\url{https://github.com/}), and Bitbucket (\url{http://www.bitbucket.org}) are some of the other locations where R packages might be hosted. The packages needed for the code snippets in this book and how to install them are explained in the \protect\hyperlink{packages-needed-to-run-the-book-code}{Packages needed to run the book code} section in the \protect\hyperlink{preface}{Preface} of the book.

You can install CRAN packages using \texttt{install.packages()} (\# is the comment character in R).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install package named "randomForests" from CRAN}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"randomForests"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can install bioconductor packages with a specific installer script.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get the installer package if you don't have}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"BiocManager"}\NormalTok{)}

\CommentTok{# install bioconductor package "rtracklayer"}
\NormalTok{BiocManager}\OperatorTok{::}\KeywordTok{install}\NormalTok{(}\StringTok{"rtracklayer"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can install packages from GitHub using the \texttt{install\_github()} function from \texttt{devtools} package.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(devtools)}
\KeywordTok{install_github}\NormalTok{(}\StringTok{"hadley/stringr"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Another way to install packages is from the source.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# download the source file}
\KeywordTok{download.file}\NormalTok{(}
\StringTok{"https://github.com/al2na/methylKit/releases/download/v0.99.2/methylKit_0.99.2.tar.gz"}\NormalTok{,}
               \DataTypeTok{destfile=}\StringTok{"methylKit_0.99.2.tar.gz"}\NormalTok{)}
\CommentTok{# install the package from the source file}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"methylKit_0.99.2.tar.gz"}\NormalTok{,}
                 \DataTypeTok{repos=}\OtherTok{NULL}\NormalTok{,}\DataTypeTok{type=}\StringTok{"source"}\NormalTok{)}
\CommentTok{# delete the source file}
\KeywordTok{unlink}\NormalTok{(}\StringTok{"methylKit_0.99.2.tar.gz"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can also update CRAN and Bioconductor packages.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# updating CRAN packages}
\KeywordTok{update.packages}\NormalTok{()}

\CommentTok{# updating bioconductor packages}
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{requireNamespace}\NormalTok{(}\StringTok{"BiocManager"}\NormalTok{, }\DataTypeTok{quietly =} \OtherTok{TRUE}\NormalTok{))}
    \KeywordTok{install.packages}\NormalTok{(}\StringTok{"BiocManager"}\NormalTok{)}
\NormalTok{BiocManager}\OperatorTok{::}\KeywordTok{install}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{installing-packages-in-custom-locations}{%
\subsection{Installing packages in custom locations}\label{installing-packages-in-custom-locations}}

If you will be using R on servers or computing clusters rather than your personal computer, it is unlikely that you will have administrator access to install packages. In that case, you can install packages in custom locations by telling R where to look for additional packages. This is done by setting up an \texttt{.Renviron} file in your home directory and add the following line:

\begin{verbatim}
R_LIBS=~/Rlibs
\end{verbatim}

This tells R that the ``Rlibs'' directory at your home directory will be the first choice of locations to look for packages and install packages (the directory name and location is up to you, the above is just an example). You should go and create that directory now. After that, start a fresh R session and start installing packages. From now on, packages will be installed to your local directory where you have read-write access.

\hypertarget{getting-help-on-functions-and-packages}{%
\subsection{Getting help on functions and packages}\label{getting-help-on-functions-and-packages}}

You can get help on functions by using \texttt{help()} and \texttt{help.search()} functions. You can list the functions in a package with the \texttt{ls()} function

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(MASS)}
\KeywordTok{ls}\NormalTok{(}\StringTok{"package:MASS"}\NormalTok{) }\CommentTok{# functions in the package}
\KeywordTok{ls}\NormalTok{() }\CommentTok{# objects in your R enviroment}
\CommentTok{# get help on hist() function}
\NormalTok{?hist}
\KeywordTok{help}\NormalTok{(}\StringTok{"hist"}\NormalTok{)}
\CommentTok{# search the word "hist" in help pages}
\KeywordTok{help.search}\NormalTok{(}\StringTok{"hist"}\NormalTok{)}
\NormalTok{??hist}
\end{Highlighting}
\end{Shaded}

\hypertarget{more-help-needed}{%
\subsubsection{More help needed?}\label{more-help-needed}}

In addition, check package vignettes for help and practical understanding of the functions. All Bioconductor packages have vignettes that walk you through example analysis. Google search will always be helpful as well; there are many blogs and web pages that have posts about R. R-help mailing list (\url{https://stat.ethz.ch/mailman/listinfo/r-help}), Stackoverflow.com and R-bloggers.com are usually sources of good and reliable information.

\hypertarget{computations-in-r}{%
\section{Computations in R}\label{computations-in-r}}

R can be used as an ordinary calculator, and some say it is an over-grown calculator. Here are some examples. Remember that \texttt{\#} is the comment character. The comments give details about the operations in case they are not clear.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \OperatorTok{+}\StringTok{ }\DecValTok{3} \OperatorTok{*}\StringTok{ }\DecValTok{5}       \CommentTok{# Note the order of operations.}
\KeywordTok{log}\NormalTok{(}\DecValTok{10}\NormalTok{)        }\CommentTok{# Natural logarithm with base e}
\DecValTok{5}\OperatorTok{^}\DecValTok{2}            \CommentTok{# 5 raised to the second power}
\DecValTok{3}\OperatorTok{/}\DecValTok{2}            \CommentTok{# Division}
\KeywordTok{sqrt}\NormalTok{(}\DecValTok{16}\NormalTok{)      }\CommentTok{# Square root}
\KeywordTok{abs}\NormalTok{(}\DecValTok{3-7}\NormalTok{)      }\CommentTok{# Absolute value of 3-7}
\NormalTok{pi             }\CommentTok{# The number}
\KeywordTok{exp}\NormalTok{(}\DecValTok{2}\NormalTok{)        }\CommentTok{# exponential function}
\CommentTok{# This is a comment line}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-structures}{%
\section{Data structures}\label{data-structures}}

R has multiple data structures. If you are familiar with Excel, you can think of a single Excel sheet as a table and data structures as building blocks of that table. Most of the time you will deal with tabular data sets or you will want to transform your raw data to a tabular data set, and you will try to manipulate this tabular data set in some way. For example, you may want to take sub-sections of the table or extract all the values in a column. For these and similar purposes, it is essential to know the common data structures in R and how they can be used. R deals with named data structures, which means you can give names to data structures and manipulate or operate on them using those names. It will be clear soon what we mean by this if ``named data structures'' does not ring a bell.

\hypertarget{vectors}{%
\subsection{Vectors}\label{vectors}}

Vectors are one of the core R data structures. It is basically a list of elements of the same type (numeric, character or logical). Later you will see that every column of a table will be represented as a vector. R handles vectors easily and intuitively. You can create vectors with the \texttt{c()} function, however that is not the only way. The operations on vectors will propagate to all the elements of the vectors.\index{R Programming Language!vector}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x<-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{5}\NormalTok{)    }\CommentTok{#create a vector named x with 5 components}
\NormalTok{x =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{5}\NormalTok{)  }
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  1  3  2 10  5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y<-}\DecValTok{1}\OperatorTok{:}\DecValTok{5}              \CommentTok{#create a vector of consecutive integers y}
\NormalTok{y}\OperatorTok{+}\DecValTok{2}                 \CommentTok{#scalar addition}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3 4 5 6 7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2}\OperatorTok{*}\NormalTok{y                 }\CommentTok{#scalar multiplication}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  2  4  6  8 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y}\OperatorTok{^}\DecValTok{2}                 \CommentTok{#raise each component to the second power}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  1  4  9 16 25
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2}\OperatorTok{^}\NormalTok{y                 }\CommentTok{#raise 2 to the first through fifth power}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  2  4  8 16 32
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y                   }\CommentTok{#y itself has not been unchanged}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3 4 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y<-y}\OperatorTok{*}\DecValTok{2}
\NormalTok{y                   }\CommentTok{#it is now changed}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  2  4  6  8 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r1<-}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{)        }\CommentTok{# create a vector of 1s, length 3}
\KeywordTok{length}\NormalTok{(r1)           }\CommentTok{#length of the vector}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(r1)            }\CommentTok{# class of the vector}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "numeric"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a<-}\DecValTok{1}                \CommentTok{# this is actually a vector length one}
\end{Highlighting}
\end{Shaded}

The standard assignment operator in R is \texttt{\textless{}-}. This operator is preferentially used in books and documentation. However, it is also possible to use the \texttt{=} operator for the assignment.
We have an example in the above code snippet and throughout the book we use \texttt{\textless{}-} and \texttt{=} interchangeably for assignment.

\hypertarget{matrices}{%
\subsection{Matrices}\label{matrices}}

A matrix refers to a numeric array of rows and columns. You can think of it as a stacked version of vectors where each row or column is a vector. One of the easiest ways to create a matrix is to combine vectors of equal length using \texttt{cbind()}, meaning `column bind'.\index{R Programming Language!matrix}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x<-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\NormalTok{y<-}\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{7}\NormalTok{)}
\NormalTok{m1<-}\KeywordTok{cbind}\NormalTok{(x,y);m1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      x y
## [1,] 1 4
## [2,] 2 5
## [3,] 3 6
## [4,] 4 7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t}\NormalTok{(m1)                }\CommentTok{# transpose of m1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [,1] [,2] [,3] [,4]
## x    1    2    3    4
## y    4    5    6    7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(m1)              }\CommentTok{# 2 by 5 matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4 2
\end{verbatim}

You can also directly list the elements and specify the matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m2<-}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{5}\NormalTok{,}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{9}\NormalTok{),}\DataTypeTok{nrow=}\DecValTok{3}\NormalTok{)}
\NormalTok{m2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1    5    2
## [2,]    3   -1    3
## [3,]    2    2    9
\end{verbatim}

Matrices and the next data structure, \textbf{data frames}, are tabular data structures. You can subset them using \texttt{{[}{]}} and providing desired rows and columns to subset. Figure \ref{fig:slicingDataFrames} shows how that works conceptually.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/slicingDataFrames} 

}

\caption{Slicing/subsetting of a matrix and a data frame.}\label{fig:slicingDataFrames}
\end{figure}

\hypertarget{data-frames}{%
\subsection{Data frames}\label{data-frames}}

A data frame is more general than a matrix, in that different columns can have different modes (numeric, character, factor, etc.). A data frame can be constructed by the \texttt{data.frame()} function. For example, we illustrate how to construct a data frame from genomic intervals or coordinates.\index{R Programming Language!data frame}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chr <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"chr1"}\NormalTok{, }\StringTok{"chr1"}\NormalTok{, }\StringTok{"chr2"}\NormalTok{, }\StringTok{"chr2"}\NormalTok{)}
\NormalTok{strand <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"-"}\NormalTok{,}\StringTok{"-"}\NormalTok{,}\StringTok{"+"}\NormalTok{,}\StringTok{"+"}\NormalTok{)}
\NormalTok{start<-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{200}\NormalTok{,}\DecValTok{4000}\NormalTok{,}\DecValTok{100}\NormalTok{,}\DecValTok{400}\NormalTok{)}
\NormalTok{end<-}\KeywordTok{c}\NormalTok{(}\DecValTok{250}\NormalTok{,}\DecValTok{410}\NormalTok{,}\DecValTok{200}\NormalTok{,}\DecValTok{450}\NormalTok{)}
\NormalTok{mydata <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(chr,start,end,strand)}
\CommentTok{#change column names}
\KeywordTok{names}\NormalTok{(mydata) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"chr"}\NormalTok{,}\StringTok{"start"}\NormalTok{,}\StringTok{"end"}\NormalTok{,}\StringTok{"strand"}\NormalTok{)}
\NormalTok{mydata }\CommentTok{# OR this will work too}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    chr start end strand
## 1 chr1   200 250      -
## 2 chr1  4000 410      -
## 3 chr2   100 200      +
## 4 chr2   400 450      +
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mydata <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{chr=}\NormalTok{chr,}\DataTypeTok{start=}\NormalTok{start,}\DataTypeTok{end=}\NormalTok{end,}\DataTypeTok{strand=}\NormalTok{strand)}
\NormalTok{mydata}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    chr start end strand
## 1 chr1   200 250      -
## 2 chr1  4000 410      -
## 3 chr2   100 200      +
## 4 chr2   400 450      +
\end{verbatim}

There are a variety of ways to extract the elements of a data frame. You can extract certain columns using column numbers or names, or you can extract certain rows by using row numbers. You can also extract data using logical arguments, such as extracting all rows that have a value in a column larger than your threshold.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mydata[,}\DecValTok{2}\OperatorTok{:}\DecValTok{4}\NormalTok{] }\CommentTok{# columns 2,3,4 of data frame}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   start end strand
## 1   200 250      -
## 2  4000 410      -
## 3   100 200      +
## 4   400 450      +
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mydata[,}\KeywordTok{c}\NormalTok{(}\StringTok{"chr"}\NormalTok{,}\StringTok{"start"}\NormalTok{)] }\CommentTok{# columns chr and start from data frame}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    chr start
## 1 chr1   200
## 2 chr1  4000
## 3 chr2   100
## 4 chr2   400
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mydata}\OperatorTok{$}\NormalTok{start }\CommentTok{# variable start in the data frame}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  200 4000  100  400
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mydata[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{),] }\CommentTok{# get 1st and 3rd rows}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    chr start end strand
## 1 chr1   200 250      -
## 3 chr2   100 200      +
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mydata[mydata}\OperatorTok{$}\NormalTok{start}\OperatorTok{>}\DecValTok{400}\NormalTok{,] }\CommentTok{# get all rows where start>400}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    chr start end strand
## 2 chr1  4000 410      -
\end{verbatim}

\hypertarget{lists}{%
\subsection{Lists}\label{lists}}

A list in R is an ordered collection of objects (components). A list allows you to gather a variety of (possibly unrelated) objects under one name. You can create a list with the \texttt{list()} function. Each object or element in the list has a numbered position and can have names. Below we show a few examples of how to create lists.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# example of a list with 4 components}
\CommentTok{# a string, a numeric vector, a matrix, and a scalar}
\NormalTok{w <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{name=}\StringTok{"Fred"}\NormalTok{,}
       \DataTypeTok{mynumbers=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{),}
       \DataTypeTok{mymatrix=}\KeywordTok{matrix}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{,}\DataTypeTok{ncol=}\DecValTok{2}\NormalTok{),}
       \DataTypeTok{age=}\FloatTok{5.3}\NormalTok{)}
\NormalTok{w}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $name
## [1] "Fred"
## 
## $mynumbers
## [1] 1 2 3
## 
## $mymatrix
##      [,1] [,2]
## [1,]    1    3
## [2,]    2    4
## 
## $age
## [1] 5.3
\end{verbatim}

You can extract elements of a list using the \texttt{{[}{[}{]}{]}}, the double square-bracket, convention using either its position in the list or its name.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{w[[}\DecValTok{3}\NormalTok{]] }\CommentTok{# 3rd component of the list}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]    1    3
## [2,]    2    4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{w[[}\StringTok{"mynumbers"}\NormalTok{]] }\CommentTok{# component named mynumbers in list}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{w}\OperatorTok{$}\NormalTok{age}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.3
\end{verbatim}

\hypertarget{factors}{%
\subsection{Factors}\label{factors}}

Factors are used to store categorical data. They are important for statistical modeling since categorical variables are treated differently in statistical models than continuous variables. This ensures categorical data treated accordingly in statistical models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{features=}\KeywordTok{c}\NormalTok{(}\StringTok{"promoter"}\NormalTok{,}\StringTok{"exon"}\NormalTok{,}\StringTok{"intron"}\NormalTok{)}
\NormalTok{f.feat=}\KeywordTok{factor}\NormalTok{(features)}
\end{Highlighting}
\end{Shaded}

An important thing to note is that when you are reading a data frame with \texttt{read.table()} or creating a data frame with \texttt{data.frame()} function, the character columns are stored as factors by default, to change this behavior you need to set \texttt{stringsAsFactors=FALSE} in \texttt{read.table()} and/or \texttt{data.frame()} function arguments.\index{R Programming Language!factor}

\hypertarget{data-types}{%
\section{Data types}\label{data-types}}

There are four common data types in R, they are \texttt{numeric}, \texttt{logical}, \texttt{character} and \texttt{integer}. All these data types can be used to create vectors natively.\index{R Programming Language!data types}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#create a numeric vector x with 5 components}
\NormalTok{x<-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{5}\NormalTok{)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  1  3  2 10  5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#create a logical vector x}
\NormalTok{x<-}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{,}\OtherTok{FALSE}\NormalTok{,}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE FALSE  TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a character vector}
\NormalTok{x<-}\KeywordTok{c}\NormalTok{(}\StringTok{"sds"}\NormalTok{,}\StringTok{"sd"}\NormalTok{,}\StringTok{"as"}\NormalTok{)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "sds" "sd"  "as"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create an integer vector}
\NormalTok{x<-}\KeywordTok{c}\NormalTok{(1L,2L,3L)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "integer"
\end{verbatim}

\hypertarget{reading-and-writing-data}{%
\section{Reading and writing data}\label{reading-and-writing-data}}

Most of the genomics data sets are in the form of genomic intervals associated with a score. That means mostly the data will be in table format with columns denoting chromosome, start positions, end positions, strand and score. One of the popular formats is the BED format, which is used primarily by the UCSC genome browser \index{UCSC Genome Browser} but most other genome browsers and tools will support the BED file format\index{BED file}. We have all the annotation data in BED format. You will read more about data formats in Chapter \ref{genomicIntervals}. In R, you can easily read tabular format data with the \texttt{read.table()} function. \index{R Programming Language!reading in data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{enhancerFilePath=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                \StringTok{"subset.enhancers.hg18.bed"}\NormalTok{,}
                \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{cpgiFilePath=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                \StringTok{"subset.cpgi.hg18.bed"}\NormalTok{,}
                \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\CommentTok{# read enhancer marker BED file}
\NormalTok{enh.df <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(enhancerFilePath, }\DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{) }

\CommentTok{# read CpG island BED file}
\NormalTok{cpgi.df <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(cpgiFilePath, }\DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{) }

\CommentTok{# check first lines to see how the data looks like}
\KeywordTok{head}\NormalTok{(enh.df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      V1     V2     V3 V4   V5 V6    V7      V8 V9
## 1 chr20 266275 267925  . 1000  .  9.11 13.1693 -1
## 2 chr20 287400 294500  . 1000  . 10.53 13.0231 -1
## 3 chr20 300500 302500  . 1000  .  9.10 13.3935 -1
## 4 chr20 330400 331800  . 1000  .  6.39 13.5105 -1
## 5 chr20 341425 343400  . 1000  .  6.20 12.9852 -1
## 6 chr20 437975 439900  . 1000  .  6.31 13.5184 -1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(cpgi.df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      V1     V2     V3       V4
## 1 chr20 195575 195851  CpG:_28
## 2 chr20 207789 208148  CpG:_32
## 3 chr20 219055 219437  CpG:_33
## 4 chr20 225831 227155 CpG:_135
## 5 chr20 252826 256323 CpG:_286
## 6 chr20 275376 276977 CpG:_116
\end{verbatim}

You can save your data by writing it to disk as a text file. A data frame or matrix can be written out by using the \texttt{write.table()} function. Now let us write out \texttt{cpgi.df}. We will write it out as a tab-separated file; pay attention to the arguments.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write.table}\NormalTok{(cpgi.df,}\DataTypeTok{file=}\StringTok{"cpgi.txt"}\NormalTok{,}\DataTypeTok{quote=}\OtherTok{FALSE}\NormalTok{,}
            \DataTypeTok{row.names=}\OtherTok{FALSE}\NormalTok{,}\DataTypeTok{col.names=}\OtherTok{FALSE}\NormalTok{,}\DataTypeTok{sep=}\StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can save your R objects directly into a file using \texttt{save()} and \texttt{saveRDS()} and load them back in with \texttt{load()} and \texttt{readRDS()}. By using these functions you can save any R object whether or not it is in data frame or matrix classes.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{save}\NormalTok{(cpgi.df,enh.df,}\DataTypeTok{file=}\StringTok{"mydata.RData"}\NormalTok{)}
\KeywordTok{load}\NormalTok{(}\StringTok{"mydata.RData"}\NormalTok{)}
\CommentTok{# saveRDS() can save one object at a type}
\KeywordTok{saveRDS}\NormalTok{(cpgi.df,}\DataTypeTok{file=}\StringTok{"cpgi.rds"}\NormalTok{)}
\NormalTok{x=}\KeywordTok{readRDS}\NormalTok{(}\StringTok{"cpgi.rds"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

One important thing is that with \texttt{save()} you can save many objects at a time, and when they are loaded into memory with \texttt{load()} they retain their variable names. For example, in the above code when you use \texttt{load("mydata.RData")} in a fresh R session, an object named \texttt{cpg.df} will be created. That means you have to figure out what name you gave to the objects before saving them. Conversely, when you save an object by \texttt{saveRDS()} and read by \texttt{readRDS()}, the name of the object is not retained, and you need to assign the output of \texttt{readRDS()} to a new variable (\texttt{x} in the above code chunk).\index{R Programming Language!writing data}

\hypertarget{reading-large-files}{%
\subsection{Reading large files}\label{reading-large-files}}

Reading large files that contain tables with base R function \texttt{read.table()} might take a very long time. Therefore, there are additional packages that provide faster functions to read the files. The \texttt{data.table} \index{R Packages!\texttt{data.table}} and \texttt{readr} \index{R Packages!\texttt{readr}}packages provide this functionality. Below, we show how to use them. These functions with provided parameters will return equivalent output to the \texttt{read.table()} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(data.table)}
\NormalTok{df.f=}\KeywordTok{d}\NormalTok{(enhancerFilePath, }\DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{,}\DataTypeTok{data.table=}\OtherTok{FALSE}\NormalTok{)}

\KeywordTok{library}\NormalTok{(readr)}
\NormalTok{df.f2=}\KeywordTok{read_table}\NormalTok{(enhancerFilePath, }\DataTypeTok{col_names =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{plotting-in-r-with-base-graphics}{%
\section{Plotting in R with base graphics}\label{plotting-in-r-with-base-graphics}}

R has great support for plotting and customizing plots by default. This basic capability for plotting in R is referred to as ``base graphics'' or ``R base graphics''. We will show only a few below. Let us sample 50 values from the normal distribution \index{normal distribution} and plot them as a histogram. A histogram is an approximate representation of a distribution. Bars show how frequently we observe certain values in our sample.\index{R Programming Language!plotting} The resulting histogram from the code chunk below is shown in Figure \ref{fig:sampleForPlots}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# sample 50 values from normal distribution}
\CommentTok{# and store them in vector x}
\NormalTok{x<-}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{50}\NormalTok{)}
\KeywordTok{hist}\NormalTok{(x) }\CommentTok{# plot the histogram of those values}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{02-intro2R_files/figure-latex/sampleForPlots-1} 

}

\caption{Histogram of values sampled from normal distribution.}\label{fig:sampleForPlots}
\end{figure}

We can modify all the plots by providing certain arguments to the plotting function. Now let's give a title to the plot using the \texttt{main} argument. We can also change the color of the bars using the \texttt{col} argument. You can simply provide the name of the color. Below, we are using \texttt{\textquotesingle{}red\textquotesingle{}} for the color. See Figure \ref{fig:makeHist} for the result of this code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(x,}\DataTypeTok{main=}\StringTok{"Hello histogram!!!"}\NormalTok{,}\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{02-intro2R_files/figure-latex/makeHist-1} 

}

\caption{Histogram in red color.}\label{fig:makeHist}
\end{figure}

Next, we will make a scatter plot. Scatter plots are one the most common plots you will encounter in data analysis. We will sample another set of 50 values and plot those against the ones we sampled earlier. The scatter plot shows values of two variables for a set of data points. It is useful to visualize relationships between two variables. It is frequently used in connection with correlation and linear regression. There are other variants of scatter plots which show density of the points with different colors. We will show examples of those scatter plots in later chapters. The scatter plot from our sampling experiment is shown in Figure \ref{fig:makeScatter}. Notice that, in addition to \texttt{main} argument we used \texttt{xlab} and \texttt{ylab} arguments to give labels to the plot. You can customize the plots even more than this. See \texttt{?plot} and \texttt{?par} for more arguments that can help you customize the plots.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# randomly sample 50 points from normal distribution}
\NormalTok{y<-}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{50}\NormalTok{)}
\CommentTok{#plot a scatter plot}
\CommentTok{# control x-axis and y-axis labels}
\KeywordTok{plot}\NormalTok{(x,y,}\DataTypeTok{main=}\StringTok{"scatterplot of random samples"}\NormalTok{,}
        \DataTypeTok{ylab=}\StringTok{"y values"}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"x values"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{02-intro2R_files/figure-latex/makeScatter-1} 

}

\caption{Scatter plot example.}\label{fig:makeScatter}
\end{figure}

We can also plot boxplots for vectors x and y. Boxplots depict groups of numerical data through their quartiles. The edges of the box denote the 1st and 3rd quartiles, and the line that crosses the box is the median. The distance between the 1st and the 3rd quartiles is called interquartile tange. The whiskers (lines extending from the boxes) are usually defined using the interquartile range for symmetric distributions as follows: \texttt{lowerWhisker=Q1-1.5{[}IQR{]}} and \texttt{upperWhisker=Q3+1.5{[}IQR{]}}.

In addition, outliers can be depicted as dots. In this case, outliers are the values that remain outside the whiskers. The resulting plot from the code snippet below is shown in Figure \ref{fig:makeBoxplot}.

\begin{Shaded}
\begin{Highlighting}[]
 \KeywordTok{boxplot}\NormalTok{(x,y,}\DataTypeTok{main=}\StringTok{"boxplots of random samples"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{02-intro2R_files/figure-latex/makeBoxplot-1} 

}

\caption{Boxplot example}\label{fig:makeBoxplot}
\end{figure}

Next up is the bar plot, which you can plot using the \texttt{barplot()} function. We are going to plot four imaginary percentage values and color them with two colors, and this time we will also show how to draw a legend on the plot using the \texttt{legend()} function. The resulting plot is in Figure \ref{fig:makebarplot}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{perc=}\KeywordTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{70}\NormalTok{,}\DecValTok{35}\NormalTok{,}\DecValTok{25}\NormalTok{)}
\KeywordTok{barplot}\NormalTok{(}\DataTypeTok{height=}\NormalTok{perc,}
        \DataTypeTok{names.arg=}\KeywordTok{c}\NormalTok{(}\StringTok{"CpGi"}\NormalTok{,}\StringTok{"exon"}\NormalTok{,}\StringTok{"CpGi"}\NormalTok{,}\StringTok{"exon"}\NormalTok{),}
        \DataTypeTok{ylab=}\StringTok{"percentages"}\NormalTok{,}\DataTypeTok{main=}\StringTok{"imagine %s"}\NormalTok{,}
        \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{,}\StringTok{"red"}\NormalTok{,}\StringTok{"blue"}\NormalTok{,}\StringTok{"blue"}\NormalTok{))}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{,}\DataTypeTok{legend=}\KeywordTok{c}\NormalTok{(}\StringTok{"test"}\NormalTok{,}\StringTok{"control"}\NormalTok{),}
       \DataTypeTok{fill=}\KeywordTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{,}\StringTok{"blue"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{02-intro2R_files/figure-latex/makebarplot-1} 

}

\caption{Bar plot example}\label{fig:makebarplot}
\end{figure}

\hypertarget{combining-multiple-plots}{%
\subsection{Combining multiple plots}\label{combining-multiple-plots}}

In R, we can combine multiple plots in the same graphic. For this purpose, we use the \texttt{par()} function for simple combinations. More complicated arrangements with different sizes of sub-plots can be created with the \texttt{layout()} function. Below we will show how to combine two plots side-by-side using \texttt{par(mfrow=c(1,2))}. The \texttt{mfrow=c(nrows,\ ncols)} construct will create a matrix of \texttt{nrows} x \texttt{ncols} plots that are filled in by row. The following code will produce a histogram and a scatter plot stacked side by side. The result is shown in Figure \ref{fig:combineBasePlots}. If you want to see the plots on top of each other, simply change \texttt{mfrow=c(1,2)} to \texttt{mfrow=c(2,1)}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)) }\CommentTok{# }

\CommentTok{# make the plots}
\KeywordTok{hist}\NormalTok{(x,}\DataTypeTok{main=}\StringTok{"Hello histogram!!!"}\NormalTok{,}\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(x,y,}\DataTypeTok{main=}\StringTok{"scatterplot"}\NormalTok{,}
        \DataTypeTok{ylab=}\StringTok{"y values"}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"x values"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{02-intro2R_files/figure-latex/combineBasePlots-1} 

}

\caption{Combining two plots, a histogram and a scatter plot, with `par()` function.}\label{fig:combineBasePlots}
\end{figure}

\hypertarget{saving-plots}{%
\subsection{Saving plots}\label{saving-plots}}

If you want to save your plots to an image file there are couple of ways of doing that. Normally, you will have to do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Open a graphics device.
\item
  Create the plot.
\item
  Close the graphics device.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pdf}\NormalTok{(}\StringTok{"mygraphs/myplot.pdf"}\NormalTok{,}\DataTypeTok{width=}\DecValTok{5}\NormalTok{,}\DataTypeTok{height=}\DecValTok{5}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(x,y)}
\KeywordTok{dev.off}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Alternatively, you can first create the plot then copy the plot to a graphics device.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(x,y)}
\KeywordTok{dev.copy}\NormalTok{(pdf,}\StringTok{"mygraphs/myplot.pdf"}\NormalTok{,}\DataTypeTok{width=}\DecValTok{7}\NormalTok{,}\DataTypeTok{height=}\DecValTok{5}\NormalTok{)}
\KeywordTok{dev.off}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{plotting-in-r-with-ggplot2}{%
\section{Plotting in R with ggplot2}\label{plotting-in-r-with-ggplot2}}

In R, there are other plotting systems besides ``base graphics'', which is what we have shown until now. There is another popular plotting system called \texttt{ggplot2}\index{R Packages!\texttt{ggplot2}} which implements a different logic when constructing the plots. This system or logic is known as the ``grammar of graphics''. This system defines a plot or graphics as a combination of different components. For example, in the scatter plot in \ref{fig:makeScatter}, we have the points which are geometric shapes, we have the coordinate system and scales of data. In addition, data transformations are also part of a plot. In Figure \ref{fig:makeHist}, the histogram has a binning operation and it puts the data into bins before displaying it as geometric shapes, the bars. The \texttt{ggplot2} system and its implementation of ``grammar of graphics''\footnote{This is a concept developed by Leland Wilkinson and popularized in R community by Hadley Wickham: \url{https://doi.org/10.1198/jcgs.2009.07098}} allows us to build the plot layer by layer using the predefined components.

Next we will see how this works in practice. Let's start with a simple scatter plot using \texttt{ggplot2}. In order to make basic plots in \texttt{ggplot2}, one needs to combine different components. First, we need the data and its transformation to a geometric object; for a scatter plot this would be mapping data to points, for histograms it would be binning the data and making bars. Second, we need the scales and coordinate system, which generates axes and legends so that we can see the values on the plot. And the last component is the plot annotation such as plot title and the background.

The main \texttt{ggplot2} function, called \texttt{ggplot()}, requires a data frame to work with, and this data frame is its first argument as shown in the code snippet below. The second thing you will notice is the \texttt{aes()} function in the \texttt{ggplot()} function. This function defines which columns in the data frame map to x and y coordinates and if they should be colored or have different shapes based on the values in a different column. These elements are the ``aesthetic'' elements, this is what we observe in the plot. The last line in the code represents the geometric object to be plotted. These geometric objects define the type of the plot. In this case, the object is a point, indicated by the \texttt{geom\_point()}function. Another, peculiar thing in the code is the \texttt{+} operation. In \texttt{ggplot2}, this operation is used to add layers and modify the plot. The resulting scatter plot from the code snippet below can be seen in Figure \ref{fig:ggScatterchp3}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}

\NormalTok{myData=}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{col1=}\NormalTok{x,}\DataTypeTok{col2=}\NormalTok{y)}

 \CommentTok{# the data is myData and I’m using col1 and col2 }
\CommentTok{# columns on x and y axes}
\KeywordTok{ggplot}\NormalTok{(myData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{col1, }\DataTypeTok{y=}\NormalTok{col2)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\CommentTok{# map x and y as points}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{02-intro2R_files/figure-latex/ggScatterchp3-1} 

}

\caption{Scatter plot with ggplot2}\label{fig:ggScatterchp3}
\end{figure}

Now, let's re-create the histogram we created before. For this, we will start again with the \texttt{ggplot()} function. We are interested only in the x-axis in the histogram, so we will only use one column of the data frame. Then, we will add the histogram layer with the \texttt{geom\_histogram()} function. In addition, we will be showing how to modify your plot further by adding an additional layer with the \texttt{labs()} function, which controls the axis labels and titles. The resulting plot from the code chunk below is shown in Figure \ref{fig:ggHistChp3}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(myData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{col1)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{() }\OperatorTok{+}\StringTok{ }\CommentTok{# map x and y as points}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title=}\StringTok{"Histogram for a random variable"}\NormalTok{, }\DataTypeTok{x=}\StringTok{"my variable"}\NormalTok{, }\DataTypeTok{y=}\StringTok{"Count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{02-intro2R_files/figure-latex/ggHistChp3-1} 

}

\caption{Histograms made with ggplot2, the left histogram contains additional modifications introduced by `labs()` function.}\label{fig:ggHistChp3}
\end{figure}

We can also plot boxplots using \texttt{ggplot2}. Let's re-create the boxplot we did in Figure \ref{fig:makeBoxplot}. This time we will have to put all our data into a single data frame with extra columns denoting the group of our values. In the base graphics case, we could just input variables containing different vectors. However, \texttt{ggplot2} does not work like that and we need to create a data frame with the right format to use the \texttt{ggplot()} function. Below, we first concatenate the \texttt{x} and \texttt{y} vectors and create a second column denoting the group for the vectors. In this case, the x-axis will be the ``group'' variable which is just a character denoting the group, and the y-axis will be the numeric ``values'' for the \texttt{x} and \texttt{y} vectors. You can see how this is passed to the \texttt{aes()} function below. The resulting plot is shown in Figure \ref{fig:ggBoxplotchp3}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# data frame with group column showing which }
\CommentTok{# groups the vector x and y belong}
\NormalTok{myData2=}\KeywordTok{rbind}\NormalTok{(}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{values=}\NormalTok{x,}\DataTypeTok{group=}\StringTok{"x"}\NormalTok{),}
              \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{values=}\NormalTok{y,}\DataTypeTok{group=}\StringTok{"y"}\NormalTok{))}

\CommentTok{# x-axis will be group and y-axis will be values}
\KeywordTok{ggplot}\NormalTok{(myData2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{group,}\DataTypeTok{y=}\NormalTok{values)) }\OperatorTok{+}\StringTok{ }
\StringTok{         }\KeywordTok{geom_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{02-intro2R_files/figure-latex/ggBoxplotchp3-1} 

}

\caption{Boxplots using ggplot2.}\label{fig:ggBoxplotchp3}
\end{figure}

\hypertarget{combining-multiple-plots-1}{%
\subsection{Combining multiple plots}\label{combining-multiple-plots-1}}

There are different options for combining multiple plots. If we are trying to make similar plots for the subsets of the same data set, we can use faceting. This is a built-in and very useful feature of \texttt{ggplot2}. This feature is frequently used when investigating whether patterns are the same or different in different conditions or subsets of the data. It can be used via the \texttt{facet\_grid()} function. Below, we will make two histograms faceted by the \texttt{group} variable in the input data frame. We will be using the same data frame we created for the boxplot in the previous section. The resulting plot is in Figure \ref{fig:facetHistChp3}.



\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(myData2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{values)) }\OperatorTok{+}\StringTok{ }
\StringTok{         }\KeywordTok{geom_histogram}\NormalTok{() }\OperatorTok{+}\KeywordTok{facet_grid}\NormalTok{(.}\OperatorTok{~}\NormalTok{group)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{02-intro2R_files/figure-latex/facetHistChp3-1} 

}

\caption{Combining two plots using \texttt{ggplot2::facet\_grid()} function.}\label{fig:facetHistChp3}
\end{figure}

Faceting only works when you are using the subsets of the same data set. However, you may want to combine different types of plots from different data sets. The base R functions such as \texttt{par()} and \texttt{layout()} will not work with \texttt{ggplot2} because it uses a different graphics system and this system does not recognize base R functionality for plotting. However, there are multiple ways you can combine plots from \texttt{ggplot2}. One way is using the \texttt{cowplot} package. This package aligns the individual plots in a grid and will help you create publication-ready compound plots. Below, we will show how to combine a histogram and a scatter plot side by side. The resulting plot is shown in Figure \ref{fig:cowPlotChp3}.



\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(cowplot)}
\CommentTok{# histogram}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(myData2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{values,}\DataTypeTok{fill=}\NormalTok{group)) }\OperatorTok{+}\StringTok{ }
\StringTok{        }\KeywordTok{geom_histogram}\NormalTok{()}
\CommentTok{# scatterplot}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(myData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{col1, }\DataTypeTok{y=}\NormalTok{col2)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }

\CommentTok{# plot two plots in a grid and label them as A and B}
\KeywordTok{plot_grid}\NormalTok{(p1, p2, }\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{'A'}\NormalTok{, }\StringTok{'B'}\NormalTok{), }\DataTypeTok{label_size =} \DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{02-intro2R_files/figure-latex/cowPlotChp3-1} 

}

\caption{Combining a histogram and scatter plot using \texttt{cowplot} package. The plots are labeled as A and B using the arguments in \texttt{plot\_grid()} function.}\label{fig:cowPlotChp3}
\end{figure}

\hypertarget{ggplot2-and-tidyverse}{%
\subsection{ggplot2 and tidyverse}\label{ggplot2-and-tidyverse}}

\texttt{ggplot2} is actually part of a larger ecosystem. You will need packages from this ecosystem when you want to use \texttt{ggplot2} in a more sophisticated manner or if you need additional functionality that is not readily available in base R or other packages. For example, when you want to make more complicated plots using \texttt{ggplot2}, you will need to modify your data frames to the formats required by the \texttt{ggplot()} function, and you will need to learn about the \texttt{dplyr}\index{R Packages!\texttt{dplyr}} and \texttt{tidyr}\index{R Packages!\texttt{tidyr}} packages for data formatting purposes. If you are working with character strings, \texttt{stringr} package might have functionality that is not available in base R. There are many more packages that users find useful in \texttt{tidyverse} and it could be important to know about this ecosystem of R packages.

\begin{rmdtip}
\begin{rmdtip}

\textbf{Want to know more ?}

\begin{itemize}
\tightlist
\item
  \texttt{ggplot2} has a free online book written by Hadley Wickham: \url{https://ggplot2-book.org/}\\
\item
  The \texttt{tidyverse} packages and the ecosystem is described in their website: \url{https://www.tidyverse.org/}. There you will find extensive documentation and resources on \texttt{tidyverse} packages.
\end{itemize}

\end{rmdtip}
\end{rmdtip}

\hypertarget{functions-and-control-structures-for-ifelse-etc.}{%
\section{Functions and control structures (for, if/else etc.)}\label{functions-and-control-structures-for-ifelse-etc.}}

\hypertarget{user-defined-functions}{%
\subsection{User-defined functions}\label{user-defined-functions}}

Functions are useful for transforming larger chunks of code to re-usable pieces of code. Generally, if you need to execute certain tasks with variable parameters, then it is time you write a function. A function in R takes different arguments and returns a definite output, much like mathematical functions. Here is a simple function that takes two arguments, \texttt{x} and \texttt{y}, and returns the sum of their squares\index{R Programming Language!functions}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sqSum<-}\ControlFlowTok{function}\NormalTok{(x,y)\{}
\NormalTok{result=x}\OperatorTok{^}\DecValTok{2}\OperatorTok{+}\NormalTok{y}\OperatorTok{^}\DecValTok{2}
\KeywordTok{return}\NormalTok{(result)}
\NormalTok{\}}
\CommentTok{# now try the function out}
\KeywordTok{sqSum}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 13
\end{verbatim}

Functions can also output plots and/or messages to the terminal. Here is a function that prints a message to the terminal:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sqSumPrint<-}\ControlFlowTok{function}\NormalTok{(x,y)\{}
\NormalTok{result=x}\OperatorTok{^}\DecValTok{2}\OperatorTok{+}\NormalTok{y}\OperatorTok{^}\DecValTok{2}
\KeywordTok{cat}\NormalTok{(}\StringTok{"here is the result:"}\NormalTok{,result,}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{\}}
\CommentTok{# now try the function out}
\KeywordTok{sqSumPrint}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## here is the result: 13
\end{verbatim}

Sometimes we would want to execute a certain part of the code only if a certain condition is satisfied. This condition can be anything from the type of an object (Ex: if the object is a matrix, execute certain code), or it can be more complicated, such as if the object value is between certain thresholds. Let us see how these if statements can be used. They can be used anywhere in your code; now we will use it in a function to decide if the CpG island is large, normal length or short.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cpgi.df <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\StringTok{"intro2R_data/data/subset.cpgi.hg18.bed"}\NormalTok{, }\DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{)}
\CommentTok{# function takes input one row}
\CommentTok{# of CpGi data frame}
\NormalTok{largeCpGi<-}\ControlFlowTok{function}\NormalTok{(bedRow)\{}
\NormalTok{ cpglen=bedRow[}\DecValTok{3}\NormalTok{]}\OperatorTok{-}\NormalTok{bedRow[}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\DecValTok{1}
 \ControlFlowTok{if}\NormalTok{(cpglen}\OperatorTok{>}\DecValTok{1500}\NormalTok{)\{}
    \KeywordTok{cat}\NormalTok{(}\StringTok{"this is large}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{ \}}
 \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{(cpglen}\OperatorTok{<=}\DecValTok{1500} \OperatorTok{&}\StringTok{ }\NormalTok{cpglen}\OperatorTok{>}\DecValTok{700}\NormalTok{)\{}
    \KeywordTok{cat}\NormalTok{(}\StringTok{"this is normal}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{ \}}
 \ControlFlowTok{else}\NormalTok{\{}
    \KeywordTok{cat}\NormalTok{(}\StringTok{"this is short}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{ \}}
\NormalTok{\}}
\KeywordTok{largeCpGi}\NormalTok{(cpgi.df[}\DecValTok{10}\NormalTok{,])}
\KeywordTok{largeCpGi}\NormalTok{(cpgi.df[}\DecValTok{100}\NormalTok{,])}
\KeywordTok{largeCpGi}\NormalTok{(cpgi.df[}\DecValTok{1000}\NormalTok{,])}
\end{Highlighting}
\end{Shaded}

\hypertarget{loops-and-looping-structures-in-r}{%
\subsection{Loops and looping structures in R}\label{loops-and-looping-structures-in-r}}

When you need to repeat a certain task or execute a function multiple times, you can do that with the help of loops. A loop will execute the task until a certain condition is reached. The loop below is called a ``for-loop'' and it executes the task sequentially 10 times.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{)\{ }\CommentTok{# number of repetitions}
\KeywordTok{cat}\NormalTok{(}\StringTok{"This is iteration"}\NormalTok{) }\CommentTok{# the task to be repeated}
\KeywordTok{print}\NormalTok{(i)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## This is iteration[1] 1
## This is iteration[1] 2
## This is iteration[1] 3
## This is iteration[1] 4
## This is iteration[1] 5
## This is iteration[1] 6
## This is iteration[1] 7
## This is iteration[1] 8
## This is iteration[1] 9
## This is iteration[1] 10
\end{verbatim}

The task above is a bit pointless. Normally in a loop, you would want to do something meaningful. Let us calculate the length of the CpG islands we read in earlier. Although this is not the most efficient way of doing that particular task, it serves as a good example for looping. The code below will execute a hundred times, and it will calculate the length of the CpG islands for the first 100 islands in
the data frame (by subtracting the end coordinate from the start coordinate).\index{R Programming Language!loops}

\textbf{Note:}If you are going to run a loop that has a lot of repetitions, it is smart to try the loop with few repetitions first and check the results. This will help you make sure the code in the loop works before executing it thousands of times.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this is where we will keep the lenghts}
\CommentTok{# for now it is an empty vector}
\NormalTok{result=}\KeywordTok{c}\NormalTok{()}
\CommentTok{# start the loop}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{)\{}
    \CommentTok{#calculate the length}
\NormalTok{    len=cpgi.df[i,}\DecValTok{3}\NormalTok{]}\OperatorTok{-}\NormalTok{cpgi.df[i,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\DecValTok{1}
    \CommentTok{#append the length to the result}
\NormalTok{    result=}\KeywordTok{c}\NormalTok{(result,len)}
\NormalTok{\}}
\CommentTok{# check the results}
\KeywordTok{head}\NormalTok{(result)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  277  360  383 1325 3498 1602
\end{verbatim}

\hypertarget{apply-family-functions-instead-of-loops}{%
\subsubsection{Apply family functions instead of loops}\label{apply-family-functions-instead-of-loops}}

R has other ways of repeating tasks, which tend to be more efficient than using loops. They are known collectively as the ``apply'' family of functions, which include \texttt{apply}, \texttt{lapply},\texttt{mapply} and \texttt{tapply} (and some other variants). All of these functions apply a given function to a set of instances and return the results of those functions for each instance. The difference between them is that they take different types of inputs. For example, \texttt{apply} works on data frames or matrices and applies the function on each row or column of the data structure. \texttt{lapply} works on lists or vectors and applies a function which takes the list element as an argument. Next we will demonstrate how to use \texttt{apply()} on a matrix. The example applies the sum function on the rows of a matrix; it basically sums up the values on each row of the matrix, which is conceptualized in Figure \ref{fig:applyConcept}.\index{R Programming Language!apply family functions}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/apply} 

}

\caption{apply() concept in R.}\label{fig:applyConcept}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat=}\KeywordTok{cbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{3}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\NormalTok{result<-}\KeywordTok{apply}\NormalTok{(mat,}\DecValTok{1}\NormalTok{,sum)}
\NormalTok{result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12  3  5  6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# OR you can define the function as an argument to apply()}
\NormalTok{result<-}\KeywordTok{apply}\NormalTok{(mat,}\DecValTok{1}\NormalTok{,}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{(x))}
\NormalTok{result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12  3  5  6
\end{verbatim}

Notice that we used a second argument which equals 1, that indicates that rows of the matrix/ data frame will be the input for the function. If we change the second argument to 2, this will indicate that columns should be the input for the function that will be applied. See Figure \ref{fig:applyConcept2} for the visualization of apply() on columns.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/apply2} 

}

\caption{apply() function on columns}\label{fig:applyConcept2}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result<-}\KeywordTok{apply}\NormalTok{(mat,}\DecValTok{2}\NormalTok{,sum)}
\NormalTok{result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9 3 6 2 3 3
\end{verbatim}

Next, we will use \texttt{lapply()}, which applies a function on a list or a vector. The function that will be applied is a simple function that takes the square of a given number.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{input=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\KeywordTok{lapply}\NormalTok{(input,}\ControlFlowTok{function}\NormalTok{(x) x}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] 1
## 
## [[2]]
## [1] 4
## 
## [[3]]
## [1] 9
\end{verbatim}

\texttt{mapply()} is another member of the \texttt{apply} family, it can apply a function on an unlimited set of vectors/lists, it is like a version of \texttt{lapply} that can handle multiple vectors as arguments. In this case, the argument to the \texttt{mapply()} is the function to be applied and the sets of parameters to be supplied as arguments of the function. As shown in the conceptualized Figure \ref{fig:mapplyConcept}, the function to be applied is a function that takes two arguments and sums them up. The arguments to be summed up are in the format of vectors \texttt{Xs} and \texttt{Ys}. \texttt{mapply()} applies the summation function to each pair in the \texttt{Xs} and \texttt{Ys} vector. Notice that the order of the input function and extra arguments are different for \texttt{mapply}.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/mapply} 

}

\caption{mapply() concept.}\label{fig:mapplyConcept}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Xs=}\DecValTok{0}\OperatorTok{:}\DecValTok{5}
\NormalTok{Ys=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\NormalTok{result<-}\KeywordTok{mapply}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x,y) }\KeywordTok{sum}\NormalTok{(x,y),Xs,Ys)}
\NormalTok{result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2 3 4 6 7 8
\end{verbatim}

\hypertarget{apply-family-functions-on-multiple-cores}{%
\subsubsection{Apply family functions on multiple cores}\label{apply-family-functions-on-multiple-cores}}

If you have large data sets, apply family functions can be slow (although probably still better than for loops). If that is the case, you can easily use the parallel versions of those functions from the parallel package. These functions essentially divide your tasks to smaller chunks, run them on separate CPUs, and merge the results from those parallel operations. This concept is visualized in Figure below \ref{fig:mcapplyConcept}, \texttt{mcapply} runs the summation function on three different processors. Each processor executes the summation function on a part of the data set, and the results are merged and returned as a single vector that has the same order as the input parameters Xs and Ys.\index{R Programming Language!apply family functions}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/mcmapply} 

}

\caption{mcapply() concept.}\label{fig:mcapplyConcept}
\end{figure}

\hypertarget{vectorized-functions-in-r}{%
\subsubsection{Vectorized functions in R}\label{vectorized-functions-in-r}}

The above examples have been put forward to illustrate functions and loops in R because functions using sum() are not complicated and are easy to understand. You will probably need to use loops and looping structures with more complicated functions. In reality, most of the operations we used do not need the use of loops or looping structures because there are already vectorized functions that can achieve the same outcomes, meaning if the input arguments are R vectors, the output will be a vector as well, so no need for loops or vectorization.

For example, instead of using \texttt{mapply()} and \texttt{sum()} functions, we can just use the \texttt{+} operator and sum up \texttt{Xs} and \texttt{Ys}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result=Xs}\OperatorTok{+}\NormalTok{Ys}
\NormalTok{result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2 3 4 6 7 8
\end{verbatim}

In order to get the column or row sums, we can use the vectorized functions \texttt{colSums()} and \texttt{rowSums()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{colSums}\NormalTok{(mat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9 3 6 2 3 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rowSums}\NormalTok{(mat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12  3  5  6
\end{verbatim}

However, remember that not every function is vectorized in R, so use the ones that are. But sooner or later, apply family functions will come in handy.

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

\hypertarget{computations-in-r-1}{%
\subsection{Computations in R}\label{computations-in-r-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Sum 2 and 3 using the \texttt{+} operator. {[}Difficulty: \textbf{Beginner}{]}
\item
  Take the square root of 36, use \texttt{sqrt()}. {[}Difficulty: \textbf{Beginner}{]}
\item
  Take the log10 of 1000, use function \texttt{log10()}. {[}Difficulty: \textbf{Beginner}{]}
\item
  Take the log2 of 32, use function \texttt{log2()}. {[}Difficulty: \textbf{Beginner}{]}
\item
  Assign the sum of 2,3 and 4 to variable x. {[}Difficulty: \textbf{Beginner}{]}
\item
  Find the absolute value of the expression \texttt{5\ -\ 145} using the \texttt{abs()} function. {[}Difficulty: \textbf{Beginner}{]}
\item
  Calculate the square root of 625, divide it by 5, and assign it to variable \texttt{x}.Ex: \texttt{y=\ log10(1000)/5}, the previous statement takes log10 of 1000, divides it by 5, and assigns the value to variable y. {[}Difficulty: \textbf{Beginner}{]}
\item
  Multiply the value you get from previous exercise by 10000, assign it to variable x
  Ex: \texttt{y=y*5}, multiplies \texttt{y} by 5 and assigns the value to \texttt{y}.
  \textbf{KEY CONCEPT:} results of computations or arbitrary values can be stored in variables we can re-use those variables later on and over-write them with new values.
  {[}Difficulty: \textbf{Beginner}{]}
\end{enumerate}

\hypertarget{data-structures-in-r}{%
\subsection{Data structures in R}\label{data-structures-in-r}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\item
  Make a vector of 1,2,3,5 and 10 using \texttt{c()}, and assign it to the \texttt{vec} variable. Ex: \texttt{vec1=c(1,3,4)} makes a vector out of 1,3,4. {[}Difficulty: \textbf{Beginner}{]}
\item
  Check the length of your vector with length().
  Ex: \texttt{length(vec1)} should return 3. {[}Difficulty: \textbf{Beginner}{]}
\item
  Make a vector of all numbers between 2 and 15.
  Ex: \texttt{vec=1:6} makes a vector of numbers between 1 and 6, and assigns it to the \texttt{vec} variable. {[}Difficulty: \textbf{Beginner}{]}
\item
  Make a vector of 4s repeated 10 times using the \texttt{rep()} function. Ex: \texttt{rep(x=2,times=5)} makes a vector of 2s repeated 5 times. {[}Difficulty: \textbf{Beginner}{]}
\item
  Make a logical vector with TRUE, FALSE values of length 4, use \texttt{c()}.
  Ex: \texttt{c(TRUE,FALSE)}. {[}Difficulty: \textbf{Beginner}{]}
\item
  Make a character vector of the gene names PAX6,ZIC2,OCT4 and SOX2.
  Ex: \texttt{avec=c("a","b","c")} makes a character vector of a,b and c.~{[}Difficulty: \textbf{Beginner}{]}
\item
  Subset the vector using \texttt{{[}{]}} notation, and get the 5th and 6th elements.
  Ex: \texttt{vec1{[}1{]}} gets the first element. \texttt{vec1{[}c(1,3){]}} gets the 1st and 3rd elements. {[}Difficulty: \textbf{Beginner}{]}
\item
  You can also subset any vector using a logical vector in \texttt{{[}{]}}. Run the following:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myvec=}\DecValTok{1}\OperatorTok{:}\DecValTok{5}
\CommentTok{# the length of the logical vector }
\CommentTok{# should be equal to length(myvec) }
\NormalTok{myvec[}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{,}\OtherTok{TRUE}\NormalTok{,}\OtherTok{FALSE}\NormalTok{,}\OtherTok{FALSE}\NormalTok{,}\OtherTok{FALSE}\NormalTok{)] }
\NormalTok{myvec[}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{,}\OtherTok{FALSE}\NormalTok{,}\OtherTok{FALSE}\NormalTok{,}\OtherTok{FALSE}\NormalTok{,}\OtherTok{TRUE}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

{[}Difficulty: \textbf{Beginner}{]}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{17}
\tightlist
\item
  \texttt{==,\textgreater{},\textless{},\ \textgreater{}=,\ \textless{}=} operators create logical vectors. See the results of the following operations:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myvec }\OperatorTok{>}\StringTok{ }\DecValTok{3}
\NormalTok{myvec }\OperatorTok{==}\StringTok{ }\DecValTok{4}
\NormalTok{myvec }\OperatorTok{<=}\StringTok{ }\DecValTok{2}
\NormalTok{myvec }\OperatorTok{!=}\StringTok{ }\DecValTok{4}
\end{Highlighting}
\end{Shaded}

{[}Difficulty: \textbf{Beginner}{]}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{18}
\item
  Use the \texttt{\textgreater{}} operator in \texttt{myvec{[}\ {]}} to get elements larger than 2 in \texttt{myvec} which is described above. {[}Difficulty: \textbf{Beginner}{]}
\item
  Make a 5x3 matrix (5 rows, 3 columns) using \texttt{matrix()}.
  Ex: \texttt{matrix(1:6,nrow=3,ncol=2)} makes a 3x2 matrix using numbers between 1 and 6. {[}Difficulty: \textbf{Beginner}{]}
\item
  What happens when you use \texttt{byrow\ =\ TRUE} in your matrix() as an additional argument?
  Ex: \texttt{mat=matrix(1:6,nrow=3,ncol=2,byrow\ =\ TRUE)}. {[}Difficulty: \textbf{Beginner}{]}
\item
  Extract the first 3 columns and first 3 rows of your matrix using \texttt{{[}{]}} notation. {[}Difficulty: \textbf{Beginner}{]}
\item
  Extract the last two rows of the matrix you created earlier.
  Ex: \texttt{mat{[}2:3,{]}} or \texttt{mat{[}c(2,3),{]}} extracts the 2nd and 3rd rows.
  {[}Difficulty: \textbf{Beginner}{]}
\item
  Extract the first two columns and run \texttt{class()} on the result.
  {[}Difficulty: \textbf{Beginner}{]}
\item
  Extract the first column and run \texttt{class()} on the result, compare with the above exercise.
  {[}Difficulty: \textbf{Beginner}{]}
\item
  Make a data frame with 3 columns and 5 rows. Make sure first column is a sequence
  of numbers 1:5, and second column is a character vector.
  Ex: \texttt{df=data.frame(col1=1:3,col2=c("a","b","c"),col3=3:1)\ \#\ 3x3\ data\ frame}.
  Remember you need to make a 3x5 data frame. {[}Difficulty: \textbf{Beginner}{]}
\item
  Extract the first two columns and first two rows.
  \textbf{HINT:} Use the same notation as matrices. {[}Difficulty: \textbf{Beginner}{]}
\item
  Extract the last two rows of the data frame you made.
  \textbf{HINT:} Same notation as matrices. {[}Difficulty: \textbf{Beginner}{]}
\item
  Extract the last two columns using the column names of the data frame you made. {[}Difficulty: \textbf{Beginner}{]}
\item
  Extract the second column using the column names.
  You can use \texttt{{[}{]}} or \texttt{\$} as in lists; use both in two different answers. {[}Difficulty: \textbf{Beginner}{]}
\item
  Extract rows where the 1st column is larger than 3.
  \textbf{HINT:} You can get a logical vector using the \texttt{\textgreater{}} operator
  , and logical vectors can be used in \texttt{{[}{]}} when subsetting. {[}Difficulty: \textbf{Beginner}{]}
\item
  Extract rows where the 1st column is larger than or equal to 3.
  {[}Difficulty: \textbf{Beginner}{]}
\item
  Convert a data frame to the matrix. \textbf{HINT:} Use \texttt{as.matrix()}.
  Observe what happens to numeric values in the data frame. {[}Difficulty: \textbf{Beginner}{]}
\item
  Make a list using the \texttt{list()} function. Your list should have 4 elements;
  the one below has 2. Ex: \texttt{mylist=\ list(a=c(1,2,3),b=c("apple,"orange"))}
  {[}Difficulty: \textbf{Beginner}{]}
\item
  Select the 1st element of the list you made using \texttt{\$} notation.
  Ex: \texttt{mylist\$a} selects first element named ``a''.
  {[}Difficulty: \textbf{Beginner}{]}
\item
  Select the 4th element of the list you made earlier using \texttt{\$} notation. {[}Difficulty: \textbf{Beginner}{]}
\item
  Select the 1st element of your list using \texttt{{[}\ {]}} notation.
  Ex: \texttt{mylist{[}1{]}} selects the first element named ``a'', and you get a list with one element. \texttt{mylist{[}"a"{]}} selects the first element named ``a'', and you get a list with one element.
  {[}Difficulty: \textbf{Beginner}{]}
\item
  Select the 4th element of your list using \texttt{{[}\ {]}} notation. {[}Difficulty: \textbf{Beginner}{]}
\item
  Make a factor using factor(), with 5 elements.
  Ex: \texttt{fa=factor(c("a","a","b"))}. {[}Difficulty: \textbf{Beginner}{]}
\item
  Convert a character vector to a factor using \texttt{as.factor()}.
  First, make a character vector using \texttt{c()} then use \texttt{as.factor()}.
  {[}Difficulty: \textbf{Intermediate}{]}
\item
  Convert the factor you made above to a character using \texttt{as.character()}. {[}Difficulty: \textbf{Beginner}{]}
\end{enumerate}

\hypertarget{reading-in-and-writing-data-out-in-r}{%
\subsection{Reading in and writing data out in R}\label{reading-in-and-writing-data-out-in-r}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Read CpG island (CpGi) data from the compGenomRData package \texttt{CpGi.table.hg18.txt}. This is a tab-separated file. Store it in a variable called \texttt{cpgi}. Use
\end{enumerate}

\begin{verbatim}
cpgFilePath=system.file("extdata",
                "CpGi.table.hg18.txt",
                package="compGenomRData")
\end{verbatim}

to get the file path within the installed \texttt{compGenomRData} package. {[}Difficulty: \textbf{Beginner}{]}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Use \texttt{head()} on CpGi to see the first few rows. {[}Difficulty: \textbf{Beginner}{]}
\item
  Why doesn't the following work? See \texttt{sep} argument at \texttt{help(read.table)}. {[}Difficulty: \textbf{Beginner}{]}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cpgtFilePath=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                \StringTok{"CpGi.table.hg18.txt"}\NormalTok{,}
                \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{cpgtFilePath}
\NormalTok{cpgiSepComma=}\KeywordTok{read.table}\NormalTok{(cpgtFilePath,}\DataTypeTok{header=}\OtherTok{TRUE}\NormalTok{,}\DataTypeTok{sep=}\StringTok{","}\NormalTok{)}
\KeywordTok{head}\NormalTok{(cpgiSepComma)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  What happens when you set \texttt{stringsAsFactors=FALSE} in \texttt{read.table()}? {[}Difficulty: \textbf{Beginner}{]}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cpgiHF=}\KeywordTok{read.table}\NormalTok{(}\StringTok{"intro2R_data/data/CpGi.table.hg18.txt"}\NormalTok{,}
                     \DataTypeTok{header=}\OtherTok{FALSE}\NormalTok{,}\DataTypeTok{sep=}\StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{,}
                     \DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\item
  Read only the first 10 rows of the CpGi table. {[}Difficulty: \textbf{Beginner/Intermediate}{]}
\item
  Use \texttt{cpgFilePath=system.file("extdata","CpGi.table.hg18.txt",}
  \texttt{package="compGenomRData")} to get the file path, then use
  \texttt{read.table()} with argument \texttt{header=FALSE}. Use \texttt{head()} to see the results. {[}Difficulty: \textbf{Beginner}{]}
\item
  Write CpG islands to a text file called ``my.cpgi.file.txt''. Write the file
  to your home folder; you can use \texttt{file="\textasciitilde{}/my.cpgi.file.txt"} in linux. \texttt{\textasciitilde{}/} denotes
  home folder.{[}Difficulty: \textbf{Beginner}{]}
\item
  Same as above but this time make sure to use the \texttt{quote=FALSE},\texttt{sep="\textbackslash{}t"} and \texttt{row.names=FALSE} arguments. Save the file to ``my.cpgi.file2.txt'' and compare it with ``my.cpgi.file.txt''. {[}Difficulty: \textbf{Beginner}{]}
\item
  Write out the first 10 rows of the \texttt{cpgi} data frame.
  \textbf{HINT:} Use subsetting for data frames we learned before. {[}Difficulty: \textbf{Beginner}{]}
\item
  Write the first 3 columns of the \texttt{cpgi} data frame. {[}Difficulty: \textbf{Beginner}{]}
\item
  Write CpG islands only on chr1. \textbf{HINT:} Use subsetting with \texttt{{[}{]}}, feed a logical vector using \texttt{==} operator.{[}Difficulty: \textbf{Beginner/Intermediate}{]}
\item
  Read two other data sets ``rn4.refseq.bed'' and ``rn4.refseq2name.txt'' with \texttt{header=FALSE}, and assign them to df1 and df2 respectively.
  They are again included in the compGenomRData package, and you
  can use the \texttt{system.file()} function to get the file paths. {[}Difficulty: \textbf{Beginner}{]}
\item
  Use \texttt{head()} to see what is inside the data frames above. {[}Difficulty: \textbf{Beginner}{]}
\item
  Merge data sets using \texttt{merge()} and assign the results to a variable named `new.df', and use \texttt{head()} to see the results. {[}Difficulty: \textbf{Intermediate}{]}
\end{enumerate}

\hypertarget{plotting-in-r}{%
\subsection{Plotting in R}\label{plotting-in-r}}

Please run the following code snippet for the rest of the exercises.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1001}\NormalTok{)}
\NormalTok{x1=}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\OperatorTok{+}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{0}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{15}\NormalTok{)}
\NormalTok{y1=}\DecValTok{1}\OperatorTok{:}\DecValTok{100}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Make a scatter plot using the \texttt{x1} and \texttt{y1} vectors generated above. {[}Difficulty: \textbf{Beginner}{]}
\item
  Use the \texttt{main} argument to give a title to \texttt{plot()} as in \texttt{plot(x,y,main="title")}. {[}Difficulty: \textbf{Beginner}{]}
\item
  Use the \texttt{xlab} argument to set a label for the x-axis. Use \texttt{ylab} argument to set a label for the y-axis. {[}Difficulty: \textbf{Beginner}{]}
\item
  Once you have the plot, run the following expression in R console. \texttt{mtext(side=3,text="hi\ there")} does. \textbf{HINT:} \texttt{mtext} stands for margin text. {[}Difficulty: \textbf{Beginner}{]}
\item
  See what \texttt{mtext(side=2,text="hi\ there")} does. Check your plot after execution. {[}Difficulty: \textbf{Beginner}{]}
\item
  Use \emph{mtext()} and \emph{paste()} to put a margin text on the plot. You can use \texttt{paste()} as `text' argument in \texttt{mtext()}. \textbf{HINT:} \texttt{mtext(side=3,text=paste(...))}. See how \texttt{paste()} is used for below. {[}Difficulty: \textbf{Beginner/Intermediate}{]}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{paste}\NormalTok{(}\StringTok{"Text"}\NormalTok{,}\StringTok{"here"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Text here"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myText=}\KeywordTok{paste}\NormalTok{(}\StringTok{"Text"}\NormalTok{,}\StringTok{"here"}\NormalTok{)}
\NormalTok{myText}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Text here"
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\item
  \texttt{cor()} calculates the correlation between two vectors.
  Pearson correlation is a measure of the linear correlation (dependence)
  between two variables X and Y. Try using the \texttt{cor()} function on the \texttt{x1} and \texttt{y1} variables. {[}Difficulty: \textbf{Intermediate}{]}
\item
  Try to use \texttt{mtext()},\texttt{cor()} and \texttt{paste()} to display the correlation coefficient on your scatter plot. {[}Difficulty: \textbf{Intermediate}{]}
\item
  Change the colors of your plot using the \texttt{col} argument.
  Ex: \texttt{plot(x,y,col="red")}. {[}Difficulty: \textbf{Beginner}{]}
\item
  Use \texttt{pch=19} as an argument in your \texttt{plot()} command. {[}Difficulty: \textbf{Beginner}{]}
\item
  Use \texttt{pch=18} as an argument to your \texttt{plot()} command. {[}Difficulty: \textbf{Beginner}{]}
\item
  Make a histogram of \texttt{x1} with the \texttt{hist()} function. A histogram is a graphical representation of the data distribution. {[}Difficulty: \textbf{Beginner}{]}
\item
  You can change colors with `col', add labels with `xlab', `ylab', and add a `title' with `main' arguments. Try all these in a histogram.
  {[}Difficulty: \textbf{Beginner}{]}
\item
  Make a boxplot of y1 with \texttt{boxplot()}.{[}Difficulty: \textbf{Beginner}{]}
\item
  Make boxplots of \texttt{x1} and \texttt{y1} vectors in the same plot.{[}Difficulty: \textbf{Beginner}{]}
\item
  In boxplot, use the \texttt{horizontal\ =\ TRUE} argument. {[}Difficulty: \textbf{Beginner}{]}
\item
  Make multiple plots with \texttt{par(mfrow=c(2,1))}

  \begin{itemize}
  \tightlist
  \item
    run \texttt{par(mfrow=c(2,1))}
  \item
    make a boxplot
  \item
    make a histogram
    {[}Difficulty: \textbf{Beginner/Intermediate}{]}
  \end{itemize}
\item
  Do the same as above but this time with \texttt{par(mfrow=c(1,2))}. {[}Difficulty: \textbf{Beginner/Intermediate}{]}
\item
  Save your plot using the ``Export'' button in Rstudio. {[}Difficulty: \textbf{Beginner}{]}
\item
  You can make a scatter plot showing the density
  of points rather than points themselves. If you use points it looks like this:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x2=}\DecValTok{1}\OperatorTok{:}\DecValTok{1000}\OperatorTok{+}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{0}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{200}\NormalTok{)}
\NormalTok{y2=}\DecValTok{1}\OperatorTok{:}\DecValTok{1000}
\KeywordTok{plot}\NormalTok{(x2,y2,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.5\linewidth]{02-intro2R_files/figure-latex/colorScatterEx-1} \end{center}

If you use the \texttt{smoothScatter()} function, you get the densities.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{smoothScatter}\NormalTok{(x2,y2,}
              \DataTypeTok{colramp=}\KeywordTok{colorRampPalette}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"white"}\NormalTok{,}\StringTok{"blue"}\NormalTok{,}
                                         \StringTok{"green"}\NormalTok{,}\StringTok{"yellow"}\NormalTok{,}\StringTok{"red"}\NormalTok{))) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.5\linewidth]{02-intro2R_files/figure-latex/smoothScatterEx-1} \end{center}

Now, plot with the \texttt{colramp=heat.colors} argument and then use a custom color scale using the following argument.

\begin{verbatim}
colramp = colorRampPalette(c("white","blue", "green","yellow","red")))
\end{verbatim}

{[}Difficulty: \textbf{Beginner/Intermediate}{]}

\hypertarget{functions-and-control-structures-for-ifelse-etc.-1}{%
\subsection{Functions and control structures (for, if/else, etc.)}\label{functions-and-control-structures-for-ifelse-etc.-1}}

Read CpG island data as shown below for the rest of the exercises.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cpgtFilePath=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                \StringTok{"CpGi.table.hg18.txt"}\NormalTok{,}
                \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{cpgi=}\KeywordTok{read.table}\NormalTok{(cpgtFilePath,}\DataTypeTok{header=}\OtherTok{TRUE}\NormalTok{,}\DataTypeTok{sep=}\StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(cpgi)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   chrom chromStart chromEnd     name length cpgNum gcNum perCpg perGc obsExp
## 1  chr1      18598    19673 CpG: 116   1075    116   787   21.6  73.2   0.83
## 2  chr1     124987   125426  CpG: 30    439     30   295   13.7  67.2   0.64
## 3  chr1     317653   318092  CpG: 29    439     29   295   13.2  67.2   0.62
## 4  chr1     427014   428027  CpG: 84   1013     84   734   16.6  72.5   0.64
## 5  chr1     439136   440407  CpG: 99   1271     99   777   15.6  61.1   0.84
## 6  chr1     523082   523977  CpG: 94    895     94   570   21.0  63.7   1.04
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Check values in the perGc column using a histogram.
  The `perGc' column in the data stands for GC percent =\textgreater{} percentage of C+G nucleotides. {[}Difficulty: \textbf{Beginner}{]}
\item
  Make a boxplot for the `perGc' column. {[}Difficulty: \textbf{Beginner}{]}
\item
  Use if/else structure to decide if the given GC percent is high, low or medium.
  If it is low, high, or medium: low \textless{} 60, high\textgreater75, medium is between 60 and 75;
  use greater or less than operators, \texttt{\textless{}} or \texttt{\textgreater{}}. Fill in the values in the code below, where it is written `YOU\_FILL\_IN'. {[}Difficulty: \textbf{Intermediate}{]}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GCper=}\DecValTok{65}

  \CommentTok{# check if GC value is lower than 60, }
  \CommentTok{# assign "low" to result}
  \ControlFlowTok{if}\NormalTok{(}\StringTok{'YOU_FILL_IN'}\NormalTok{)\{}
\NormalTok{    result=}\StringTok{"low"}
    \KeywordTok{cat}\NormalTok{(}\StringTok{"low"}\NormalTok{)}
\NormalTok{  \}}
  \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{(}\StringTok{'YOU_FILL_IN'}\NormalTok{)\{  }\CommentTok{# check if GC value is higher than 75,      }
                           \CommentTok{#assign "high" to result}
\NormalTok{    result=}\StringTok{"high"}
    \KeywordTok{cat}\NormalTok{(}\StringTok{"high"}\NormalTok{)}
\NormalTok{  \}}\ControlFlowTok{else}\NormalTok{\{ }\CommentTok{# if those two conditions fail then it must be "medium"}
\NormalTok{    result=}\StringTok{"medium"}
\NormalTok{  \}}

\NormalTok{result}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Write a function that takes a value of GC percent and decides
  if it is low, high, or medium: low \textless{} 60, high\textgreater75, medium is between 60 and 75.
  Fill in the values in the code below, where it is written `YOU\_FILL\_IN'. {[}Difficulty: \textbf{Intermediate/Advanced}{]}
\end{enumerate}

\begin{verbatim}
GCclass<-function(my.gc){
  
  YOU_FILL_IN
  
  return(result)
}
GCclass(10) # should return "low"
GCclass(90) # should return "high"
GCclass(65) # should return "medium"
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Use a for loop to get GC percentage classes for \texttt{gcValues} below. Use the function
  you wrote above.{[}Difficulty: \textbf{Intermediate/Advanced}{]}
\end{enumerate}

\begin{verbatim}
gcValues=c(10,50,70,65,90)
for( i in YOU_FILL_IN){
  YOU_FILL_IN
}
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Use \texttt{lapply} to get GC percentage classes for \texttt{gcValues}. {[}Difficulty: \textbf{Intermediate/Advanced}{]}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vec=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{)}
\NormalTok{power2=}\ControlFlowTok{function}\NormalTok{(x)\{ }\KeywordTok{return}\NormalTok{(x}\OperatorTok{^}\DecValTok{2}\NormalTok{)  \}}
    \KeywordTok{lapply}\NormalTok{(vec,power2)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\item
  Use sapply to get values to get GC percentage classes for \texttt{gcValues}. {[}Difficulty: \textbf{Intermediate}{]}
\item
  Is there a way to decide on the GC percentage class of a given vector of \texttt{GCpercentages}
  without using if/else structure and loops ? if so, how can you do it?
  \textbf{HINT:} Subsetting using \textless{} and \textgreater{} operators.
  {[}Difficulty: \textbf{Intermediate}{]}
\end{enumerate}

\hypertarget{stats}{%
\chapter{Statistics for Genomics}\label{stats}}

This chapter will summarize statistics methods frequently used
in computational genomics. As these fields are continuously evolving, the
techniques introduced here do not form an exhaustive list but mostly cornerstone methods
that are often and still being used. In addition, we focused on giving intuitive and
practical understanding of the methods with relevant examples from the field. If you want to dig deeper into statistics and math, beyond what is described
here, we included appropriate references with annotation after each major
section.

\hypertarget{how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions}{%
\section{How to summarize collection of data points: The idea behind statistical distributions}\label{how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions}}

In biology and many other fields, data is collected via experimentation.
The nature of the experiments and natural variation in biology makes
it impossible to get the same exact measurements every time you measure something.
For example, if you are measuring gene expression values for
a certain gene, say PAX6, and let's assume you are measuring expression
per sample and cell with any method (microarrays, rt-qPCR, etc.). You will not \index{gene expression}
get the same expression value even if your samples are homogeneous, due
to technical bias in experiments or natural variation in the samples. Instead,
we would like to describe this collection of data some other way
that represents the general properties of the data. Figure \ref{fig:pax6ReplicatesChp3} shows a sample of
20 expression values from the PAX6 gene.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{03-statsForGenomics_files/figure-latex/pax6ReplicatesChp3-1} 

}

\caption{Expression of the PAX6 gene in 20 replicate experiments.}\label{fig:pax6ReplicatesChp3}
\end{figure}

\hypertarget{describing-the-central-tendency-mean-and-median}{%
\subsection{Describing the central tendency: Mean and median}\label{describing-the-central-tendency-mean-and-median}}

As seen in Figure \ref{fig:pax6ReplicatesChp3}, the points from this sample are distributed around
a central value and the histogram below the dot plot shows the number of points in
each bin. Another observation is that there are some bins that have more points than others. If we want to summarize what we observe, we can try
to represent the collection of data points
with an expression value that is typical to get, something that represents the
general tendency we observe on the dot plot and the histogram. This value is
sometimes called the central
value or central tendency, and there are different ways to calculate such a value.
In Figure \ref{fig:pax6ReplicatesChp3}, we see that all the values are spread around 6.13 (red line),
and that is indeed what we call the mean value of this sample of expression values.
It can be calculated with the following formula \(\overline{X}=\sum_{i=1}^n x_i/n\),
where \(x_i\) is the expression value of an experiment and \(n\) is the number of
expression values obtained from the experiments. In R, the \texttt{mean()} function will calculate the \index{mean}
mean of a provided vector of numbers. This is called a ``sample mean''. In reality, there are many more than 20 possible PAX6 expression values (provided each cell is of the
identical cell type and is in identical conditions). If we had the time and the funding to sample all cells and measure PAX6 expression we would
get a collection of values that would be called, in statistics, a ``population''. In
our case, the population will look like the left hand side of the Figure \ref{fig:pax6MorereplicatesChp3}. What we have done with
our 20 data points is that we took a sample of PAX6 expression values from this
population, and calculated the sample mean.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{03-statsForGenomics_files/figure-latex/pax6MorereplicatesChp3-1} 

}

\caption{Expression of all possible PAX6 gene expression measures on all available biological samples (left). Expression of the PAX6 gene from the statistical sample, a random subset from the population of biological samples (right). }\label{fig:pax6MorereplicatesChp3}
\end{figure}

The mean of the population is calculated the same way but traditionally the
Greek letter \(\mu\) is used to denote the population mean. Normally, we would not
have access to the population and we will use the sample mean and other quantities
derived from the sample to estimate the population properties. This is the basic
idea behind statistical inference, which we will see in action in later
sections as well. We
estimate the population parameters from the sample parameters and there is some
uncertainty associated with those estimates. We will be trying to assess those
uncertainties and make decisions in the presence of those uncertainties. \index{mean}

We are not yet done with measuring central tendency.
There are other ways to describe it, such as the median value. The
mean can be affected by outliers easily\index{outliers}.
If certain values are very high or low compared to the
bulk of the sample, this will shift mean toward those outliers. However, the median is not affected by outliers. It is simply the value in a distribution where half
of the values are above and the other half are below. In R, the \texttt{median()} function
will calculate the mean of a provided vector of numbers. \index{median}Let's create a set of random numbers and calculate their mean and median using
R.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#create 10 random numbers from uniform distribution }
\NormalTok{x=}\KeywordTok{runif}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\CommentTok{# calculate mean}
\KeywordTok{mean}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3738963
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate median}
\KeywordTok{median}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3277896
\end{verbatim}

\hypertarget{describing-the-spread-measurements-of-variation}{%
\subsection{Describing the spread: Measurements of variation}\label{describing-the-spread-measurements-of-variation}}

Another useful way to summarize a collection of data points is to measure
how variable the values are. You can simply describe the range of the values,
such as the minimum and maximum values. You can easily do that in R with the \texttt{range()}
function. A more common way to calculate variation is by calculating something
called ``standard deviation'' or the related quantity called ``variance''. This is a
quantity that shows how variable the values are. A value around zero indicates
there is not much variation in the values of the data points, and a high value
indicates high variation in the values. The variance is the squared distance of
data points from the mean. Population variance\index{variance} is again a quantity we usually
do not have access to and is simply calculated as follows \(\sigma^2=\sum_{i=1}^n \frac{(x_i-\mu)^2}{n}\), where \(\mu\) is the population mean, \(x_i\) is the \(i\)th
data point in the population and \(n\) is the population size. However, when we only have access to a sample, this formulation is biased. That means that it
underestimates the population variance, so we make a small adjustment when we
calculate the sample variance, denoted as \(s^2\):

\[
\begin{aligned}
s^2=\sum_{i=1}^n \frac{(x_i-\overline{X})^2}{n-1} && \text{ where $x_i$ is the ith data point and
$\overline{X}$ is the sample mean.}
\end{aligned}
\]

The sample standard deviation is simply the square root of the sample variance, \(s=\sqrt{\sum_{i=1}^n \frac{(x_i-\overline{X})^2}{n-1}}\).
The good thing about standard deviation is that it has the same unit as the mean
so it is more intuitive.

We can calculate the sample standard deviation and variation with the \texttt{sd()} and \texttt{var()}
functions in R. These functions take a vector of numeric values as input and
calculate the desired quantities. Below we use those functions on a randomly
generated vector of numbers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{20}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{6}\NormalTok{,}\DataTypeTok{sd=}\FloatTok{0.7}\NormalTok{)}
\KeywordTok{var}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2531495
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sd}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5031397
\end{verbatim}

One potential problem with the variance is that it could be affected by
outliers.\index{outliers} The points that are too far away from the mean will have a large
effect on the variance even though there might be few of them.
A way to measure variance that could be less affected by outliers is
looking at where the bulk of the distribution is. How do we define where the bulk is?
One common way is to look at the difference between 75th percentile and 25th
percentile, this effectively removes a lot of potential outliers which will be\index{outliers}
towards the edges of the range of values.
This is called the interquartile range\index{interquartile range}, and
can be easily calculated using R via the \texttt{IQR()} function and the quantiles of a vector
are calculated with the \texttt{quantile()} function.

Let us plot the boxplot for a random vector and also calculate IQR using R.
In the boxplot (Figure \ref{fig:boxplot2Chp3}), 25th and 75th percentiles are the edges of the box, and
the median is marked with a thick line cutting through the box.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{20}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{6}\NormalTok{,}\DataTypeTok{sd=}\FloatTok{0.7}\NormalTok{)}
\KeywordTok{IQR}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5010954
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{quantile}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       0%      25%      50%      75%     100% 
## 5.437119 5.742895 5.860302 6.243991 6.558112
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boxplot}\NormalTok{(x,}\DataTypeTok{horizontal =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{03-statsForGenomics_files/figure-latex/boxplot2Chp3-1} 

}

\caption{Boxplot showing the 25th percentile and 75th percentile and median for a set of points sampled from a normal distribution with mean=6 and standard deviation=0.7.}\label{fig:boxplot2Chp3}
\end{figure}

\hypertarget{frequently-used-statistical-distributions}{%
\subsubsection{Frequently used statistical distributions}\label{frequently-used-statistical-distributions}}

The distributions have parameters (such as mean and variance) that
summarize them, but also they are functions that assign each outcome of a \index{normal distribution}
statistical experiment to its probability of occurrence.
One distribution that you
will frequently encounter is the normal distribution or Gaussian distribution.
The normal distribution has a typical ``bell-curve'' shape
and is characterized by mean and standard deviation. A set of data points
that
follow normal distribution will mostly be close to the mean
but spread around it, controlled by the standard deviation parameter. That
means that if we sample data points from a normal distribution, we are more
likely to sample data points near the mean and sometimes away from the mean.
The probability of an event occurring is higher if it is nearby the mean.
The effect
of the parameters for the normal distribution can be observed in the following
plot.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{03-statsForGenomics_files/figure-latex/normDistChp3-1} 

}

\caption{Different parameters for normal distribution and effect of those on the shape of the distribution}\label{fig:normDistChp3}
\end{figure}

The normal distribution is often denoted by \(\mathcal{N}(\mu,\,\sigma^2)\). When a random variable \(X\) is distributed normally with mean \(\mu\) and variance \(\sigma^2\), we write:

\[X\ \sim\ \mathcal{N}(\mu,\,\sigma^2)\]

The probability
density function of the normal distribution with mean \(\mu\) and standard deviation
\(\sigma\) is as follows:

\[P(x)=\frac{1}{\sigma\sqrt{2\pi} } \; e^{ -\frac{(x-\mu)^2}{2\sigma^2} } \]

The probability density function gives the probability of observing a value
on a normal distribution defined by the \(\mu\) and
\(\sigma\) parameters.

Oftentimes, we do not need the exact probability of a value, but we need the
probability of observing a value larger or smaller than a critical value or reference
point. For example, we might want to know the probability of \(X\) being smaller than or
equal to -2 for a normal distribution with mean \(0\) and standard deviation \(2\): \(P(X <= -2 \; | \; \mu=0,\sigma=2)\). In this case, what we want is the area under the
curve shaded in dark blue. To be able to do that, we need to integrate the probability
density function but we will usually let software do that. Traditionally,
one calculates a Z-score which is simply \((X-\mu)/\sigma=(-2-0)/2= -1\), and
corresponds to how many standard deviations you are away from the mean.
This is also called ``standardization'', the corresponding value is distributed in ``standard normal distribution'' where \(\mathcal{N}(0,\,1)\). After calculating the Z-score,
we can look up the area under the curve for the left and right sides of the Z-score in a table, but again, we use software for that.
The tables are outdated when you can use a computer.

Below in Figure \ref{fig:zscore}, we show the Z-score and the associated probabilities derived
from the calculation above for \(P(X <= -2 \; | \; \mu=0,\sigma=2)\).

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{03-statsForGenomics_files/figure-latex/zscore-1} 

}

\caption{Z-score and associated probabilities for Z= -1}\label{fig:zscore}
\end{figure}

In R, the family of \texttt{*norm} functions (\texttt{rnorm},\texttt{dnorm},\texttt{qnorm} and \texttt{pnorm}) can
be used to
operate with the normal distribution, such as calculating probabilities and
generating random numbers drawn from a normal distribution. We show some of those capabilities below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get the value of probability density function when X= -2,}
\CommentTok{# where mean=0 and sd=2}
\KeywordTok{dnorm}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{, }\DataTypeTok{mean=}\DecValTok{0}\NormalTok{, }\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1209854
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get the probability of P(X =< -2) where mean=0 and sd=2}
\KeywordTok{pnorm}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{, }\DataTypeTok{mean=}\DecValTok{0}\NormalTok{, }\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1586553
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get the probability of P(X > -2) where mean=0 and sd=2}
\KeywordTok{pnorm}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{, }\DataTypeTok{mean=}\DecValTok{0}\NormalTok{, }\DataTypeTok{sd=}\DecValTok{2}\NormalTok{,}\DataTypeTok{lower.tail =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8413447
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get 5 random numbers from normal dist with  mean=0 and sd=2}
\KeywordTok{rnorm}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DataTypeTok{mean=}\DecValTok{0}\NormalTok{ , }\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -1.8109030 -1.9220710 -0.5146717  0.8216728 -0.7900804
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get y value corresponding to P(X > y) = 0.15 with  mean=0 and sd=2}
\KeywordTok{qnorm}\NormalTok{( }\FloatTok{0.15}\NormalTok{, }\DataTypeTok{mean=}\DecValTok{0}\NormalTok{ , }\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -2.072867
\end{verbatim}

There are many other distribution functions in R that can be used the same
way. You have to enter the distribution-specific parameters along
with your critical value, quantiles, or number of random numbers depending
on which function you are using in the family. We will list some of those functions below.

\begin{itemize}
\item
  \texttt{dbinom} is for the binomial distribution\index{binomial disdistribution}. This distribution is usually used
  to model fractional data and binary data. Examples from genomics include
  methylation data.
\item
  \texttt{dpois} is used for the Poisson distribution and \texttt{dnbinom} is used for
  the negative binomial distribution. These distributions are used to model count \index{Poisson distribution}
  data such as sequencing read counts.
\item
  \texttt{df} (F distribution) and \texttt{dchisq} (Chi-Squared distribution) are used \index{F distribution}
  in relation to the distribution of variation. The F distribution is used to model \index{Chi-Squared distribution}
  ratios of variation and Chi-Squared distribution is used to model
  distribution of variations. You will frequently encounter these in linear models and generalized linear models.
\end{itemize}

\hypertarget{precision-of-estimates-confidence-intervals}{%
\subsection{Precision of estimates: Confidence intervals}\label{precision-of-estimates-confidence-intervals}}

When we take a random sample from a population and compute a statistic, such as
the mean, we are trying to approximate the mean of the population. How well this \index{confidence intervals}
sample statistic estimates the population value will always be a
concern. A confidence interval addresses this concern because it provides a
range of values which will plausibly contain the population parameter of interest.
Normally, we would not have access to a population. If we did, we would not have to estimate the population parameters and their precision.

When we do not have access
to the population, one way to estimate intervals is to repeatedly take samples from the
original sample with replacement, that is, we take a data point from the sample
we replace, and we take another data point until we have sample size of the
original sample. Then, we calculate the parameter of interest, in this case the mean, and
repeat this process a large number of times, such as 1000. At this point, we would have a distribution of re-sampled
means. We can then calculate the 2.5th and 97.5th percentiles and these will
be our so-called 95\% confidence interval. This procedure, resampling with replacement to
estimate the precision of population parameter estimates, is known as the \textbf{bootstrap resampling} or \textbf{bootstraping}.\index{bootstrap resampling}

Let's see how we can do this in practice. We simulate a sample
coming from a normal distribution (but we pretend we don't know the
population parameters). We will estimate the precision
of the mean of the sample using bootstrapping to build confidence intervals, the resulting plot after this procedure is shown in Figure \ref{fig:bootstrapChp3}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(mosaic)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{21}\NormalTok{)}
\NormalTok{sample1=}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{20}\NormalTok{,}\DecValTok{5}\NormalTok{) }\CommentTok{# simulate a sample}

\CommentTok{# do bootstrap resampling, sampling with replacement}
\NormalTok{boot.means=}\KeywordTok{do}\NormalTok{(}\DecValTok{1000}\NormalTok{) }\OperatorTok{*}\StringTok{ }\KeywordTok{mean}\NormalTok{(}\KeywordTok{resample}\NormalTok{(sample1))}

\CommentTok{# get percentiles from the bootstrap means}
\NormalTok{q=}\KeywordTok{quantile}\NormalTok{(boot.means[,}\DecValTok{1}\NormalTok{],}\DataTypeTok{p=}\KeywordTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{,}\FloatTok{0.975}\NormalTok{))}

\CommentTok{# plot the histogram}
\KeywordTok{hist}\NormalTok{(boot.means[,}\DecValTok{1}\NormalTok{],}\DataTypeTok{col=}\StringTok{"cornflowerblue"}\NormalTok{,}\DataTypeTok{border=}\StringTok{"white"}\NormalTok{,}
                    \DataTypeTok{xlab=}\StringTok{"sample means"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\KeywordTok{c}\NormalTok{(q[}\DecValTok{1}\NormalTok{], q[}\DecValTok{2}\NormalTok{] ),}\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\DataTypeTok{x=}\NormalTok{q[}\DecValTok{1}\NormalTok{],}\DataTypeTok{y=}\DecValTok{200}\NormalTok{,}\KeywordTok{round}\NormalTok{(q[}\DecValTok{1}\NormalTok{],}\DecValTok{3}\NormalTok{),}\DataTypeTok{adj=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\KeywordTok{text}\NormalTok{(}\DataTypeTok{x=}\NormalTok{q[}\DecValTok{2}\NormalTok{],}\DataTypeTok{y=}\DecValTok{200}\NormalTok{,}\KeywordTok{round}\NormalTok{(q[}\DecValTok{2}\NormalTok{],}\DecValTok{3}\NormalTok{),}\DataTypeTok{adj=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{03-statsForGenomics_files/figure-latex/bootstrapChp3-1} 

}

\caption{Precision estimate of the sample mean using 1000 bootstrap samples. Confidence intervals derived from the bootstrap samples are shown with red lines.}\label{fig:bootstrapChp3}
\end{figure}

If we had a convenient mathematical method to calculate the confidence interval,
we could also do without resampling methods. It turns out that if we take
repeated
samples from a population with sample size \(n\), the distribution of means
(\(\overline{X}\)) of those samples
will be approximately normal with mean \(\mu\) and standard deviation
\(\sigma/\sqrt{n}\). This is also known as the \textbf{Central Limit Theorem(CLT)} and
is one of the most important theorems in statistics. This also means that
\(\frac{\overline{X}-\mu}{\sigma\sqrt{n}}\) has a standard normal
distribution and we can calculate the Z-score, and then we can get
the percentiles associated with the Z-score. Below, we are showing the
Z-score
calculation for the distribution of \(\overline{X}\), and then
we are deriving the confidence intervals starting with the fact that
the probability of Z being between \(-1.96\) and \(1.96\) is \(0.95\). We then use algebra
to show that the probability that unknown \(\mu\) is captured between
\(\overline{X}-1.96\sigma/\sqrt{n}\) and \(\overline{X}+1.96\sigma/\sqrt{n}\) is \(0.95\), which is commonly known as the 95\% confidence interval.

\[\begin{array}{ccc}
Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\\
P(-1.96 < Z < 1.96)=0.95 \\
P(-1.96 < \frac{\overline{X}-\mu}{\sigma/\sqrt{n}} < 1.96)=0.95\\
P(\mu-1.96\sigma/\sqrt{n} < \overline{X} < \mu+1.96\sigma/\sqrt{n})=0.95\\
P(\overline{X}-1.96\sigma/\sqrt{n} < \mu < \overline{X}+1.96\sigma/\sqrt{n})=0.95\\
confint=[\overline{X}-1.96\sigma/\sqrt{n},\overline{X}+1.96\sigma/\sqrt{n}]
\end{array}\]

A 95\% confidence interval for the population mean is the most common interval to use, and would \index{confidence interval}
mean that we would expect 95\% of the interval estimates to include the
population parameter, in this case, the mean. However, we can pick any value
such as 99\% or 90\%. We can generalize the confidence interval for
\(100(1-\alpha)\) as follows:

\[\overline{X} \pm Z_{\alpha/2}\sigma/\sqrt{n}\]

In R, we can do this using the \texttt{qnorm()} function to get Z-scores associated
with \({\alpha/2}\) and \({1-\alpha/2}\). As you can see, the confidence intervals we calculated using CLT are very
similar to the ones we got from the bootstrap for the same sample. For bootstrap we got \([19.21, 21.989]\) and for the CLT-based estimate we got \([19.23638, 22.00819]\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alpha=}\FloatTok{0.05}
\NormalTok{sd=}\DecValTok{5}
\NormalTok{n=}\DecValTok{50}
\KeywordTok{mean}\NormalTok{(sample1)}\OperatorTok{+}\KeywordTok{qnorm}\NormalTok{(}\KeywordTok{c}\NormalTok{(alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{,}\DecValTok{1}\OperatorTok{-}\NormalTok{alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{))}\OperatorTok{*}\NormalTok{sd}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(n)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 19.23638 22.00819
\end{verbatim}

The good thing about CLT is, as long as the sample size is large, regardless of \index{central limit theorem (CLT)}
the population distribution, the distribution of sample means drawn from
that population will always be normal. In Figure \ref{fig:sampleMeanschp3}, we repeatedly
draw samples 1000 times with sample size \(n=10\),\(30\), and \(100\) from a bimodal,
exponential and a uniform distribution and we are getting sample mean distributions
following normal distribution.

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{03-statsForGenomics_files/figure-latex/sampleMeanschp3-1} 

}

\caption{Sample means are normally distributed regardless of the population distribution they are drawn from.}\label{fig:sampleMeanschp3}
\end{figure}

However, we should note that how we constructed the confidence interval
using standard normal distribution, \(N(0,1)\), only works when we know the \index{normal distribution}
population standard deviation. In reality, we usually have only access
to a sample and have no idea about the population standard deviation. If
this is the case, we should estimate the standard deviation using
the sample standard deviation and use something called the \emph{t distribution} instead \index{t distribution}
of the standard normal distribution in our interval calculation. Our confidence interval becomes
\(\overline{X} \pm t_{\alpha/2}s/\sqrt{n}\), with t distribution
parameter \(d.f=n-1\), since now the following quantity is t distributed \(\frac{\overline{X}-\mu}{s/\sqrt{n}}\) instead of standard normal distribution.

The t distribution is similar to the standard normal distribution and has mean \(0\) but its spread is larger than the normal distribution
especially when the sample size is small, and has one parameter \(v\) for
the degrees of freedom, which is \(n-1\) in this case. Degrees of freedom
is simply the number of data points minus the number of parameters estimated.\index{degrees of freedom}Here we are estimating the mean from the data, therefore the degrees of freedom is \(n-1\). The resulting distributions are shown in Figure \ref{fig:tdistChp3}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{03-statsForGenomics_files/figure-latex/tdistChp3-1} 

}

\caption{Normal distribution and t distribution with different degrees of freedom. With increasing degrees of freedom, the t distribution approximates the normal distribution better.}\label{fig:tdistChp3}
\end{figure}

\hypertarget{how-to-test-for-differences-between-samples}{%
\section{How to test for differences between samples}\label{how-to-test-for-differences-between-samples}}

Oftentimes we would want to compare sets of samples. Such comparisons include
if wild-type samples have different expression compared to mutants or if healthy
samples are different from disease samples in some measurable feature (blood count,
gene expression, methylation of certain loci). Since there is variability in our
measurements, we need to take that into account when comparing the sets of samples.
We can simply subtract the means of two samples, but given the variability
of sampling, at the very least we need to decide a cutoff value for differences
of means; small differences of means can be explained by random chance due to
sampling. That means we need to compare the difference we get to a value that
is typical to get if the difference between two group means were only due to
sampling. If you followed the logic above, here we actually introduced two core
ideas of something called ``hypothesis testing'', which is simply using
statistics to \index{hypothesis testing}
determine the probability that a given hypothesis (Ex: if two sample sets
are from the same population or not) is true. Formally, expanded version of those two core ideas are as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Decide on a hypothesis to test, often called the ``null hypothesis'' (\(H_0\)). In our
  case, the hypothesis is that there is no difference between sets of samples. An ``alternative hypothesis'' (\(H_1\)) is that there is a difference between the
  samples.
\item
  Decide on a statistic to test the truth of the null hypothesis.
\item
  Calculate the statistic.
\item
  Compare it to a reference value to establish significance, the P-value. Based on that, either reject or not reject the null hypothesis, \(H_0\).
\end{enumerate}

\hypertarget{randomization-based-testing-for-difference-of-the-means}{%
\subsection{Randomization-based testing for difference of the means}\label{randomization-based-testing-for-difference-of-the-means}}

There is one intuitive way to go about this. If we believe there are no
differences between samples, that means the sample labels (test vs.~control or
healthy vs.~disease) have no meaning. So, if we randomly assign labels to the
samples and calculate the difference of the means, this creates a null
distribution for \(H_0\) where we can compare the real difference and
measure how unlikely it is to get such a value under the expectation of the
null hypothesis. We can calculate all possible permutations to calculate
the null distribution. However, sometimes that is not very feasible and the
equivalent approach would be generating the null distribution by taking a
smaller number of random samples with shuffled group membership.

Below, we are doing this process in R. We are first simulating two samples
from two different distributions.
These would be equivalent to gene expression measurements obtained under
different conditions. Then, we calculate the differences in the means
and do the randomization procedure to get a null distribution when we
assume there is no difference between samples, \(H_0\). We then calculate how
often we would get the original difference we calculated under the
assumption that \(H_0\) is true. The resulting null distribution and the original value is shown in Figure \ref{fig:randomTestchp3}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{gene1=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{4}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\NormalTok{gene2=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{2}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\NormalTok{org.diff=}\KeywordTok{mean}\NormalTok{(gene1)}\OperatorTok{-}\KeywordTok{mean}\NormalTok{(gene2)}
\NormalTok{gene.df=}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{exp=}\KeywordTok{c}\NormalTok{(gene1,gene2),}
                  \DataTypeTok{group=}\KeywordTok{c}\NormalTok{( }\KeywordTok{rep}\NormalTok{(}\StringTok{"test"}\NormalTok{,}\DecValTok{30}\NormalTok{),}\KeywordTok{rep}\NormalTok{(}\StringTok{"control"}\NormalTok{,}\DecValTok{30}\NormalTok{) ) )}


\NormalTok{exp.null <-}\StringTok{ }\KeywordTok{do}\NormalTok{(}\DecValTok{1000}\NormalTok{) }\OperatorTok{*}\StringTok{ }\KeywordTok{diff}\NormalTok{(mosaic}\OperatorTok{::}\KeywordTok{mean}\NormalTok{(exp }\OperatorTok{~}\StringTok{ }\KeywordTok{shuffle}\NormalTok{(group), }\DataTypeTok{data=}\NormalTok{gene.df))}
\KeywordTok{hist}\NormalTok{(exp.null[,}\DecValTok{1}\NormalTok{],}\DataTypeTok{xlab=}\StringTok{"null distribution | no difference in samples"}\NormalTok{,}
     \DataTypeTok{main=}\KeywordTok{expression}\NormalTok{(}\KeywordTok{paste}\NormalTok{(H[}\DecValTok{0}\NormalTok{],}\StringTok{" :no difference in means"}\NormalTok{) ),}
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{),}\DataTypeTok{col=}\StringTok{"cornflowerblue"}\NormalTok{,}\DataTypeTok{border=}\StringTok{"white"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\KeywordTok{quantile}\NormalTok{(exp.null[,}\DecValTok{1}\NormalTok{],}\FloatTok{0.95}\NormalTok{),}\DataTypeTok{col=}\StringTok{"red"}\NormalTok{ )}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{org.diff,}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{ )}
\KeywordTok{text}\NormalTok{(}\DataTypeTok{x=}\KeywordTok{quantile}\NormalTok{(exp.null[,}\DecValTok{1}\NormalTok{],}\FloatTok{0.95}\NormalTok{),}\DataTypeTok{y=}\DecValTok{200}\NormalTok{,}\StringTok{"0.05"}\NormalTok{,}\DataTypeTok{adj=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{),}\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\DataTypeTok{x=}\NormalTok{org.diff,}\DataTypeTok{y=}\DecValTok{200}\NormalTok{,}\StringTok{"org. diff."}\NormalTok{,}\DataTypeTok{adj=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{),}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{03-statsForGenomics_files/figure-latex/randomTestchp3-1} 

}

\caption{The null distribution for differences of means obtained via randomization. The original difference is marked via the blue line. The red line marks the value that corresponds to P-value of 0.05}\label{fig:randomTestchp3}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p.val=}\KeywordTok{sum}\NormalTok{(exp.null[,}\DecValTok{1}\NormalTok{]}\OperatorTok{>}\NormalTok{org.diff)}\OperatorTok{/}\KeywordTok{length}\NormalTok{(exp.null[,}\DecValTok{1}\NormalTok{])}
\NormalTok{p.val}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.001
\end{verbatim}

After doing random permutations and getting a null distribution, it is possible to get a confidence interval for the distribution of difference in means.
This is simply the \(2.5th\) and \(97.5th\) percentiles of the null distribution, and
directly related to the P-value calculation above.

\hypertarget{using-t-test-for-difference-of-the-means-between-two-samples}{%
\subsection{Using t-test for difference of the means between two samples}\label{using-t-test-for-difference-of-the-means-between-two-samples}}

We can also calculate the difference between means using a t-test\index{t-test}. Sometimes we will have too few data points in a sample to do a meaningful
randomization test, also randomization takes more time than doing a t-test.
This is a test that depends on the t distribution\index{t distribution}. The line of thought follows
from the CLT and we can show differences in means are t distributed.
There are a couple of variants of the t-test for this purpose. If we assume
the population variances are equal we can use the following version

\[t = \frac{\bar {X}_1 - \bar{X}_2}{s_{X_1X_2} \cdot \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\]
where
\[s_{X_1X_2} = \sqrt{\frac{(n_1-1)s_{X_1}^2+(n_2-1)s_{X_2}^2}{n_1+n_2-2}}\]
In the first equation above, the quantity is t distributed with \(n_1+n_2-2\) degrees of freedom. We can calculate the quantity and then use software
to look for the percentile of that value in that t distribution, which is our P-value. When we cannot assume equal variances, we use ``Welch's t-test''
which is the default t-test in R and also works well when variances and
the sample sizes are the same. For this test we calculate the following
quantity:

\[t = \frac{\overline{X}_1 - \overline{X}_2}{s_{\overline{X}_1 - \overline{X}_2}}\]
where
\[s_{\overline{X}_1 - \overline{X}_2} = \sqrt{\frac{s_1^2 }{ n_1} + \frac{s_2^2 }{n_2}}\]

and the degrees of freedom equals to

\[\mathrm{d.f.} = \frac{(s_1^2/n_1 + s_2^2/n_2)^2}{(s_1^2/n_1)^2/(n_1-1) + (s_2^2/n_2)^2/(n_2-1)}
\]

Luckily, R does all those calculations for us. Below we will show the use of \texttt{t.test()} function in R. We will use it on the samples we simulated
above.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Welch's t-test}
\NormalTok{stats}\OperatorTok{::}\KeywordTok{t.test}\NormalTok{(gene1,gene2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Welch Two Sample t-test
## 
## data:  gene1 and gene2
## t = 3.7653, df = 47.552, p-value = 0.0004575
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.872397 2.872761
## sample estimates:
## mean of x mean of y 
##  4.057728  2.185149
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# t-test with equal variance assumption}
\NormalTok{stats}\OperatorTok{::}\KeywordTok{t.test}\NormalTok{(gene1,gene2,}\DataTypeTok{var.equal=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Two Sample t-test
## 
## data:  gene1 and gene2
## t = 3.7653, df = 58, p-value = 0.0003905
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.8770753 2.8680832
## sample estimates:
## mean of x mean of y 
##  4.057728  2.185149
\end{verbatim}

A final word on t-tests: they generally assume a population where samples coming
from them have a normal
distribution, however it is been shown t-test can tolerate deviations from
normality, especially, when two distributions are moderately skewed in the
same direction. This is due to the central limit theorem, which says that the means of
samples will be distributed normally no matter the population distribution
if sample sizes are large.

\hypertarget{multiple-testing-correction}{%
\subsection{Multiple testing correction}\label{multiple-testing-correction}}

We should think of hypothesis testing as a non-error-free method of making \index{multiple testing correction}
decisions. There will be times when we declare something significant and accept
\(H_1\) but we will be wrong.
These decisions are also called ``false positives'' or ``false discoveries'', and are also known as ``type I errors''. Similarly, we can fail to reject a hypothesis
when we actually should. These cases are known as ``false negatives'', also known
as ``type II errors''.

The ratio of true negatives to the sum of
true negatives and false positives (\(\frac{TN}{FP+TN}\)) is known as specificity.
And we usually want to decrease the FP and get higher specificity.
The ratio of true positives to the sum of
true positives and false negatives (\(\frac{TP}{TP+FN}\)) is known as sensitivity.
And, again, we usually want to decrease the FN and get higher sensitivity.
Sensitivity is also known as the ``power of a test'' in the context of hypothesis
testing. More powerful tests will be highly sensitive and will have fewer type
II errors. For the t-test, the power is positively associated with sample size
and the effect size. The larger the sample size, the smaller the standard error, and
looking for the larger effect sizes will similarly increase the power.

The general summary of these different decision combinations are
included in the table below.

\begin{longtable}[]{@{}lccl@{}}
\toprule
\begin{minipage}[b]{0.17\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\centering
\(H_0\) is
TRUE,
{[}Gene is NOT
differentially
expressed{]}\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\centering
\(H_1\) is
TRUE,
{[}Gene is
differentially
expressed{]}\strut
\end{minipage} & \begin{minipage}[b]{0.27\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.17\columnwidth}\raggedright
Accept \(H_0\)
(claim that
the gene is not
differentially
expressed)\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
True Negatives (TN)\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
False Negatives (FN)
,type II error\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
\(m_0\): number of truly
null hypotheses\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\raggedright
reject \(H_0\)
(claim that
the gene is
differentially
expressed)\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
False Positives (FP)
,type I error\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
True Positives (TP)\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
\(m-m_0\): number of
truly alternative
hypotheses\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

We expect to make more type I errors as the number of tests increase, which
means we will reject the null hypothesis by mistake. For example, if we
perform a test at the 5\% significance level, there is a 5\% chance of
incorrectly rejecting the null hypothesis if the null hypothesis is true.
However, if we make 1000 tests where all null hypotheses are true for
each of them, the average number of incorrect rejections is 50. And if we
apply the rules of probability, there is almost a 100\% chance that
we will have at least one incorrect rejection.
There are multiple statistical techniques to prevent this from happening.
These techniques generally push the P-values obtained from multiple
tests to higher values; if the individual P-value is low enough it survives
this process. The simplest method is just to multiply the individual
P-value (\(p_i\)) by the number of tests (\(m\)), \(m \cdot p_i\). This is
called ``Bonferroni correction''. However, this is too harsh if you have thousands
of tests. Other methods are developed to remedy this. Those methods
rely on ranking the P-values and dividing \(m \cdot p_i\) by the
rank, \(i\), :\(\frac{m \cdot p_i }{i}\), which is derived from the Benjamini--Hochberg \index{P-value}
procedure. This procedure is developed to control for ``False Discovery Rate (FDR)''
, which is the proportion of false positives among all significant tests. And in
practical terms, we get the ``FDR-adjusted P-value'' from the procedure described
above. This gives us an estimate of the proportion of false discoveries for a given
test. To elaborate, p-value of 0.05 implies that 5\% of all tests will be false positives. An FDR-adjusted p-value of 0.05 implies that 5\% of significant tests will be false positives. The FDR-adjusted P-values will result in a lower number of false positives.

One final method that is also popular is called the ``q-value''
method and related to the method above. This procedure relies on estimating the proportion of true null
hypotheses from the distribution of raw p-values and using that quantity
to come up with what is called a ``q-value'', which is also an FDR-adjusted P-value \citep{Storey2003-nv}. That can be practically defined
as ``the proportion of significant features that turn out to be false
leads.'' A q-value 0.01 would mean 1\% of the tests called significant at this \index{q-value}
level will be truly null on average. Within the genomics community
q-value and FDR adjusted P-value are synonymous although they can be
calculated differently.

In R, the base function \texttt{p.adjust()} implements most of the p-value correction
methods described above. For the q-value, we can use the \texttt{qvalue} package from
Bioconductor. Below we demonstrate how to use them on a set of simulated
p-values. The plot in Figure \ref{fig:multtest} shows that Bonferroni correction does a terrible job. FDR(BH) and q-value
approach are better but, the q-value approach is more permissive than FDR(BH).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(qvalue)}
\KeywordTok{data}\NormalTok{(hedenfalk)}

\NormalTok{qvalues <-}\StringTok{ }\KeywordTok{qvalue}\NormalTok{(hedenfalk}\OperatorTok{$}\NormalTok{p)}\OperatorTok{$}\NormalTok{q}
\NormalTok{bonf.pval=}\KeywordTok{p.adjust}\NormalTok{(hedenfalk}\OperatorTok{$}\NormalTok{p,}\DataTypeTok{method =}\StringTok{"bonferroni"}\NormalTok{)}
\NormalTok{fdr.adj.pval=}\KeywordTok{p.adjust}\NormalTok{(hedenfalk}\OperatorTok{$}\NormalTok{p,}\DataTypeTok{method =}\StringTok{"fdr"}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(hedenfalk}\OperatorTok{$}\NormalTok{p,qvalues,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
     \DataTypeTok{xlab=}\StringTok{"raw P-values"}\NormalTok{,}\DataTypeTok{ylab=}\StringTok{"adjusted P-values"}\NormalTok{)}
\KeywordTok{points}\NormalTok{(hedenfalk}\OperatorTok{$}\NormalTok{p,bonf.pval,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\KeywordTok{points}\NormalTok{(hedenfalk}\OperatorTok{$}\NormalTok{p,fdr.adj.pval,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{,}\DataTypeTok{legend=}\KeywordTok{c}\NormalTok{(}\StringTok{"q-value"}\NormalTok{,}\StringTok{"FDR (BH)"}\NormalTok{,}\StringTok{"Bonferroni"}\NormalTok{),}
       \DataTypeTok{fill=}\KeywordTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{,}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{03-statsForGenomics_files/figure-latex/multtest-1} 

}

\caption{Adjusted P-values via different methods and their relationship to raw P-values}\label{fig:multtest}
\end{figure}

\hypertarget{moderated-t-tests-using-information-from-multiple-comparisons}{%
\subsection{Moderated t-tests: Using information from multiple comparisons}\label{moderated-t-tests-using-information-from-multiple-comparisons}}

In genomics, we usually do not do one test but many, as described above. That means we\index{moderated t-test}
may be able to use the information from the parameters obtained from all
comparisons to influence the individual parameters. For example, if you have many variances
calculated for thousands of genes across samples, you can force individual
variance estimates to shrink toward the mean or the median of the distribution
of variances. This usually creates better performance in individual variance
estimates and therefore better performance in significance testing, which
depends on variance estimates. How much the values are shrunk toward a common
value depends on the exact method used. These tests in general are called moderated
t-tests or shrinkage t-tests. One approach popularized by Limma software is
to use so-called ``Empirical Bayes methods''\index{empirical Bayes methods}. The main formulation in these
methods is \(\hat{V_g} = aV_0 + bV_g\), where \(V_0\) is the background variability and \(V_g\) is the individual variability. Then, these methods estimate \(a\) and \(b\) in various ways to come up with a ``shrunk'' version of the variability, \(\hat{V_g}\). Bayesian inference can make use of prior knowledge to make inference about properties of the data. In a Bayesian viewpoint,
the prior knowledge, in this case variability of other genes, can be used to calculate the variability of an individual gene. In our
case, \(V_0\) would be the prior knowledge we have on the variability of
the genes and we
use that knowledge to influence our estimate for the individual genes.

Below we are simulating a gene expression matrix with 1000 genes, and 3 test
and 3 control groups. Each row is a gene, and in normal circumstances we would
like to find differentially expressed genes. In this case, we are simulating
them from the same distribution, so in reality we do not expect any differences.
We then use the adjusted standard error estimates in empirical Bayesian spirit but, in a very crude way. We just shrink the gene-wise standard error estimates towards the median with equal \(a\) and \(b\) weights. That is to say, we add the individual estimate to the
median of the standard error distribution from all genes and divide that quantity by 2. So if we plug that into the above formula, what we do is:

\[ \hat{V_g} = (V_0 + V_g)/2 \]

In the code below, we are avoiding for loops or apply family functions
by using vectorized operations. The code below samples gene expression values from a hypothetical distribution. Since all the values come from the same distribution, we do not expect differences between groups. We then calculate moderated and unmoderated t-test statistics and plot the P-value distributions for tests. The results are shown in Figure \ref{fig:modTtestChp3}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}

\CommentTok{#sample data matrix from normal distribution}

\NormalTok{gset=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{3000}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{200}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{70}\NormalTok{)}
\NormalTok{data=}\KeywordTok{matrix}\NormalTok{(gset,}\DataTypeTok{ncol=}\DecValTok{6}\NormalTok{)}

\CommentTok{# set groups}
\NormalTok{group1=}\DecValTok{1}\OperatorTok{:}\DecValTok{3}
\NormalTok{group2=}\DecValTok{4}\OperatorTok{:}\DecValTok{6}
\NormalTok{n1=}\DecValTok{3}
\NormalTok{n2=}\DecValTok{3}
\NormalTok{dx=}\KeywordTok{rowMeans}\NormalTok{(data[,group1])}\OperatorTok{-}\KeywordTok{rowMeans}\NormalTok{(data[,group2])}
  
\KeywordTok{require}\NormalTok{(matrixStats)}

\CommentTok{# get the esimate of pooled variance }
\NormalTok{stderr =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{( (}\KeywordTok{rowVars}\NormalTok{(data[,group1])}\OperatorTok{*}\NormalTok{(n1}\DecValTok{-1}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{       }\KeywordTok{rowVars}\NormalTok{(data[,group2])}\OperatorTok{*}\NormalTok{(n2}\DecValTok{-1}\NormalTok{)) }\OperatorTok{/}\StringTok{ }\NormalTok{(n1}\OperatorTok{+}\NormalTok{n2}\DecValTok{-2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{( }\DecValTok{1}\OperatorTok{/}\NormalTok{n1 }\OperatorTok{+}\StringTok{ }\DecValTok{1}\OperatorTok{/}\NormalTok{n2 ))}

\CommentTok{# do the shrinking towards median}
\NormalTok{mod.stderr =}\StringTok{ }\NormalTok{(stderr }\OperatorTok{+}\StringTok{ }\KeywordTok{median}\NormalTok{(stderr)) }\OperatorTok{/}\StringTok{ }\DecValTok{2} \CommentTok{# moderation in variation}

\CommentTok{# esimate t statistic with moderated variance}
\NormalTok{t.mod <-}\StringTok{ }\NormalTok{dx }\OperatorTok{/}\StringTok{ }\NormalTok{mod.stderr}

\CommentTok{# calculate P-value of rejecting null }
\NormalTok{p.mod =}\StringTok{ }\DecValTok{2}\OperatorTok{*}\KeywordTok{pt}\NormalTok{( }\OperatorTok{-}\KeywordTok{abs}\NormalTok{(t.mod), n1}\OperatorTok{+}\NormalTok{n2}\DecValTok{-2}\NormalTok{ )}

\CommentTok{# esimate t statistic without moderated variance}
\NormalTok{t =}\StringTok{ }\NormalTok{dx }\OperatorTok{/}\StringTok{ }\NormalTok{stderr}

\CommentTok{# calculate P-value of rejecting null }
\NormalTok{p =}\StringTok{ }\DecValTok{2}\OperatorTok{*}\KeywordTok{pt}\NormalTok{( }\OperatorTok{-}\KeywordTok{abs}\NormalTok{(t), n1}\OperatorTok{+}\NormalTok{n2}\DecValTok{-2}\NormalTok{ )}

\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{hist}\NormalTok{(p,}\DataTypeTok{col=}\StringTok{"cornflowerblue"}\NormalTok{,}\DataTypeTok{border=}\StringTok{"white"}\NormalTok{,}\DataTypeTok{main=}\StringTok{""}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"P-values t-test"}\NormalTok{)}
\KeywordTok{mtext}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"signifcant tests:"}\NormalTok{,}\KeywordTok{sum}\NormalTok{(p}\OperatorTok{<}\FloatTok{0.05}\NormalTok{))  )}
\KeywordTok{hist}\NormalTok{(p.mod,}\DataTypeTok{col=}\StringTok{"cornflowerblue"}\NormalTok{,}\DataTypeTok{border=}\StringTok{"white"}\NormalTok{,}\DataTypeTok{main=}\StringTok{""}\NormalTok{,}
     \DataTypeTok{xlab=}\StringTok{"P-values mod. t-test"}\NormalTok{)}
\KeywordTok{mtext}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"signifcant tests:"}\NormalTok{,}\KeywordTok{sum}\NormalTok{(p.mod}\OperatorTok{<}\FloatTok{0.05}\NormalTok{))  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{03-statsForGenomics_files/figure-latex/modTtestChp3-1} 

}

\caption{The distributions of P-values obtained by t-tests and moderated t-tests}\label{fig:modTtestChp3}
\end{figure}

\begin{rmdtip}
\begin{rmdtip}

\textbf{Want to know more ?}

\begin{itemize}
\tightlist
\item
  Basic statistical concepts

  \begin{itemize}
  \tightlist
  \item
    ``Cartoon guide to statistics'' by Gonick \& Smith \citep{gonick2005cartoon}. Provides central concepts depicted as cartoons in a funny but clear and accurate manner.
  \item
    ``OpenIntro Statistics'' \citep{diez2015openintro} (Free e-book \url{http://openintro.org}). This book provides fundamental statistical concepts in a clear and easy way. It includes R code.
  \end{itemize}
\item
  Hands-on statistics recipes with R

  \begin{itemize}
  \tightlist
  \item
    ``The R book'' \citep{crawley2012r}. This is the main R book for anyone interested in statistical concepts and their application in R. It requires some background in statistics since the main focus is applications in R.
  \end{itemize}
\item
  Moderated tests

  \begin{itemize}
  \tightlist
  \item
    Comparison of moderated tests for differential expression \citep{de2010benchmark} \url{http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-17}
  \item
    Limma method developed for testing differential expression between genes using a moderated test \citep{smyth2004linear} \url{http://www.statsci.org/smyth/pubs/ebayes.pdf}
  \end{itemize}
\end{itemize}

\end{rmdtip}
\end{rmdtip}

\hypertarget{relationship-between-variables-linear-models-and-correlation}{%
\section{Relationship between variables: Linear models and correlation}\label{relationship-between-variables-linear-models-and-correlation}}

In genomics, we would often need to measure or model the relationship between
variables. We might want to know about expression of a particular gene in liver
in relation to the dosage of a drug that patient receives. Or, we may want to know
DNA methylation of a certain locus in the genome in relation to the age of the sample donor. Or, we might be interested in the relationship between histone
modifications and gene expression\index{histone modification}. Is there a linear relationship, the more \index{gene expression}
histone modification the more the gene is expressed ?

In these
situations and many more, linear regression or linear models can be used to \index{linear regression}
model the relationship with a ``dependent'' or ``response'' variable (expression or
methylation
in the above examples) and one or more ``independent'' or ``explanatory'' variables (age, drug dosage or histone modification in the above examples). Our simple linear model has the
following components.

\[  Y= \beta_0+\beta_1X + \epsilon \]

In the equation above, \(Y\) is the response variable and \(X\) is the explanatory
variable. \(\epsilon\) is the mean-zero error term. Since the line fit will not
be able to precisely predict the \(Y\) values, there will be some error associated
with each prediction when we compare it to the original \(Y\) values. This error
is captured in the \(\epsilon\) term. We can alternatively write the model as
follows to emphasize that the model approximates \(Y\), in this case notice that we removed the \(\epsilon\) term: \(Y \sim \beta_0+\beta_1X\).

The plot below in Figure \ref{fig:histoneLmChp3} shows the relationship between
histone modification (trimethylated forms of histone H3 at lysine 4, aka H3K4me3)
and gene expression for 100 genes. The blue line is our model with estimated
coefficients (\(\hat{y}=\hat{\beta}_0 + \hat{\beta}_1X\), where \(\hat{\beta}_0\)
and \(\hat{\beta}_1\) are the estimated values of \(\beta_0\) and
\(\beta_1\), and \(\hat{y}\) indicates the prediction). The red lines indicate the individual
errors per data point, indicated as \(\epsilon\) in the formula above.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{03-statsForGenomics_files/figure-latex/histoneLmChp3-1} 

}

\caption{Relationship between histone modification score and gene expression. Increasing histone modification, H3K4me3, seems to be associated with increasing gene expression. Each dot is a gene}\label{fig:histoneLmChp3}
\end{figure}

There could be more than one explanatory variable. We then simply add more \(X\)
and \(\beta\) to our model. If there are two explanatory variables our model
will look like this:

\[  Y= \beta_0+\beta_1X_1 +\beta_2X_2 + \epsilon \]

In this case, we will be fitting a plane rather than a line. However, the fitting
process which we will describe in the later sections will not change for our
gene expression problem. We can introduce one more histone modification, H3K27me3. We will then have a linear model with 2 explanatory variables and the
fitted plane will look like the one in Figure \ref{fig:histoneLm2chp3}. The gene expression values are shown
as dots below and above the fitted plane. Linear regression and its extensions which make use of other distributions (generalized linear models) \index{generalized linear model} are central in computational genomics for statistical tests. We will see more of how regression is used in statistical hypothesis testing for computational genomics in Chapters \ref{rnaseqanalysis} and \ref{bsseq}.

\begin{figure}

{\centering \includegraphics[width=0.65\linewidth]{03-statsForGenomics_files/figure-latex/histoneLm2chp3-1} 

}

\caption{Association of gene expression with H3K4me3 and H3K27me3 histone modifications.}\label{fig:histoneLm2chp3}
\end{figure}

\hypertarget{matrix-notation-for-linear-models}{%
\subsubsection{Matrix notation for linear models}\label{matrix-notation-for-linear-models}}

We can naturally have more explanatory variables than just two. The formula
below has \(n\) explanatory variables.

\[Y= \beta_0+\beta_1X_1+\beta_2X_2 +  \beta_3X_3 + .. + \beta_nX_n +\epsilon\]

If there are many variables, it would be easier
to write the model in matrix notation. The matrix form of linear model with
two explanatory variables will look like the one
below. The first matrix would be our data matrix. This contains our explanatory
variables and a column of 1s. The second term is a column vector of \(\beta\)
values. We also add a vector of error terms, \(\epsilon\)s, to the matrix multiplication.

\[
 \mathbf{Y} = \left[\begin{array}{rrr}
1 & X_{1,1} & X_{1,2} \\
1 & X_{2,1} & X_{2,2} \\
1 & X_{3,1} & X_{3,2} \\
1 & X_{4,1} & X_{4,2}
\end{array}\right]
%
\left[\begin{array}{rrr}
\beta_0 \\
\beta_1 \\
\beta_2 
\end{array}\right]
% 
+
\left[\begin{array}{rrr}
\epsilon_1 \\
\epsilon_2 \\ 
\epsilon_3 \\ 
\epsilon_0
\end{array}\right]
\]

The multiplication of the data matrix and \(\beta\) vector and addition of the
error terms simply results in the following set of equations per data point:

\[
\begin{aligned}
Y_1= \beta_0+\beta_1X_{1,1}+\beta_2X_{1,2} +\epsilon_1 \\
Y_2= \beta_0+\beta_1X_{2,1}+\beta_2X_{2,2} +\epsilon_2 \\
Y_3= \beta_0+\beta_1X_{3,1}+\beta_2X_{3,2} +\epsilon_3 \\
Y_4= \beta_0+\beta_1X_{4,1}+\beta_2X_{4,2} +\epsilon_4 
\end{aligned}
\]

This expression involving the multiplication of the data matrix, the
\(\beta\) vector and vector of error terms (\(\epsilon\))
could be simply written as follows.

\[Y=X\beta + \epsilon\]

In the equation, above \(Y\) is the vector of response variables, \(X\) is the
data matrix, and \(\beta\) is the vector of coefficients.
This notation is more concise and often used in scientific papers. However, this
also means you need some understanding of linear algebra to follow the math
laid out in such resources.

\hypertarget{how-to-fit-a-line}{%
\subsection{How to fit a line}\label{how-to-fit-a-line}}

At this point a major question is left unanswered: How did we fit this line?
We basically need to define \(\beta\) values in a structured way.
There are multiple ways of understanding how
to do this, all of which converge to the same
end point. We will describe them one by one.

\hypertarget{the-cost-or-loss-function-approach}{%
\subsubsection{The cost or loss function approach}\label{the-cost-or-loss-function-approach}}

This is the first approach and in my opinion is easiest to understand. \index{cost function}
We try to optimize a function, often called the ``cost function'' or ``loss function''. \index{loss function}
The cost function
is the sum of squared differences between the predicted \(\hat{Y}\) values from our model
and the original \(Y\) values. The optimization procedure tries to find \(\beta\) values \index{optimization}
that minimize this difference between the reality and predicted values.

\[min \sum{(y_i-(\beta_0+\beta_1x_i))^2}\]

Note that this is related to the error term, \(\epsilon\), we already mentioned
above. We are trying to minimize the squared sum of \(\epsilon_i\) for each data
point. We can do this minimization by a bit of calculus.
The rough algorithm is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Pick a random starting point, random \(\beta\) values.
\item
  Take the partial derivatives of the cost function to see which direction is
  the way to go in the cost function.
\item
  Take a step toward the direction that minimizes the cost function.

  \begin{itemize}
  \tightlist
  \item
    Step size is a parameter to choose, there are many variants.
  \end{itemize}
\item
  Repeat step 2,3 until convergence.
\end{enumerate}

This is the basis of the ``gradient descent'' algorithm.\index{gradient descent} With the help of partial
derivatives we define a ``gradient'' on the cost function and follow that through
multiple iterations until convergence, meaning until the results do not
improve defined by a margin. The algorithm usually converges to optimum \(\beta\)
values. In Figure \ref{fig:3dcostfunc}, we show the cost function over various \(\beta_0\) and \(\beta_1\)
values for the histone modification and gene expression data set. The algorithm
will pick a point on this graph and traverse it incrementally based on the
derivatives and converge to the bottom of the cost function ``well''. Such optimization methods are the core of machine learning methods we will cover later in Chapters \ref{unsupervisedLearning} and
\ref{supervisedLearning}.

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{03-statsForGenomics_files/figure-latex/3dcostfunc-1} 

}

\caption{Cost function landscape for linear regression with changing beta values. The optimization process tries to find the lowest point in this landscape by implementing a strategy for updating beta values toward the lowest point in the landscape.}\label{fig:3dcostfunc}
\end{figure}

\hypertarget{not-cost-function-but-maximum-likelihood-function}{%
\subsubsection{Not cost function but maximum likelihood function}\label{not-cost-function-but-maximum-likelihood-function}}

We can also think of this problem from a more statistical point of view. In \index{maximum likelihood estimation}
essence, we are looking for best statistical parameters, in this
case \(\beta\) values, for our model that are most likely to produce such a
scatter of data points given the explanatory variables. This is called the
``maximum likelihood'' approach. The approach assumes that a given response variable \(y_i\) follows a normal distribution with mean \(\beta_0+\beta_1x_i\) and \index{variance} variance \(s^2\). Therefore the probability of observing any given \(y_i\) value is dependent on the \(\beta_0\) and \(\beta_1\) values. Since \(x_i\), the explanatory variable, is fixed within our data set, we can maximize the probability of observing any given \(y_i\) by varying \(\beta_0\) and \(\beta_1\) values. The trick is to find \(\beta_0\) and \(\beta_1\) values that maximizes the probability of observing all the response variables in the dataset given the explanatory variables. The probability of observing a response variable \(y_i\) with assumptions we described above is shown below. Note that this assumes variance is constant and \(s^2=\frac{\sum{\epsilon_i}}{n-2}\) is an unbiased estimation for population variance, \(\sigma^2\).\index{variance}

\[P(y_{i})=\frac{1}{s\sqrt{2\pi} }e^{-\frac{1}{2}\left(\frac{y_i-(\beta_0 + \beta_1x_i)}{s}\right)^2}\]

Following from the probability equation above, the likelihood function (shown as \(L\) below) for
linear regression is \index{linear regression} multiplication of \(P(y_{i})\) for all data points.

\[L=P(y_1)P(y_2)P(y_3)..P(y_n)=\prod\limits_{i=1}^n{P_i}\]

This can be simplified to the following equation by some algebra, assumption of normal distribution, and taking logs (since it is
easier to add than multiply).

\[ln(L) = -nln(s\sqrt{2\pi}) - \frac{1}{2s^2} \sum\limits_{i=1}^n{(y_i-(\beta_0 + \beta_1x_i))^2} \]

As you can see, the right part of the function is the negative of the cost function
defined above. If we wanted to optimize this function we would need to take the derivative of
the function with respect to the \(\beta\) parameters. That means we can ignore the
first part since there are no \(\beta\) terms there. This simply reduces to the
negative of the cost function. Hence, this approach produces exactly the same
result as the cost function approach. The difference is that we defined our
problem
within the domain of statistics. This particular function has still to be optimized. This can be done with some calculus without the need for an
iterative approach.

The maximum likelihood approach also opens up other possibilities for regression. For the case above, we assumed that the points around the mean are distributed by normal distribution. However, there are other cases where this assumption may not hold. For example, for the count data the mean and variance relationship is not constant; the higher the mean counts, the higher the variance. In these cases, the regression framework with maximum likelihood estimation can still be used. We simply change the underlying assumptions about the distribution and calculate the likelihood with a new distribution in mind,
and maximize the parameters for that likelihood. This gives way to ``generalized linear model''\index{generalized linear model} approach where errors for the response variables can have other distributions than normal distribution. We will see examples of these generalized linear models in Chapter \ref{rnaseqanalysis} and \ref{bsseq}.

\hypertarget{linear-algebra-and-closed-form-solution-to-linear-regression}{%
\subsubsection{Linear algebra and closed-form solution to linear regression}\label{linear-algebra-and-closed-form-solution-to-linear-regression}}

The last approach we will describe is the minimization process using linear \index{linear regression}
algebra. If you find this concept challenging, feel free to skip it, but scientific publications and other books frequently use matrix notation and linear algebra to define and solve regression problems. In this case, we do not use an iterative approach. Instead, we will
minimize the cost function by explicitly taking its derivatives with respect to
\(\beta\)'s and setting them to zero. This is doable by employing linear algebra
and matrix calculus. This approach is also called ``ordinary least squares''. We \index{ordinary least squares regression}
will not
show the whole derivation here, but the following expression
is what we are trying to minimize in matrix notation, which is basically a
different notation of the same minimization problem defined above. Remember
\(\epsilon_i=Y_i-(\beta_0+\beta_1x_i)\)

\[
\begin{aligned}
\sum\epsilon_{i}^2=\epsilon^T\epsilon=(Y-{\beta}{X})^T(Y-{\beta}{X}) \\
=Y^T{Y}-2{\beta}^T{Y}+{\beta}^TX^TX{\beta}
\end{aligned}
\]
After rearranging the terms, we take the derivative of \(\epsilon^T\epsilon\)
with respect to \(\beta\), and equalize that to zero. We then arrive at
the following for estimated \(\beta\) values, \(\hat{\beta}\):

\[\hat{\beta}=(X^TX)^{-1}X^TY\]

This requires you to calculate the inverse of the \(X^TX\) term, which could
be slow for large matrices. Using an iterative approach over the cost function
derivatives will be faster for larger problems.
The linear algebra notation is something you will see in the papers
or other resources often. If you input the data matrix X and solve the \((X^TX)^{-1}\)
,
you get the following values for \(\beta_0\) and \(\beta_1\) for simple regression \index{linear regression}. However, we should note that this simple linear regression case can easily
be solved algebraically without the need for matrix operations. This can be done
by taking the derivative of \(\sum{(y_i-(\beta_0+\beta_1x_i))^2}\) with respect to
\(\beta_1\), rearranging the terms and equalizing the derivative to zero.

\[\hat{\beta_1}=\frac{\sum{(x_i-\overline{X})(y_i-\overline{Y})}}{ \sum{(x_i-\overline{X})^2} }\]
\[\hat{\beta_0}=\overline{Y}-\hat{\beta_1}\overline{X}\]

\hypertarget{fitting-lines-in-r}{%
\subsubsection{Fitting lines in R}\label{fitting-lines-in-r}}

After all this theory, you will be surprised how easy it is to fit lines in R.
This is achieved just by the \texttt{lm()} function, which stands for linear models. Let's do this
for a simulated data set and plot the fit. The first step is to simulate the
data. We will decide on \(\beta_0\) and \(\beta_1\) values. Then we will decide
on the variance parameter, \(\sigma\), to be used in simulation of error terms,\index{variance}
\(\epsilon\). We will first find \(Y\) values, just using the linear equation
\(Y=\beta0+\beta_1X\), for
a set of \(X\) values. Then, we will add the error terms to get our simulated values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random number seed, so that the random numbers from the text}
\CommentTok{# is the same when you run the code.}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{32}\NormalTok{)}

\CommentTok{# get 50 X values between 1 and 100}
\NormalTok{x =}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{)}

\CommentTok{# set b0,b1 and variance (sigma)}
\NormalTok{b0 =}\StringTok{ }\DecValTok{10}
\NormalTok{b1 =}\StringTok{ }\DecValTok{2}
\NormalTok{sigma =}\StringTok{ }\DecValTok{20}
\CommentTok{# simulate error terms from normal distribution}
\NormalTok{eps =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{0}\NormalTok{,sigma)}
\CommentTok{# get y values from the linear equation and addition of error terms}
\NormalTok{y =}\StringTok{ }\NormalTok{b0 }\OperatorTok{+}\StringTok{ }\NormalTok{b1}\OperatorTok{*}\NormalTok{x}\OperatorTok{+}\StringTok{ }\NormalTok{eps}
\end{Highlighting}
\end{Shaded}

Now let us fit a line using the \texttt{lm()} function. The function requires a formula, and
optionally a data frame. We need to pass the following expression within the
\texttt{lm()} function, \texttt{y\textasciitilde{}x}, where \texttt{y} is the simulated \(Y\) values and \texttt{x} is the explanatory variables \(X\). We will then use the \texttt{abline()} function to draw the fit. The resulting plot is shown in Figure \ref{fig:geneExpLinearModel}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod1=}\KeywordTok{lm}\NormalTok{(y}\OperatorTok{~}\NormalTok{x)}

\CommentTok{# plot the data points}
\KeywordTok{plot}\NormalTok{(x,y,}\DataTypeTok{pch=}\DecValTok{20}\NormalTok{,}
     \DataTypeTok{ylab=}\StringTok{"Gene Expression"}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"Histone modification score"}\NormalTok{)}
\CommentTok{# plot the linear fit}
\KeywordTok{abline}\NormalTok{(mod1,}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{03-statsForGenomics_files/figure-latex/geneExpLinearModel-1} 

}

\caption{Gene expression and histone modification score modeled by linear regression.}\label{fig:geneExpLinearModel}
\end{figure}

\hypertarget{how-to-estimate-the-error-of-the-coefficients}{%
\subsection{How to estimate the error of the coefficients}\label{how-to-estimate-the-error-of-the-coefficients}}

Since we are using a sample to estimate the coefficients, they are
not exact; with every random sample they will vary. In Figure \ref{fig:regCoeffRandomSamples}, we
take multiple samples from the population and fit lines to each
sample; with each sample the lines slightly change. We are overlaying the
points and the lines for each sample on top of the other samples. When we take 200 samples and fit lines for each of them, the line fits are
variable. And,
we get a normal-like distribution of \(\beta\) values with a defined mean \index{linear regression}
and standard deviation, which is called standard error of the
coefficients.

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{03-statsForGenomics_files/figure-latex/regCoeffRandomSamples-1} 

}

\caption{Regression coefficients vary with every random sample. The figure illustrates the variability of regression coefficients when regression is done using a sample of data points. Histograms depict this variability for $b_0$ and $b_1$ coefficients.}\label{fig:regCoeffRandomSamples}
\end{figure}

Normally, we will not have access to the population to do repeated sampling,
model fitting, and estimation of the standard error for the coefficients. But
there is statistical theory that helps us infer the population properties from
the sample. When we assume that error terms have constant variance and mean zero
, we can model the uncertainty in the regression coefficients, \(\beta\)s.
The estimates for standard errors of \(\beta\)s for simple regression are as \index{linear regression}
follows and shown without derivation.

\[
\begin{aligned}
s=RSE=\sqrt{\frac{\sum{(y_i-(\beta_0+\beta_1x_i))^2}}{n-2}  } =\sqrt{\frac{\sum{\epsilon^2}}{n-2}  } \\
SE(\hat{\beta_1})=\frac{s}{\sqrt{\sum{(x_i-\overline{X})^2}}} \\
SE(\hat{\beta_0})=s\sqrt{ \frac{1}{n} + \frac{\overline{X}^2}{\sum{(x_i-\overline{X})^2} }  }
\end{aligned}
\]

Notice that that \(SE(\beta_1)\) depends on the estimate of variance of
residuals shown as \(s\) or \textbf{Residual Standard Error (RSE)}.\index{variance} \index{residuals}
Notice also the standard error depends on the spread of \(X\). If \(X\) values have more \index{Residual Standard Error (RSE)}
variation, the standard error will be lower. This intuitively makes sense since if the
spread of \(X\) is low, the regression line will be able to wiggle more
compared to a regression line that is fit to the same number of points but
covers a greater range on the X-axis.

The standard error estimates can also be used to calculate confidence intervals and test
hypotheses, since the following quantity, called t-score, approximately follows a
t-distribution with \(n-p\) degrees of freedom, where \(n\) is the number
of data points and \(p\) is the number of coefficients estimated.

\[ \frac{\hat{\beta_i}-\beta_test}{SE(\hat{\beta_i})}\]

Often, we would like to test the null hypothesis if a coefficient is equal to
zero or not. For simple regression, this could mean if there is a relationship
between the explanatory variable and the response variable. We would calculate the
t-score as follows \(\frac{\hat{\beta_i}-0}{SE(\hat{\beta_i})}\), and compare it
to the t-distribution with \(d.f.=n-p\) to get the p-value.

We can also
calculate the uncertainty of the regression coefficients using confidence
intervals, the range of values that are likely to contain \(\beta_i\). The 95\%
confidence interval for \(\hat{\beta_i}\) is
\(\hat{\beta_i}\) ± \(t_{0.975}SE(\hat{\beta_i})\).
\(t_{0.975}\) is the 97.5\% percentile of
the t-distribution with \(d.f. = n – p\).

In R, the \texttt{summary()} function will test all the coefficients for the null hypothesis
\(\beta_i=0\). The function takes the model output obtained from the \texttt{lm()}
function. To demonstrate this, let us first get some data. The procedure below
simulates data to be used in a regression setting and it is useful to examine \index{linear regression}
what the linear model expects to model the data.

Since we have the data, we can build our model and call the \texttt{summary} function.
We will then use the \texttt{confint()} function to get the confidence intervals on the
coefficients and the \texttt{coef()} function to pull out the estimated coefficients from
the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod1=}\KeywordTok{lm}\NormalTok{(y}\OperatorTok{~}\NormalTok{x)}
\KeywordTok{summary}\NormalTok{(mod1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -77.11 -18.44   0.33  16.06  57.23 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 13.24538    6.28869   2.106   0.0377 *  
## x            0.49954    0.05131   9.736 4.54e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 28.77 on 98 degrees of freedom
## Multiple R-squared:  0.4917, Adjusted R-squared:  0.4865 
## F-statistic: 94.78 on 1 and 98 DF,  p-value: 4.537e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get confidence intervals }
\KeywordTok{confint}\NormalTok{(mod1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                 2.5 %     97.5 %
## (Intercept) 0.7656777 25.7250883
## x           0.3977129  0.6013594
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# pull out coefficients from the model}
\KeywordTok{coef}\NormalTok{(mod1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)           x 
##  13.2453830   0.4995361
\end{verbatim}

The \texttt{summary()} function prints out an extensive list of values.
The ``Coefficients'' section has the estimates, their standard error, t score,
and the p-value from the hypothesis test \(H_0:\beta_i=0\). As you can see, the
estimate we get for the coefficients and their standard errors are close to
the ones we get from repeatedly sampling and getting a distribution of
coefficients. This is statistical inference at work, so we can estimate the
population properties within a certain error using just a sample.

\hypertarget{accuracy-of-the-model}{%
\subsection{Accuracy of the model}\label{accuracy-of-the-model}}

If you have observed the table output of the \texttt{summary()} function, you must have noticed there are some other outputs, such as ``Residual standard error'',
``Multiple R-squared'' and ``F-statistic''. These are metrics that are useful
for assessing the accuracy of the model. We will explain them one by one.

\textbf{RSE} is simply the square-root of\index{Residual Standard Error (RSE)}
the sum of squared error terms, divided by degrees of freedom, \(n-p\). For the simple
linear regression case, degrees of freedom is \(n-2\). Sum of the squares of the error terms is also
called the \textbf{``Residual sum of squares''}, RSS. \index{Residual sum of squares (RSS)}So the RSE is
calculated as follows:

\[ s=RSE=\sqrt{\frac{\sum{(y_i-\hat{Y_i})^2 }}{n-p}}=\sqrt{\frac{RSS}{n-p}}\]

The RSE is a way of assessing the model fit. The larger the RSE the worse the
model is. However, this is an absolute measure in the units of \(Y\) and we have nothing to
compare against. One idea is that we divide it by the RSS of a simpler model
for comparative purposes. That simpler model is in this case is the model
with the intercept, \(\beta_0\). A very bad model will have close to zero
coefficients for explanatory variables, and the RSS of that model
will be close to the RSS of the model with only the intercept. In such
a model the intercept will be equal to \(\overline{Y}\). As it turns out, the RSS of the model with
just the intercept is called the \emph{``Total Sum of Squares'' or TSS}. A good model will have a low \(RSS/TSS\). The metric \(R^2\) uses these quantities to calculate a score between 0 and 1, and the closer to 1, the better the model. Here is how
it is calculated:

\[R^2=1-\frac{RSS}{TSS}=\frac{TSS-RSS}{TSS}=1-\frac{RSS}{TSS}\]

The \(TSS-RSS\) part of the formula is often referred to as ``explained variability'' in
the model. The bottom part is for ``total variability''. With this interpretation, the higher
the ``explained variability'', the better the model. For simple linear regression
with one explanatory variable, the square root of \(R^2\) is a quantity known
as the absolute value of the correlation coefficient, which can be calculated for any pair of variables, not only
the
response and the explanatory variables. \emph{Correlation} is the general measure of \index{correlation}
linear
relationship between two variables. One
of the most popular flavors of correlation is the Pearson correlation coefficient. Formally, it is the
\emph{covariance} of X and Y divided by multiplication of standard deviations of \index{covariance}
X and Y. In R, it can be calculated with the \texttt{cor()} function.

\[ 
r_{xy}=\frac{cov(X,Y)}{\sigma_x\sigma_y}
      =\frac{\sum\limits_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}
            {\sqrt{\sum\limits_{i=1}^n (x_i-\bar{x})^2 \sum\limits_{i=1}^n (y_i-\bar{y})^2}}
\]
In the equation above, \(cov\) is the covariance; this is again a measure of
how much two variables change together, like correlation. If two variables \index{covariance}
show similar behavior, they will usually have a positive covariance value. If they have opposite behavior, the covariance will have a negative value.
However, these values are boundless. A normalized way of looking at
covariance is to divide covariance by the multiplication of standard
errors of X and Y. This bounds the values to -1 and 1, and as mentioned
above, is called Pearson correlation coefficient. The values that change in a similar manner will have a positive coefficient, the values that change in \index{correlation}
an opposite manner will have a negative coefficient, and pairs that do not have
a linear relationship will have \(0\) or near \(0\) correlation. In
Figure \ref{fig:CorCovar}, we are showing \(R^2\), the correlation
coefficient, and covariance for different scatter plots.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{03-statsForGenomics_files/figure-latex/CorCovar-1} 

}

\caption{Correlation and covariance for different scatter plots.}\label{fig:CorCovar}
\end{figure}

For simple linear regression, correlation can be used to assess the model. However, this becomes useless as a measure of general accuracy
if there is more than one explanatory variable as in multiple linear regression. In that case, \(R^2\) is a measure
of accuracy for the model. Interestingly, the square of the
correlation of predicted values
and original response variables (\((cor(Y,\hat{Y}))^2\) ) equals \(R^2\) for \index{$R^2$}
multiple linear regression.\index{linear regression}

The last accuracy measure, or the model fit in general we are going to explain is \emph{F-statistic}. This is a quantity that depends on the RSS and TSS again. It can also answer one important question that other metrics cannot easily answer. That question is whether or not any of the explanatory
variables have predictive value or in other words if all the explanatory variables are zero. We can write the null hypothesis as follows:

\[H_0: \beta_1=\beta_2=\beta_3=...=\beta_p=0 \]

where the alternative is:

\[H_1: \text{at least one } \beta_i \neq 0 \]

Remember that \(TSS-RSS\) is analogous to ``explained variability'' and the RSS is
analogous to ``unexplained variability''. For the F-statistic, we divide explained variance by
unexplained variance. Explained variance is just the \(TSS-RSS\) divided
by degrees of freedom, and unexplained variance is the RSE.
The ratio will follow the F-distribution
with two parameters, the degrees of freedom for the explained variance and
the degrees of freedom for the unexplained variance. The F-statistic for a linear model is calculated as follows.

\[F=\frac{(TSS-RSS)/(p-1)}{RSS/(n-p)}=\frac{(TSS-RSS)/(p-1)}{RSE} \sim F(p-1,n-p)\]

If the variances are the same, the ratio will be 1, and when \(H_0\) is true, then
it can be shown that expected value of \((TSS-RSS)/(p-1)\) will be \(\sigma^2\), which is estimated by the RSE. So, if the variances are significantly different,
the ratio will need to be significantly bigger than 1.
If the ratio is large enough we can reject the null hypothesis. To assess that,
we need to use software or look up the tables for F statistics with calculated
parameters. In R, function \texttt{qf()} can be used to calculate critical value of the
ratio. Benefit of the F-test over
looking at significance of coefficients one by one is that we circumvent
multiple testing problem. If there are lots of explanatory variables
at least 5\% of the time (assuming we use 0.05 as P-value significance
cutoff), p-values from coefficient t-tests will be wrong\index{t-test}. In summary, F-test is a better choice for testing if there is any association
between the explanatory variables and the response variable.

\hypertarget{regression-with-categorical-variables}{%
\subsection{Regression with categorical variables}\label{regression-with-categorical-variables}}

An important feature of linear regression is that categorical variables can
be used as explanatory variables, this feature is very useful in genomics
where explanatory variables can often be categorical. To put it in
context, in our histone modification \index{histone modification} example we can also include if
promoters have CpG islands or not as a variable. In addition, in
differential gene expression, we usually test the difference between
different conditions, which can be encoded as categorical variables in
a linear regression.\index{linear regression} We can sure use the t-test for that as well if there are only 2 conditions, but if there are more conditions and other variables
to control for, such as age or sex of the samples, we need to take those
into account for our statistics, and the t-test alone cannot handle such
complexity. In addition, when we have categorical variables we can also
have numeric variables in the model and we certainly do not have to include
only one type of variable in a model.

The simplest model with categorical variables includes two levels that
can be encoded in 0 and 1. Below, we show linear regression with categorical variables. We then plot the fitted line. This plot is shown in Figure \ref{fig:LMcategorical}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{gene1=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{4}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\NormalTok{gene2=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{2}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\NormalTok{gene.df=}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{exp=}\KeywordTok{c}\NormalTok{(gene1,gene2),}
                  \DataTypeTok{group=}\KeywordTok{c}\NormalTok{( }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{30}\NormalTok{),}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{30}\NormalTok{) ) )}

\NormalTok{mod2=}\KeywordTok{lm}\NormalTok{(exp}\OperatorTok{~}\NormalTok{group,}\DataTypeTok{data=}\NormalTok{gene.df)}
\KeywordTok{summary}\NormalTok{(mod2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = exp ~ group, data = gene.df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.7290 -1.0664  0.0122  1.3840  4.5629 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   2.1851     0.3517   6.214 6.04e-08 ***
## group         1.8726     0.4973   3.765 0.000391 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.926 on 58 degrees of freedom
## Multiple R-squared:  0.1964, Adjusted R-squared:  0.1826 
## F-statistic: 14.18 on 1 and 58 DF,  p-value: 0.0003905
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(mosaic)}
\KeywordTok{plotModel}\NormalTok{(mod2)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{03-statsForGenomics_files/figure-latex/LMcategorical-1} 

}

\caption{Linear model with a categorical variable coded as 0 and 1.}\label{fig:LMcategorical}
\end{figure}

We can even compare more levels, and we do not even have to encode them
ourselves. We can pass categorical variables to the \texttt{lm()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gene.df=}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{exp=}\KeywordTok{c}\NormalTok{(gene1,gene2,gene2),}
                  \DataTypeTok{group=}\KeywordTok{c}\NormalTok{( }\KeywordTok{rep}\NormalTok{(}\StringTok{"A"}\NormalTok{,}\DecValTok{30}\NormalTok{),}\KeywordTok{rep}\NormalTok{(}\StringTok{"B"}\NormalTok{,}\DecValTok{30}\NormalTok{),}\KeywordTok{rep}\NormalTok{(}\StringTok{"C"}\NormalTok{,}\DecValTok{30}\NormalTok{) ) }
\NormalTok{                  )}

\NormalTok{mod3=}\KeywordTok{lm}\NormalTok{(exp}\OperatorTok{~}\NormalTok{group,}\DataTypeTok{data=}\NormalTok{gene.df)}
\KeywordTok{summary}\NormalTok{(mod3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = exp ~ group, data = gene.df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.7290 -1.0793 -0.0976  1.4844  4.5629 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   4.0577     0.3781  10.731  < 2e-16 ***
## groupB       -1.8726     0.5348  -3.502 0.000732 ***
## groupC       -1.8726     0.5348  -3.502 0.000732 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.071 on 87 degrees of freedom
## Multiple R-squared:  0.1582, Adjusted R-squared:  0.1388 
## F-statistic: 8.174 on 2 and 87 DF,  p-value: 0.0005582
\end{verbatim}

\hypertarget{regression-pitfalls}{%
\subsection{Regression pitfalls}\label{regression-pitfalls}}

In most cases one should look at the error terms (residuals) vs.~the fitted
values plot. Any structure in this plot indicates problems such as
non-linearity, correlation of error terms\index{correlation}, non-constant variance or
unusual values driving the fit. Below we briefly explain the potential
issues with the linear regression.

\hypertarget{non-linearity}{%
\paragraph{Non-linearity}\label{non-linearity}}

If the true relationship is far from linearity, prediction accuracy
is reduced and all the other conclusions are questionable. In some cases,
transforming the data with \(logX\), \(\sqrt{X}\), and \(X^2\) could resolve
the issue.

\hypertarget{correlation-of-explanatory-variables}{%
\paragraph{Correlation of explanatory variables}\label{correlation-of-explanatory-variables}}

If the explanatory variables are correlated that could lead to something\\
known as multicolinearity. When this happens SE estimates of the coefficients will be too large. This is usually observed in time-course
data.

\hypertarget{correlation-of-error-terms}{%
\paragraph{Correlation of error terms}\label{correlation-of-error-terms}}

This assumes that the errors of the response variables are uncorrelated with each other. If they are, the confidence intervals of the coefficients
might be too narrow.

\hypertarget{non-constant-variance-of-error-terms}{%
\paragraph{Non-constant variance of error terms}\label{non-constant-variance-of-error-terms}}

This means that different response variables have the same variance in their errors, regardless of the values of the predictor variables. If
the errors are not constant (ex: the errors grow as X values increase), this
will result in unreliable estimates in standard errors as the model
assumes constant variance. Transformation of data, such as
\(logX\) and \(\sqrt{X}\), could help in some cases.

\hypertarget{outliers-and-high-leverage-points}{%
\paragraph{Outliers and high leverage points}\label{outliers-and-high-leverage-points}}

Outliers are extreme values for Y and high leverage points are unusual\index{outliers}
X values. Both of these extremes have the power to affect the fitted line
and the standard errors. In some cases (Ex: if there are measurement errors), they can be
removed from the data for a better fit.

\begin{rmdtip}
\begin{rmdtip}

\textbf{Want to know more ?}

\begin{itemize}
\tightlist
\item
  Linear models and derivations of equations including matrix notation

  \begin{itemize}
  \tightlist
  \item
    \emph{Applied Linear Statistical Models} by Kutner, Nachtsheim, et al.~\citep{kutner2003applied}
  \item
    \emph{Elements of Statistical Learning} by Hastie \& Tibshirani \citep{friedman2001elements}
  \item
    \emph{An Introduction to Statistical Learning} by James, Witten, et al.~\citep{james2013introduction}
  \end{itemize}
\end{itemize}

\end{rmdtip}
\end{rmdtip}

\hypertarget{exercises-1}{%
\section{Exercises}\label{exercises-1}}

\hypertarget{how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions-1}{%
\subsection{How to summarize collection of data points: The idea behind statistical distributions}\label{how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate the means and variances
  of the rows of the following simulated data set, and plot the distributions
  of means and variances using \texttt{hist()} and \texttt{boxplot()} functions. {[}Difficulty: \textbf{Beginner/Intermediate}{]}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}

\CommentTok{#sample data matrix from normal distribution}
\NormalTok{gset=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{600}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{200}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{70}\NormalTok{)}
\NormalTok{data=}\KeywordTok{matrix}\NormalTok{(gset,}\DataTypeTok{ncol=}\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Using the data generated above, calculate the standard deviation of the
  distribution of the means using the \texttt{sd()} function. Compare that to the expected
  standard error obtained from the central limit theorem keeping in mind the
  population parameters were \(\sigma=70\) and \(n=6\). How does the estimate from the random samples change if we simulate more data with
  \texttt{data=matrix(rnorm(6000,mean=200,sd=70),ncol=6)}? {[}Difficulty: \textbf{Beginner/Intermediate}{]}
\item
  Simulate 30 random variables using the \texttt{rpois()} function. Do this 1000 times and calculate the mean of each sample. Plot the sampling distributions of the means
  using a histogram. Get the 2.5th and 97.5th percentiles of the
  distribution. {[}Difficulty: \textbf{Beginner/Intermediate}{]}
\item
  Use the \texttt{t.test()} function to calculate confidence intervals
  of the mean on the first random sample \texttt{pois1} simulated from the \texttt{rpois()} function below. {[}Difficulty: \textbf{Intermediate}{]}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#HINT}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}

\CommentTok{#sample 30 values from poisson dist with lamda paramater =30}
\NormalTok{pois1=}\KeywordTok{rpois}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DataTypeTok{lambda=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Use the bootstrap confidence interval for the mean on \texttt{pois1}. {[}Difficulty: \textbf{Intermediate/Advanced}{]}
\item
  Compare the theoretical confidence interval of the mean from the \texttt{t.test} and the bootstrap confidence interval. Are they similar? {[}Difficulty: \textbf{Intermediate/Advanced}{]}\\
\item
  Try to re-create the following figure, which demonstrates the CLT concept.{[}Difficulty: \textbf{Advanced}{]}
\end{enumerate}

\begin{center}\includegraphics[width=0.55\linewidth]{03-statsForGenomics_files/figure-latex/unnamed-chunk-1-1} \end{center}

\hypertarget{how-to-test-for-differences-in-samples}{%
\subsection{How to test for differences in samples}\label{how-to-test-for-differences-in-samples}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Test the difference of means of the following simulated genes
  using the randomization, \texttt{t-test()}, and \texttt{wilcox.test()} functions.
  Plot the distributions using histograms and boxplots. {[}Difficulty: \textbf{Intermediate/Advanced}{]}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}
\NormalTok{gene1=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{4}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{3}\NormalTok{)}
\NormalTok{gene2=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{3}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Test the difference of the means of the following simulated genes
  using the randomization, \texttt{t-test()} and \texttt{wilcox.test()} functions.
  Plot the distributions using histograms and boxplots. {[}Difficulty: \textbf{Intermediate/Advanced}{]}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{gene1=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{4}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\NormalTok{gene2=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{2}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  We need an extra data set for this exercise. Read the gene expression data set as follows:
  \texttt{gexpFile=system.file("extdata","geneExpMat.rds",package="compGenomRData")\ data=readRDS(gexpFile)}. The data has 100 differentially expressed genes. The first 3 columns are the test samples, and the last 3 are the control samples. Do
  a t-test for each gene (each row is a gene), and record the p-values.
  Then, do a moderated t-test, as shown in section ``Moderated t-tests'' in this chapter, and record
  the p-values. Make a p-value histogram and compare two approaches in terms of the number of significant tests with the \(0.05\) threshold.
  On the p-values use FDR (BH), Bonferroni and q-value adjustment methods.
  Calculate how many adjusted p-values are below 0.05 for each approach.
  {[}Difficulty: \textbf{Intermediate/Advanced}{]}
\end{enumerate}

\hypertarget{relationship-between-variables-linear-models-and-correlation-1}{%
\subsection{Relationship between variables: Linear models and correlation}\label{relationship-between-variables-linear-models-and-correlation-1}}

Below we are going to simulate X and Y values that are needed for the
rest of the exercise.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random number seed, so that the random numbers from the text}
\CommentTok{# is the same when you run the code.}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{32}\NormalTok{)}

\CommentTok{# get 50 X values between 1 and 100}
\NormalTok{x =}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{)}

\CommentTok{# set b0,b1 and variance (sigma)}
\NormalTok{b0 =}\StringTok{ }\DecValTok{10}
\NormalTok{b1 =}\StringTok{ }\DecValTok{2}
\NormalTok{sigma =}\StringTok{ }\DecValTok{20}
\CommentTok{# simulate error terms from normal distribution}
\NormalTok{eps =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{0}\NormalTok{,sigma)}
\CommentTok{# get y values from the linear equation and addition of error terms}
\NormalTok{y =}\StringTok{ }\NormalTok{b0 }\OperatorTok{+}\StringTok{ }\NormalTok{b1}\OperatorTok{*}\NormalTok{x}\OperatorTok{+}\StringTok{ }\NormalTok{eps}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Run the code then fit a line to predict Y based on X. {[}Difficulty:\textbf{Intermediate}{]}
\item
  Plot the scatter plot and the fitted line. {[}Difficulty:\textbf{Intermediate}{]}
\item
  Calculate correlation and R\^{}2. {[}Difficulty:\textbf{Intermediate}{]}
\item
  Run the \texttt{summary()} function and
  try to extract P-values for the model from the object
  returned by \texttt{summary}. See \texttt{?summary.lm}. {[}Difficulty:\textbf{Intermediate/Advanced}{]}
\item
  Plot the residuals vs.~the fitted values plot, by calling the \texttt{plot()}
  function with \texttt{which=1} as the second argument. First argument
  is the model returned by \texttt{lm()}. {[}Difficulty:\textbf{Advanced}{]}
\item
  For the next exercises, read the data set histone modification data set. Use the following to get the path to the file:
\end{enumerate}

\begin{verbatim}
hmodFile=system.file("extdata",
                    "HistoneModeVSgeneExp.rds",
                     package="compGenomRData")`
\end{verbatim}

There are 3 columns in the dataset. These are measured levels of H3K4me3,
H3K27me3 and gene expression per gene. Once you read in the data, plot the scatter plot for H3K4me3 vs.~expression. {[}Difficulty:\textbf{Beginner}{]}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\item
  Plot the scatter plot for H3K27me3 vs.~expression. {[}Difficulty:\textbf{Beginner}{]}
\item
  Fit the model for prediction of expression data using: 1) Only H3K4me3 as explanatory variable, 2) Only H3K27me3 as explanatory variable, and 3) Using both H3K4me3 and H3K27me3 as explanatory variables. Inspect the \texttt{summary()} function output in each case, which terms are significant. {[}Difficulty:\textbf{Beginner/Intermediate}{]}
\item
  Is using H3K4me3 and H3K27me3 better than the model with only H3K4me3? {[}Difficulty:\textbf{Intermediate}{]}
\item
  Plot H3k4me3 vs.~H3k27me3. Inspect the points that do not
  follow a linear trend. Are they clustered at certain segments
  of the plot? Bonus: Is there any biological or technical interpretation
  for those points? {[}Difficulty:\textbf{Intermediate/Advanced}{]}
\end{enumerate}

\hypertarget{unsupervisedLearning}{%
\chapter{Exploratory Data Analysis with Unsupervised Machine Learning}\label{unsupervisedLearning}}

In this chapter, we will focus on using some of the machine learning techniques to explore genomics data. The goals of data exploration are usually many. Generally, we want to understand how the variables in our data set relate to each other and how the samples defined by those variables relate to each other. These points of information can be used to generate a hypothesis, find outliers \index{outliers}in the samples or identify sample groups that need more data points. In this chapter, we will focus on two main classes of techniques: ``clustering'' and ``dimension reduction''. We will show how to use these techniques and how to visualize them using R. As these techniques are fundamental for data analysis, we will see more of their use cases in Chapters \ref{rnaseqanalysis}, \ref{chipseq}, \ref{bsseq} and \ref{multiomics}.

\hypertarget{clustering-grouping-samples-based-on-their-similarity}{%
\section{Clustering: Grouping samples based on their similarity}\label{clustering-grouping-samples-based-on-their-similarity}}

In genomics, we would very frequently want to assess how our samples relate to each other. Are our replicates similar to each other? Do the samples from the same treatment group have similar genome-wide signals? Do the patients with similar diseases have similar gene expression profiles?
Take the last question for example. We need to define a distance or similarity metric between patients' expression profiles and use that metric to find groups of patients that are more similar to each other than the rest of the patients. This, in essence, is the general idea behind clustering. We need a distance metric and a method to utilize that distance metric to find self-similar groups. Clustering is a ubiquitous procedure in bioinformatics as well as any field that deals with high-dimensional data. It is very likely that every genomics paper containing multiple samples has some sort of clustering. Due to this ubiquity and general usefulness, it is an essential technique to learn.

\hypertarget{distance-metrics}{%
\subsection{Distance metrics}\label{distance-metrics}}

The first required step for clustering is the distance metric. This is simply a measurement of how similar gene expressions are to each other. There are many options for distance metrics and the choice of the metric is quite important for clustering. Consider a simple example where we have four patients and expression of three genes measured in Table \ref{tab:expTable}. Which patients look similar to each other based on their gene expression profiles \index{gene expression}?

\begin{table}

\caption{\label{tab:expTable}Gene expressions from patients}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
  & IRX4 & OCT4 & PAX6\\
\hline
patient1 & 11 & 10 & 1\\
\hline
patient2 & 13 & 13 & 3\\
\hline
patient3 & 2 & 4 & 10\\
\hline
patient4 & 1 & 3 & 9\\
\hline
\end{tabular}
\end{table}

It may not be obvious from the table at first sight, but if we plot the gene expression profile for each patient (shown in Figure \ref{fig:expPlot}), we will see that expression profiles of patient 1 and patient 2 are more similar to each other than patient 3 or patient 4.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{04-unsupervisedLearning_files/figure-latex/expPlot-1} 

}

\caption{Gene expression values for different patients. Certain patients have gene expression values that are similar to each other.}\label{fig:expPlot}
\end{figure}

But how can we quantify what we see? A simple metric for distance between gene expression vectors between a given patient pair is the sum of the absolute difference between gene expression values. This can be formulated as follows: \(d_{AB}={\sum _{i=1}^{n}|e_{Ai}-e_{Bi}|}\), where \(d_{AB}\) is the distance between patients A and B, and the \(e_{Ai}\) and \(e_{Bi}\) are expression values of the \(i\)th gene for patients A and B. This distance metric is called the \textbf{``Manhattan distance''} or \textbf{``L1 norm''}. \index{Manhattan distance}
\index{L1 norm}

Another distance metric uses the sum of squared distances and takes the square root of resulting value; this metric can be formulated as: \(d_{AB}={{\sqrt {\sum _{i=1}^{n}(e_{Ai}-e_{Bi})^{2}}}}\). This distance is called \textbf{``Euclidean Distance''} or \textbf{``L2 norm''}. This is usually the default distance metric for many clustering algorithms. Due to the squaring operation, values that are very different get higher contribution to the distance. Due to this, compared to the Manhattan distance, it can be affected more by outliers\index{outliers}. But, generally if the outliers are rare, this distance metric works well.

The last metric we will introduce is the \textbf{``correlation distance''}. This is simply \(d_{AB}=1-\rho\), where \(\rho\) is the Pearson correlation coefficient between two vectors; in our case those vectors are gene expression profiles of patients. Using this distance the gene expression vectors that have a similar pattern will have a small distance, whereas when the vectors have different patterns they will have a large distance. In this case, the linear correlation between vectors matters, although the scale of the vectors might be different.\index{correlation distance}

Now let's see how we can calculate these distances in R. First, we have our gene expression per patient table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          IRX4 OCT4 PAX6
## patient1   11   10    1
## patient2   13   13    3
## patient3    2    4   10
## patient4    1    3    9
\end{verbatim}

Next, we calculate the distance metrics using the \texttt{dist()} function and \texttt{1-cor()} expression.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dist}\NormalTok{(df,}\DataTypeTok{method=}\StringTok{"manhattan"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          patient1 patient2 patient3
## patient2        7                  
## patient3       24       27         
## patient4       25       28        3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dist}\NormalTok{(df,}\DataTypeTok{method=}\StringTok{"euclidean"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           patient1  patient2  patient3
## patient2  4.123106                    
## patient3 14.071247 15.842980          
## patient4 14.594520 16.733201  1.732051
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.dist}\NormalTok{(}\DecValTok{1}\OperatorTok{-}\KeywordTok{cor}\NormalTok{(}\KeywordTok{t}\NormalTok{(df))) }\CommentTok{# correlation distance}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             patient1    patient2    patient3
## patient2 0.004129405                        
## patient3 1.988522468 1.970725343            
## patient4 1.988522468 1.970725343 0.000000000
\end{verbatim}

\hypertarget{scaling-before-calculating-the-distance}{%
\subsubsection{Scaling before calculating the distance}\label{scaling-before-calculating-the-distance}}

Before we proceed to the clustering, there is one more thing we need to take care of. Should we normalize our data? The scale of the vectors in our expression matrix can affect the distance calculation. Gene expression tables might have some sort of normalization, so the values are in comparable scales. But somehow, if a gene's expression values are on a much higher scale than the other genes, that gene will affect the distance more than others when using Euclidean or Manhattan distance. If that is the case we can scale the variables. The traditional way of scaling variables is to subtract their mean, and divide by their standard deviation, this operation is also called ``standardization''. If this is done on all genes, each gene will have the same effect on distance measures. The decision to apply scaling ultimately depends on our data and what you want to achieve. If the gene expression values are previously normalized between patients, having genes that dominate the distance metric could have a biological meaning and therefore it may not be desirable to further scale variables. In R, the standardization is done via the \texttt{scale()} function. Here we scale the gene expression values.\index{scaling}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          IRX4 OCT4 PAX6
## patient1   11   10    1
## patient2   13   13    3
## patient3    2    4   10
## patient4    1    3    9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{scale}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                IRX4       OCT4       PAX6
## patient1  0.6932522  0.5212860 -1.0733721
## patient2  1.0194886  1.1468293 -0.6214260
## patient3 -0.7748113 -0.7298004  0.9603856
## patient4 -0.9379295 -0.9383149  0.7344125
## attr(,"scaled:center")
## IRX4 OCT4 PAX6 
## 6.75 7.50 5.75 
## attr(,"scaled:scale")
##     IRX4     OCT4     PAX6 
## 6.130525 4.795832 4.425306
\end{verbatim}

\hypertarget{hiearchical-clustering}{%
\subsection{Hiearchical clustering}\label{hiearchical-clustering}}

This is one of the most ubiquitous clustering algorithms. Using this algorithm you can see the relationship of individual data points and relationships of clusters. This is achieved by successively joining small clusters to each other based on the inter-cluster distance. Eventually, you get a tree structure or a dendrogram that shows the relationship between the individual data points and clusters. The height of the dendrogram is the distance between clusters. Here we can show how to use this on our toy data set from four patients. The base function in R to do hierarchical clustering in \texttt{hclust()}. Below, we apply that function on Euclidean distances between patients. The resulting clustering tree or dendrogram is shown in Figure \ref{fig:expPlot}.\index{clustering!hierarchical clustering}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d=}\KeywordTok{dist}\NormalTok{(df)}
\NormalTok{hc=}\KeywordTok{hclust}\NormalTok{(d,}\DataTypeTok{method=}\StringTok{"complete"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(hc)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{04-unsupervisedLearning_files/figure-latex/toyClust-1} 

}

\caption{Dendrogram of distance matrix}\label{fig:toyClust}
\end{figure}

In the above code snippet, we have used the \texttt{method="complete"} argument without explaining it. The \texttt{method} argument defines the criteria that directs how the sub-clusters are merged. During clustering, starting with single-member clusters, the clusters are merged based on the distance between them. There are many different ways to define distance between clusters, and based on which definition you use, the hierarchical clustering results change. So the \texttt{method} argument controls that. There are a couple of values this argument can take; we list them and their description below:

\begin{itemize}
\tightlist
\item
  \textbf{``complete''} stands for ``Complete Linkage'' and the distance between two clusters is defined as the largest distance between any members of the two clusters.
\item
  \textbf{``single''} stands for ``Single Linkage'' and the distance between two clusters is defined as the smallest distance between any members of the two clusters.
\item
  \textbf{``average''} stands for ``Average Linkage'' or more precisely the UPGMA (Unweighted Pair Group Method with Arithmetic Mean) method. In this case, the distance between two clusters is defined as the average distance between any members of the two clusters.
\item
  \textbf{``ward.D2''} and \textbf{``ward.D''} stands for different implementations of Ward's minimum variance method. This method aims to find compact, spherical clusters by selecting clusters to merge based on the change in the cluster variances. The clusters are merged if the increase in the combined variance over the sum of the cluster-specific variances is the minimum compared to alternative merging operations.
\end{itemize}

In real life, we would get expression profiles from thousands of genes and we will typically have many more patients than our toy example. One such data set is gene expression values from 60 bone marrow samples of patients with one of the four main types of leukemia (ALL, AML, CLL, CML) or no-leukemia controls. We trimmed that data set down to the top 1000 most variable genes to be able to work with it more easily, since genes that are not very variable do not contribute much to the distances between patients. We will now use this data set to cluster the patients and display the values as a heatmap and a dendrogram. The heatmap shows the expression values of genes across patients in a color coded manner. The heatmap function, \texttt{pheatmap()}, that we will use performs the clustering as well. The matrix that contains gene expressions has the genes in the rows and the patients in the columns. Therefore, we will also use a column-side color code to mark the patients based on their leukemia type. For the hierarchical clustering, we will use Ward's method designated by the \texttt{clustering\_method} argument to the \texttt{pheatmap()} function. The resulting heatmap is shown in Figure \ref{fig:heatmap1}. \index{heatmap}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(pheatmap)}
\NormalTok{expFile=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}\StringTok{"leukemiaExpressionSubset.rds"}\NormalTok{,}
                    \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{mat=}\KeywordTok{readRDS}\NormalTok{(expFile)}

\CommentTok{# set the leukemia type annotation for each sample}
\NormalTok{annotation_col =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
                    \DataTypeTok{LeukemiaType =}\KeywordTok{substr}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(mat),}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\KeywordTok{rownames}\NormalTok{(annotation_col)=}\KeywordTok{colnames}\NormalTok{(mat)}
  

\KeywordTok{pheatmap}\NormalTok{(mat,}\DataTypeTok{show_rownames=}\OtherTok{FALSE}\NormalTok{,}\DataTypeTok{show_colnames=}\OtherTok{FALSE}\NormalTok{,}
         \DataTypeTok{annotation_col=}\NormalTok{annotation_col,}
         \DataTypeTok{scale =} \StringTok{"none"}\NormalTok{,}\DataTypeTok{clustering_method=}\StringTok{"ward.D2"}\NormalTok{,}
         \DataTypeTok{clustering_distance_cols=}\StringTok{"euclidean"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{04-unsupervisedLearning_files/figure-latex/heatmap1-1} 

}

\caption{Heatmap of gene expression values from leukemia patients. Each column represents a patient. Columns are clustered using gene expression and color coded by disease type: ALL, AML, CLL, CML or no-leukemia }\label{fig:heatmap1}
\end{figure}

As we can observe in the heatmap, each cluster has a distinct set of expression values. The main clusters almost perfectly distinguish the leukemia types. Only one CML patient is clustered as a non-leukemia sample. This could mean that gene expression profiles are enough to classify leukemia type. More detailed analysis and experiments are needed to verify that, but by looking at this exploratory analysis we can decide where to focus our efforts next.

\hypertarget{where-to-cut-the-tree}{%
\subsubsection{Where to cut the tree ?}\label{where-to-cut-the-tree}}

The example above seems like a clear-cut example where we can pick clusters from the dendrogram by eye. This is mostly due to Ward's method, where compact clusters are preferred. However, as is usually the case, we do not have patient labels and it would be difficult to tell which leaves (patients) in the dendrogram we should consider as part of the same cluster. In other words, how deep we should cut the dendrogram so that every patient sample still connected via the remaining sub-dendrograms constitute clusters. The \texttt{cutree()} function provides the functionality to output either desired number of clusters or clusters obtained from cutting the dendrogram at a certain height. Below, we will cluster the patients with hierarchical clustering using the default method ``complete linkage'' and cut the dendrogram at a certain height. In this case, you will also observe that, changing from Ward's distance to complete linkage had an effect on clustering. Now the two clusters that are defined by Ward's distance are closer to each other and harder to separate from each other, shown in Figure \ref{fig:hclustNcut}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hcl=}\KeywordTok{hclust}\NormalTok{(}\KeywordTok{dist}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat)))}
\KeywordTok{plot}\NormalTok{(hcl,}\DataTypeTok{labels =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{hang=} \DecValTok{-1}\NormalTok{)}
\KeywordTok{rect.hclust}\NormalTok{(hcl, }\DataTypeTok{h =} \DecValTok{80}\NormalTok{, }\DataTypeTok{border =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{04-unsupervisedLearning_files/figure-latex/hclustNcut-1} 

}

\caption{Dendrogram of Leukemia patients clustered by hierarchical clustering. Rectangles show the cluster we will get if we cut the tree at `height=80`.}\label{fig:hclustNcut}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{clu.k5=}\KeywordTok{cutree}\NormalTok{(hcl,}\DataTypeTok{k=}\DecValTok{5}\NormalTok{) }\CommentTok{# cut tree so that there are 5 clusters}

\NormalTok{clu.h80=}\KeywordTok{cutree}\NormalTok{(hcl,}\DataTypeTok{h=}\DecValTok{80}\NormalTok{) }\CommentTok{# cut tree/dendrogram from height 80}
\KeywordTok{table}\NormalTok{(clu.k5) }\CommentTok{# number of samples for each cluster}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## clu.k5
##  1  2  3  4  5 
## 12  3  9 12 24
\end{verbatim}

Apart from the arbitrary values for the height or the number of clusters, how can we define clusters more systematically? As this is a general question, we will show how to decide the optimal number of clusters later in this chapter.

\hypertarget{k-means-clustering}{%
\subsection{K-means clustering}\label{k-means-clustering}}

Another very common clustering algorithm is k-means. This method divides or partitions the data points, our working example patients, into a pre-determined, ``k'' number of clusters \index{clustering!k-means} \citep{hartigan1979algorithm}. Hence, these types of methods are generally called ``partitioning'' methods. The algorithm is initialized with randomly chosen \(k\) centers or centroids. In a sense, a centroid is a data point with multiple values. In our working example, it is a hypothetical patient with gene expression values. But in the initialization phase, those gene expression values are chosen randomly within the boundaries of the gene expression distributions from real patients. As the next step in the algorithm, each patient is assigned to the closest centroid, and in the next iteration, centroids are set to the mean of values of the genes in the cluster. This process of setting centroids and assigning patients to the clusters repeats itself until the sum of squared distances to cluster centroids is minimized.

As you might see, the cluster algorithm starts with random initial centroids. This feature might yield different results for each run of the algorithm. We will now show how to use the k-means method on the gene expression data set. We will use \texttt{set.seed()} for reproducibility. In the wild, you might want to run this algorithm multiple times to see if your clustering results are stable.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}

\CommentTok{# we have to transpore the matrix t()}
\CommentTok{# so that we calculate distances between patients}
\NormalTok{kclu=}\KeywordTok{kmeans}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat),}\DataTypeTok{centers=}\DecValTok{5}\NormalTok{)  }

\CommentTok{# number of data points in each cluster}
\KeywordTok{table}\NormalTok{(kclu}\OperatorTok{$}\NormalTok{cluster)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  1  2  3  4  5 
## 12 14 11 12 11
\end{verbatim}

Now let us check the percentage of each leukemia type in each cluster. We can visualize this as a table. Looking at the table below, we see that each of the 5 clusters predominantly represents one of the 4 leukemia types or the control patients without leukemia.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{type2kclu =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
                    \DataTypeTok{LeukemiaType =}\KeywordTok{substr}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(mat),}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{),}
                    \DataTypeTok{cluster=}\NormalTok{kclu}\OperatorTok{$}\NormalTok{cluster)}

\KeywordTok{table}\NormalTok{(type2kclu)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             cluster
## LeukemiaType  1  2  3  4  5
##          ALL 12  0  0  0  0
##          AML  0  1  0  0 11
##          CLL  0  0  0 12  0
##          CML  0  1 11  0  0
##          NoL  0 12  0  0  0
\end{verbatim}

Another related and maybe more robust algorithm is called \textbf{``k-medoids''} clustering \citep{reynolds2006clustering}. The procedure is almost identical to k-means clustering with a couple of differences. \index{clustering!k-medoids} In this case, centroids chosen are real data points in our case patients, and the metric we are trying to optimize in each iteration is based on the Manhattan distance to the centroid. In k-means this was based on the sum of squared distances, so Euclidean distance. Below we show how to use the k-medoids clustering function \texttt{pam()} \index{clustering!pam} from the \texttt{cluster} package.\index{R Packages!\texttt{cluster}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kmclu=cluster}\OperatorTok{::}\KeywordTok{pam}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat),}\DataTypeTok{k=}\DecValTok{5}\NormalTok{) }\CommentTok{#  cluster using k-medoids}

\CommentTok{# make a data frame with Leukemia type and cluster id}
\NormalTok{type2kmclu =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
                    \DataTypeTok{LeukemiaType =}\KeywordTok{substr}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(mat),}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{),}
                    \DataTypeTok{cluster=}\NormalTok{kmclu}\OperatorTok{$}\NormalTok{cluster)}

\KeywordTok{table}\NormalTok{(type2kmclu)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             cluster
## LeukemiaType  1  2  3  4  5
##          ALL 12  0  0  0  0
##          AML  0 10  1  1  0
##          CLL  0  0  0  0 12
##          CML  0  0  0 12  0
##          NoL  0  0 12  0  0
\end{verbatim}

We cannot visualize the clustering from partitioning methods with a tree like we did for hierarchical clustering. Even if we can get the distances between patients the algorithm does not return the distances between clusters out of the box. However, if we had a way to visualize the distances between patients in 2 dimensions we could see the how patients and clusters relate to each other. It turns out that there is a way to compress between patient distances to a 2-dimensional plot. There are many ways to do this, and we introduce these dimension-reduction methods including the one we will use later in this chapter. For now, we are going to use a method called ``multi-dimensional scaling'' and plot the patients in a 2D plot color coded by their cluster assignments shown in Figure \ref{fig:kmeansmds}. We will explain this method in more detail in the \protect\hyperlink{multi-dimensional-scaling}{Multi-dimensional scaling} section below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Calculate distances}
\NormalTok{dists=}\KeywordTok{dist}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat))}

\CommentTok{# calculate MDS}
\NormalTok{mds=}\KeywordTok{cmdscale}\NormalTok{(dists)}

\CommentTok{# plot the patients in the 2D space}
\KeywordTok{plot}\NormalTok{(mds,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{col=}\KeywordTok{rainbow}\NormalTok{(}\DecValTok{5}\NormalTok{)[kclu}\OperatorTok{$}\NormalTok{cluster])}

\CommentTok{# set the legend for cluster colors}
\KeywordTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{,}
       \DataTypeTok{legend=}\KeywordTok{paste}\NormalTok{(}\StringTok{"clu"}\NormalTok{,}\KeywordTok{unique}\NormalTok{(kclu}\OperatorTok{$}\NormalTok{cluster)),}
       \DataTypeTok{fill=}\KeywordTok{rainbow}\NormalTok{(}\DecValTok{5}\NormalTok{)[}\KeywordTok{unique}\NormalTok{(kclu}\OperatorTok{$}\NormalTok{cluster)],}
       \DataTypeTok{border=}\OtherTok{NA}\NormalTok{,}\DataTypeTok{box.col=}\OtherTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{04-unsupervisedLearning_files/figure-latex/kmeansmds-1} 

}

\caption{K-means cluster memberships are shown in a multi-dimensional scaling plot}\label{fig:kmeansmds}
\end{figure}

The plot we obtained shows the separation between clusters. However, it does not do a great job showing the separation between clusters 3 and 4, which represent CML and ``no leukemia'' patients. We might need another dimension to properly visualize that separation. In addition, those two clusters were closely related in the hierarchical clustering as well.

\hypertarget{how-to-choose-k-the-number-of-clusters}{%
\subsection{How to choose ``k'', the number of clusters}\label{how-to-choose-k-the-number-of-clusters}}

Up to this point, we have avoided the question of selecting optimal number clusters. How do we know where to cut our dendrogram or which k to choose ?
First of all, this is a difficult question. Usually, clusters have different granularity. Some clusters are tight and compact and some are wide, and both these types of clusters can be in the same data set. When visualized, some large clusters may look like they may have sub-clusters. So should we consider the large cluster as one cluster or should we consider the sub-clusters as individual clusters? There are some metrics to help but there is no definite answer. We will show a couple of them below.

\hypertarget{silhouette}{%
\subsubsection{Silhouette}\label{silhouette}}

One way to determine the quality of the clustering is to measure the expected self-similar nature of the points in a set of clusters. The silhouette value does just that and it is a measure of how similar a data point is to its own cluster compared to other clusters \citep{rousseeuw1987silhouettes}. The silhouette value ranges from -1 to +1, where values that are positive indicate that the data point is well matched to its own cluster, if the value is zero it is a borderline case, and if the value is minus it means that the data point might be mis-clustered because it is more similar to a neighboring cluster. If most data points have a high value, then the clustering is appropriate. Ideally, one can create many different clusterings with each with a different \(k\) parameter indicating the number of clusters, and assess their appropriateness using the average
silhouette values. In R, silhouette values are referred to as silhouette widths in the documentation.\index{silhouette}

A silhouette value is calculated for each data point. In our working example, each patient will get silhouette values showing how well they are matched to their assigned clusters. Formally this calculated as follows. For each data point \(i\), we calculate \({\displaystyle a(i)}\), which denotes the average distance between \(i\) and all other data points within the same cluster. This shows how well the point fits into that cluster. For the same data point, we also calculate \({\displaystyle b(i)}\), which denotes the lowest average distance of \({\displaystyle i}\) to all points in any other cluster, of which \({\displaystyle i}\) is not a member. The cluster with this lowest average \(b(i)\) is the ``neighboring cluster'' of data point \({\displaystyle i}\) since it is the next best fit cluster for that data point. Then, the silhouette value for a given data point is \(s(i) = \frac{b(i) - a(i)}{\max\{a(i),b(i)\}}\).

As described, this quantity is positive when \(b(i)\) is high and \(a(i)\) is low, meaning that the data point \(i\) is self-similar to its cluster. And the silhouette value, \(s(i)\), is negative if it is more similar to its neighbors than its assigned cluster.

In R, we can calculate silhouette values using the \texttt{cluster::silhouette()} function. Below, we calculate the silhouette values for k-medoids clustering with the \texttt{pam()} function with \texttt{k=5}. The resulting silhouette values are shown in Figure \ref{fig:sill}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(cluster)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}
\NormalTok{pamclu=cluster}\OperatorTok{::}\KeywordTok{pam}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat),}\DataTypeTok{k=}\DecValTok{5}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{silhouette}\NormalTok{(pamclu),}\DataTypeTok{main=}\OtherTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{04-unsupervisedLearning_files/figure-latex/sill-1} 

}

\caption{Silhouette values for k-medoids with `k=5`}\label{fig:sill}
\end{figure}

Now, let us calculate the average silhouette value for different \(k\) values and compare. We will use \texttt{sapply()} function to get average silhouette values across \(k\) values between 2 and 7. Within \texttt{sapply()} there is an anonymous function that that does the clustering and calculates average silhouette values for each \(k\). The plot showing average silhouette values for different \(k\) values is shown in Figure \ref{fig:sillav}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Ks=}\KeywordTok{sapply}\NormalTok{(}\DecValTok{2}\OperatorTok{:}\DecValTok{7}\NormalTok{,}
    \ControlFlowTok{function}\NormalTok{(i) }
      \KeywordTok{summary}\NormalTok{(}\KeywordTok{silhouette}\NormalTok{(}\KeywordTok{pam}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat),}\DataTypeTok{k=}\NormalTok{i)))}\OperatorTok{$}\NormalTok{avg.width)}
\KeywordTok{plot}\NormalTok{(}\DecValTok{2}\OperatorTok{:}\DecValTok{7}\NormalTok{,Ks,}\DataTypeTok{xlab=}\StringTok{"k"}\NormalTok{,}\DataTypeTok{ylab=}\StringTok{"av. silhouette"}\NormalTok{,}\DataTypeTok{type=}\StringTok{"b"}\NormalTok{,}
     \DataTypeTok{pch=}\DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{04-unsupervisedLearning_files/figure-latex/sillav-1} 

}

\caption{Average silhouette values for k-medoids clustering for `k` values between 2 and 7}\label{fig:sillav}
\end{figure}

In this case, it seems the best value for \(k\) is 4. The k-medoids function \texttt{pam()} will usually cluster CML and ``no Leukemia'' cases together when \texttt{k=4}, which are also related clusters according to the hierarchical clustering we did earlier.

\hypertarget{gap-statistic}{%
\subsubsection{Gap statistic}\label{gap-statistic}}

As clustering aims to find self-similar data points, it would be reasonable to expect with the correct number of clusters the total within-cluster variation is minimized. Within-cluster variation for a single cluster can simply be defined as the sum of squares from the cluster mean, which in this case is the centroid we defined in the k-means algorithm. The total within-cluster variation is then the sum of within-cluster variations for each cluster. This can be formally defined as follows:\index{gap statistic}

\(\displaystyle W_k = \sum_{k=1}^K \sum_{\mathrm{x}_i \in C_k} (\mathrm{x}_i - \mu_k )^2\)

where \(\mathrm{x}_i\) is a data point in cluster \(k\), and \(\mu_k\) is the cluster mean, and \(W_k\) is the total within-cluster variation quantity we described. However, the problem is that the variation quantity decreases with the number of clusters. The more centroids we have, the smaller the distances to the centroids become. A more reliable approach would be somehow calculating the expected variation from a reference null distribution and compare that to the observed variation for each \(k\). In the gap statistic approach, the expected distribution is calculated via sampling points from the boundaries of the original data and calculating within-cluster variation quantity for multiple rounds of sampling \citep{tibshirani2001estimating}. This way we have an expectation about the variability when there is no clustering, and then compare that expected variation to the observed within-cluster variation. The expected variation should also go down with the increasing number of clusters, but for the optimal number of clusters, the expected variation will be furthest away from observed variation. This distance is called the \textbf{``gap statistic''} and defined as follows:
\(\displaystyle \mathrm{Gap}_n(k) = E_n^*\{\log W_k\} - \log W_k\), where \(E_n^*\{\log W_k\}\) is the expected variation in log-scale under a sample size \(n\) from the reference distribution and \(\log W_k\) is the observed variation. Our aim is to choose the \(k\) number of clusters that maximizes \(\mathrm{Gap}_n(k)\).

We can easily calculate the gap statistic with the \texttt{cluster::clusGap()} function. We will now use that function to calculate the gap statistic for our patient gene expression data. The resulting gap statistics are shown in Figure \ref{fig:clusGap}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(cluster)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}
\CommentTok{# define the clustering function}
\NormalTok{pam1 <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x,k) }
  \KeywordTok{list}\NormalTok{(}\DataTypeTok{cluster =} \KeywordTok{pam}\NormalTok{(x,k, }\DataTypeTok{cluster.only=}\OtherTok{TRUE}\NormalTok{))}

\CommentTok{# calculate the gap statistic}
\NormalTok{pam.gap=}\StringTok{ }\KeywordTok{clusGap}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat), }\DataTypeTok{FUN =}\NormalTok{ pam1, }\DataTypeTok{K.max =} \DecValTok{8}\NormalTok{,}\DataTypeTok{B=}\DecValTok{50}\NormalTok{)}

\CommentTok{# plot the gap statistic accross k values}
\KeywordTok{plot}\NormalTok{(pam.gap, }\DataTypeTok{main =} \StringTok{"Gap statistic for the 'Leukemia' data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{04-unsupervisedLearning_files/figure-latex/clusGap-1} 

}

\caption{Gap statistic for clustering the leukemia dataset with k-medoids (pam) algorithm.}\label{fig:clusGap}
\end{figure}

In this case, the gap statistic shows that \(k=7\) is the best if we take the maximum value as the best. However, after \(k=6\), the statistic has more or less a stable curve. This observation is incorporated into algorithms that can select the best \(k\) value based on the gap statistic. A reasonable way is to take the simulation error (error bars in \ref{fig:clusGap}) into account, and take the smallest \(k\) whose gap statistic is larger or equal to the one of \(k+1\) minus the simulation error. Formally written, we would pick the smallest \(k\) satisfying the following condition: \(\mathrm{Gap}(k) \geq \mathrm{Gap}(k+1) - s_{k+1}\), where \(s_{k+1}\) is the simulation error for \(\mathrm{Gap}(k+1)\).

Using this procedure gives us \(k=6\) as the optimum number of clusters. Biologically, we know that there are 5 main patient categories but this does not mean there are no sub-categories or sub-types for the cancers we are looking at.

\hypertarget{other-methods}{%
\subsubsection{Other methods}\label{other-methods}}

There are several other methods that provide insight into how many clusters. In fact, the package \texttt{NbClust} provides 30 different ways to determine the number of optimal clusters and can offer a voting mechanism to pick the best number. Below, we show how to use this function for some of the optimal number of cluster detection methods.\index{R Packages!\texttt{NbClust}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(NbClust)}
\NormalTok{nb =}\StringTok{ }\KeywordTok{NbClust}\NormalTok{(}\DataTypeTok{data=}\KeywordTok{t}\NormalTok{(mat), }
             \DataTypeTok{distance =} \StringTok{"euclidean"}\NormalTok{, }\DataTypeTok{min.nc =} \DecValTok{2}\NormalTok{,}
        \DataTypeTok{max.nc =} \DecValTok{7}\NormalTok{, }\DataTypeTok{method =} \StringTok{"kmeans"}\NormalTok{,}
        \DataTypeTok{index=}\KeywordTok{c}\NormalTok{(}\StringTok{"kl"}\NormalTok{,}\StringTok{"ch"}\NormalTok{,}\StringTok{"cindex"}\NormalTok{,}\StringTok{"db"}\NormalTok{,}\StringTok{"silhouette"}\NormalTok{,}
                \StringTok{"duda"}\NormalTok{,}\StringTok{"pseudot2"}\NormalTok{,}\StringTok{"beale"}\NormalTok{,}\StringTok{"ratkowsky"}\NormalTok{,}
                \StringTok{"gap"}\NormalTok{,}\StringTok{"gamma"}\NormalTok{,}\StringTok{"mcclain"}\NormalTok{,}\StringTok{"gplus"}\NormalTok{,}
                \StringTok{"tau"}\NormalTok{,}\StringTok{"sdindex"}\NormalTok{,}\StringTok{"sdbw"}\NormalTok{))}

\KeywordTok{table}\NormalTok{(nb}\OperatorTok{$}\NormalTok{Best.nc[}\DecValTok{1}\NormalTok{,]) }\CommentTok{# consensus seems to be 3 clusters }
\end{Highlighting}
\end{Shaded}

However, readers should keep in mind that clustering is an exploratory technique. If you have solid labels for your data points, maybe clustering is just a sanity check, and you should just do predictive modeling instead. However, in biology there are rarely solid labels and things have different granularity. Take the leukemia patients case we have been using for example, it is known that leukemia types have subtypes and those sub-types that have different mutation profiles and consequently have different molecular signatures. Because of this, it is not surprising that some optimal cluster number techniques will find more clusters to be appropriate. On the other hand, CML (chronic myeloid leukemia) is a slow progressing disease and maybe their molecular signatures are closer to ``no leukemia'' patients, so clustering algorithms may confuse the two depending on what granularity they are operating with. It is always good to look at the heatmaps after clustering, if you have meaningful self-similar data points, even if the labels you have do not agree that there can be different clusters, you can perform downstream analysis to understand the sub-clusters better. As we have seen, we can estimate the optimal number of clusters but we cannot take that estimation as the absolute truth. Given more data points or a different set of expression signatures, you may have different optimal clusterings, or the supposed optimal clustering might overlook previously known sub-groups of your data.

\hypertarget{dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d}{%
\section{Dimensionality reduction techniques: Visualizing complex data sets in 2D}\label{dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d}}

In statistics, dimension reduction techniques are a set of processes for reducing the number of random variables by obtaining a set of principal variables. For example, in the context of a gene expression matrix across different patient samples, this might mean getting a set of new variables that cover the variation in sets of genes. This way samples can be represented by a couple of principal variables instead of thousands of genes. This is useful for visualization, clustering and predictive modeling.\index{dimensionality reduction}

\hypertarget{principal-component-analysis}{%
\subsection{Principal component analysis}\label{principal-component-analysis}}

Principal component analysis (PCA)\index{principal component analysis (PCA)} is maybe the most popular technique to examine high-dimensional data. There are multiple interpretations of how PCA reduces dimensionality. We will first focus on geometrical interpretation, where this operation can be interpreted as rotating the original dimensions of the data. For this, we go back to our example gene expression data set. In this example, we will represent our patients with expression profiles of just two genes, CD33 (ENSG00000105383) and PYGL (ENSG00000100504). This way we can visualize them in a scatter plot (see Figure \ref{fig:scatterb4PCA}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(mat[}\KeywordTok{rownames}\NormalTok{(mat)}\OperatorTok{==}\StringTok{"ENSG00000100504"}\NormalTok{,],}
\NormalTok{     mat[}\KeywordTok{rownames}\NormalTok{(mat)}\OperatorTok{==}\StringTok{"ENSG00000105383"}\NormalTok{,],}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}
     \DataTypeTok{ylab=}\StringTok{"CD33 (ENSG00000105383)"}\NormalTok{,}
     \DataTypeTok{xlab=}\StringTok{"PYGL (ENSG00000100504)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{04-unsupervisedLearning_files/figure-latex/scatterb4PCA-1} 

}

\caption{Gene expression values of CD33 and PYGL genes across leukemia patients.}\label{fig:scatterb4PCA}
\end{figure}

PCA rotates the original data space such that the axes of the new coordinate system point to the directions of highest variance of the data. The axes or new variables are termed principal components (PCs) and are ordered by variance: The first component, PC 1, represents the direction of the highest variance of the data. The direction of the second component, PC 2, represents the highest of the remaining variance orthogonal to the first component. This can be naturally extended to obtain the required number of components, which together span a component space covering the desired amount of variance. In our toy example with only two genes, the principal components are drawn over the original scatter plot and in the next plot we show the new coordinate system based on the principal components. We will calculate the PCA with the \texttt{princomp()} function; this function returns the new coordinates as well. These new coordinates are simply a projection of data over the new coordinates. We will decorate the scatter plots with eigenvectors showing the direction of greatest variation. Then, we will plot the new coordinates (the resulting plot is shown in Figure \ref{fig:pcaRot}). These are automatically calculated by the \texttt{princomp()} function. Notice that we are using the \texttt{scale()} function when plotting coordinates and also before calculating the PCA. This function centers the data, meaning it subtracts the mean of each column vector from the elements in the vector. This essentially gives the columns a zero mean. It also divides the data by the standard deviation of the centered columns. These two operations help bring the data to a common scale, which is important for PCA not to be affected by different scales in the data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}

\CommentTok{# create the subset of the data with two genes only}
\CommentTok{# notice that we transpose the matrix so samples are }
\CommentTok{# on the columns}
\NormalTok{sub.mat=}\KeywordTok{t}\NormalTok{(mat[}\KeywordTok{rownames}\NormalTok{(mat) }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"ENSG00000100504"}\NormalTok{,}\StringTok{"ENSG00000105383"}\NormalTok{),])}

\CommentTok{# ploting our genes of interest as scatter plots}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{scale}\NormalTok{(mat[}\KeywordTok{rownames}\NormalTok{(mat)}\OperatorTok{==}\StringTok{"ENSG00000100504"}\NormalTok{,]),}
     \KeywordTok{scale}\NormalTok{(mat[}\KeywordTok{rownames}\NormalTok{(mat)}\OperatorTok{==}\StringTok{"ENSG00000105383"}\NormalTok{,]),}
     \DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}
     \DataTypeTok{ylab=}\StringTok{"CD33 (ENSG00000105383)"}\NormalTok{,}
     \DataTypeTok{xlab=}\StringTok{"PYGL (ENSG00000100504)"}\NormalTok{,}
     \DataTypeTok{col=}\KeywordTok{as.factor}\NormalTok{(annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType),}
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{),}\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}

\CommentTok{# create the legend for the Leukemia types}
\KeywordTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{,}
       \DataTypeTok{legend=}\KeywordTok{unique}\NormalTok{(annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType),}
       \DataTypeTok{fill =}\KeywordTok{palette}\NormalTok{(}\StringTok{"default"}\NormalTok{),}
       \DataTypeTok{border=}\OtherTok{NA}\NormalTok{,}\DataTypeTok{box.col=}\OtherTok{NA}\NormalTok{)}

\CommentTok{# calculate the PCA only for our genes and all the samples}
\NormalTok{pr=}\KeywordTok{princomp}\NormalTok{(}\KeywordTok{scale}\NormalTok{(sub.mat))}


\CommentTok{# plot the direction of eigenvectors}
\CommentTok{# pr$loadings returned by princomp has the eigenvectors}
\KeywordTok{arrows}\NormalTok{(}\DataTypeTok{x0=}\DecValTok{0}\NormalTok{, }\DataTypeTok{y0=}\DecValTok{0}\NormalTok{, }\DataTypeTok{x1 =}\NormalTok{ pr}\OperatorTok{$}\NormalTok{loadings[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{], }
         \DataTypeTok{y1 =}\NormalTok{ pr}\OperatorTok{$}\NormalTok{loadings[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{],}\DataTypeTok{col=}\StringTok{"pink"}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\KeywordTok{arrows}\NormalTok{(}\DataTypeTok{x0=}\DecValTok{0}\NormalTok{, }\DataTypeTok{y0=}\DecValTok{0}\NormalTok{, }\DataTypeTok{x1 =}\NormalTok{ pr}\OperatorTok{$}\NormalTok{loadings[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }
         \DataTypeTok{y1 =}\NormalTok{ pr}\OperatorTok{$}\NormalTok{loadings[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{],}\DataTypeTok{col=}\StringTok{"gray"}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}


\CommentTok{# plot the samples in the new coordinate system}
\KeywordTok{plot}\NormalTok{(}\OperatorTok{-}\NormalTok{pr}\OperatorTok{$}\NormalTok{scores,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}
     \DataTypeTok{col=}\KeywordTok{as.factor}\NormalTok{(annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType),}
     \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{),}\DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{))}

\CommentTok{# plot the new coordinate basis vectors}
\KeywordTok{arrows}\NormalTok{(}\DataTypeTok{x0=}\DecValTok{0}\NormalTok{, }\DataTypeTok{y0=}\DecValTok{0}\NormalTok{, }\DataTypeTok{x1 =}\OperatorTok{-}\DecValTok{2}\NormalTok{, }
         \DataTypeTok{y1 =} \DecValTok{0}\NormalTok{,}\DataTypeTok{col=}\StringTok{"pink"}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\KeywordTok{arrows}\NormalTok{(}\DataTypeTok{x0=}\DecValTok{0}\NormalTok{, }\DataTypeTok{y0=}\DecValTok{0}\NormalTok{, }\DataTypeTok{x1 =} \DecValTok{0}\NormalTok{, }
         \DataTypeTok{y1 =} \DecValTok{-1}\NormalTok{,}\DataTypeTok{col=}\StringTok{"gray"}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{04-unsupervisedLearning_files/figure-latex/pcaRot-1} 

}

\caption{Geometric interpretation of PCA finding eigenvectors that point to the direction of highest variance. Eigenvectors can be used as a new coordinate system.}\label{fig:pcaRot}
\end{figure}

As you can see, the new coordinate system is useful by itself. The X-axis, which represents the first component, separates the data along the lymphoblastic and myeloid leukemias.\index{principal component analysis (PCA)}

PCA in this case, is obtained by calculating eigenvectors of the covariance matrix via an operation called eigen decomposition. The covariance matrix is obtained by covariance of pairwise variables of our expression matrix, which is simply \({ \operatorname{cov} (X,Y)={\frac {1}{n}}\sum _{i=1}^{n}(x_{i}-\mu_X)(y_{i}-\mu_Y)}\), where \(X\) and \(Y\) are expression values of genes in a sample in our example. This is a measure of how things vary together, if highly expressed genes in sample A are also highly expressed in sample B and lowly expressed in sample A are also lowly expressed in sample B, then sample A and B will have positive covariance. If the opposite is true, then they will have negative covariance. This quantity is related to correlation, and as we saw in the previous chapter, correlation is standardized covariance. Covariance of variables can be obtained with the \texttt{cov()} function, and eigen decomposition of such a matrix will produce a set of orthogonal vectors that span the directions of highest variation. In 2D, you can think of this operation as rotating two perpendicular lines together until they point to the directions where most of the variation in the data lies, similar to Figure \ref{fig:pcaRot}. An important intuition is that, after the rotation prescribed by eigenvectors is complete, the covariance between variables in this rotated dataset will be zero. There is a proper mathematical relationship between covariances of the rotated dataset and the original dataset. That's why operating on the covariance matrix is related to the rotation of the original dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cov.mat=}\KeywordTok{cov}\NormalTok{(sub.mat) }\CommentTok{# calculate covariance matrix}
\NormalTok{cov.mat}
\KeywordTok{eigen}\NormalTok{(cov.mat) }\CommentTok{# obtain eigen decomposition for eigen values and vectors}
\end{Highlighting}
\end{Shaded}

Eigenvectors and eigenvalues of the covariance matrix indicate the direction and the magnitude of variation of the data. In our visual example, the eigenvectors are so-called principal components. The eigenvector indicates the direction and the eigenvalues indicate the variation in that direction. Eigenvectors and values exist in pairs: every eigenvector has a corresponding eigenvalue and the eigenvectors are linearly independent from each other, which means they are orthogonal or uncorrelated as in our working example above. The eigenvectors are ranked by their corresponding eigenvalue, the higher the eigenvalue the more important the eigenvector is, because it explains more of the variation compared to the other eigenvectors. This feature of PCA makes the dimension reduction possible. We can sometimes display data sets that have many variables only in 2D or 3D because these top eigenvectors are sometimes enough to capture most of variation in the data. The \texttt{screeplot()} function takes the output of the \texttt{princomp()} or \texttt{prcomp()} functions as input and plots the variance explained by eigenvectors.

\hypertarget{singular-value-decomposition-and-principal-component-analysis}{%
\subsubsection{Singular value decomposition and principal component analysis}\label{singular-value-decomposition-and-principal-component-analysis}}

A more common way to calculate PCA is through something called singular value decomposition (SVD). \index{singular value decomposition (SVD)}This results in another interpretation of PCA, which is called ``latent factor'' or ``latent component'' interpretation. In a moment, it \index{principal component analysis (PCA)} will be clearer what we mean by ``latent factors''. SVD is a matrix factorization or decomposition algorithm that decomposes an input matrix,\(X\), to three matrices as follows: \(\displaystyle \mathrm{X} = USV^T\). In essence, many matrices can be decomposed as a product of multiple matrices and we will come to other techniques later in this chapter. Singular value decomposition is shown in Figure \ref{fig:SVDcartoon}. \(U\) is the matrix with eigenarrays on the columns and this has the same dimensions as the input matrix; you might see elsewhere the columns are called eigenassays. \(S\) is the matrix that contains the singular values on the diagonal. The singular values are also known as eigenvalues and their square is proportional to explained variation by each eigenvector. Finally, the matrix \(V^T\) contains the eigenvectors on its rows. Its interpretation is still the same. Geometrically, eigenvectors point to the direction of highest variance in the data. They are uncorrelated or geometrically orthogonal to each other. These interpretations are identical to the ones we made before. The slight difference is that the decomposition seems to output \(V^T\), which is just the transpose of the matrix \(V\). However, the SVD algorithms in R usually return the matrix \(V\). If you want the eigenvectors, you either simply use the columns of matrix \(V\) or rows of \(V^T\).

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/SVDcartoon} 

}

\caption{Singular value decomposition (SVD) explained in a diagram. }\label{fig:SVDcartoon}
\end{figure}

One thing that is new in Figure \ref{fig:SVDcartoon} is the concept of eigenarrays. The eigenarrays, sometimes called eigenassays, represent the sample space and can be used to plot the relationship between samples rather than genes. In this way, SVD offers additional information than the PCA using the covariance matrix. It offers us a way to summarize both genes and samples. As we can project the gene expression profiles over the top two eigengenes and get a 2D representation of genes, but with the SVD, we can also project the samples over the top two eigenarrays and get a representation of samples in 2D scatter plot. The eigenvector could represent independent expression programs across samples, such as cell-cycle, if we had time-based expression profiles. However, there is no guarantee that each eigenvector will be biologically meaningful. Similarly each eigenarray represents samples with specific expression characteristics. For example, the samples that have a particular pathway activated might be correlated to an eigenarray returned by SVD.

Previously, in order to map samples to the reduced 2D space we had to transpose the genes-by-samples matrix before using the \texttt{princomp()} function. We will now first use SVD on the genes-by-samples matrix to get eigenarrays and use that to plot samples on the reduced dimensions. We will project the columns in our original expression data on eigenarrays and use the first two dimensions in the scatter plot. If you look at the code you will see that for the projection we use \(U^T X\) operation, which is just \(S V^T\) if you follow the linear algebra. We will also perform the PCA this time with the \texttt{prcomp()} function on the transposed genes-by-samples matrix to get similar information, and plot the samples on the reduced coordinates.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{d=}\KeywordTok{svd}\NormalTok{(}\KeywordTok{scale}\NormalTok{(mat)) }\CommentTok{# apply SVD}
\NormalTok{assays=}\KeywordTok{t}\NormalTok{(d}\OperatorTok{$}\NormalTok{u) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{scale}\NormalTok{(mat) }\CommentTok{# projection on eigenassays}
\KeywordTok{plot}\NormalTok{(assays[}\DecValTok{1}\NormalTok{,],assays[}\DecValTok{2}\NormalTok{,],}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}
     \DataTypeTok{col=}\KeywordTok{as.factor}\NormalTok{(annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType))}
\CommentTok{#plot(d$v[,1],d$v[,2],pch=19,}
\CommentTok{#     col=annotation_col$LeukemiaType)}
\NormalTok{pr=}\KeywordTok{prcomp}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat),}\DataTypeTok{center=}\OtherTok{TRUE}\NormalTok{,}\DataTypeTok{scale=}\OtherTok{TRUE}\NormalTok{) }\CommentTok{# apply PCA on transposed matrix}

\CommentTok{# plot new coordinates from PCA, projections on eigenvectors}
\CommentTok{# since the matrix is transposed eigenvectors represent }
\KeywordTok{plot}\NormalTok{(pr}\OperatorTok{$}\NormalTok{x[,}\DecValTok{1}\NormalTok{],pr}\OperatorTok{$}\NormalTok{x[,}\DecValTok{2}\NormalTok{],}\DataTypeTok{col=}\KeywordTok{as.factor}\NormalTok{(annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.65\linewidth]{04-unsupervisedLearning_files/figure-latex/svd-1} 

}

\caption{SVD on the matrix and its transpose}\label{fig:svd}
\end{figure}

As you can see in Figure \ref{fig:svd}, the two approaches yield separation of samples, although they are slightly different. The difference comes from the centering and scaling. In the first case, we scale and center columns and in the second case we scale and center rows since the matrix is transposed. If we do not do any scaling or centering we would get identical plots.

\hypertarget{eigenvectors-as-latent-factorsvariables}{%
\paragraph{Eigenvectors as latent factors/variables}\label{eigenvectors-as-latent-factorsvariables}}

Finally, we can introduce the latent factor interpretation of PCA via SVD. As we have already mentioned, eigenvectors can also be interpreted as expression programs that are shared by several genes such as cell cycle expression program when measuring gene expression across samples taken in different time points. In this interpretation, linear combination of expression programs makes up the expression profile of the genes. Linear combination simply means multiplying the expression program with a weight and adding them up. Our \(USV^T\) matrix multiplication can be rearranged to yield such an understanding, we can multiply eigenarrays \(U\) with the diagonal eigenvalues \(S\), to produce an m-by-n weights matrix called \(W\), so \(W=US\) and we can re-write the equation as just weights by eigenvectors matrix, \(X=WV^T\) as shown in Figure \ref{fig:SVDasWeigths}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/SVDasWeights} 

}

\caption{Singular value decomposition (SVD) reorganized as multiplication of m-by-n weights matrix and eigenvectors }\label{fig:SVDasWeigths}
\end{figure}

This simple transformation now makes it clear that indeed, if eigenvectors represent expression programs, their linear combination makes up individual gene expression profiles. As an example, we can show the linear combination of the first two eigenvectors can approximate the expression profile of a hypothetical gene in the gene expression matrix. Figure \ref{fig:SVDlatentExample} shows eigenvector 1 and eigenvector 2 combined with certain weights in \(W\) matrix can approximate gene expression pattern our example gene.

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{images/SVDlatentExample} 

}

\caption{Gene expression of a gene can be regarded as a linear combination of eigenvectors. }\label{fig:SVDlatentExample}
\end{figure}

However, SVD does not care about biology. The eigenvectors are just obtained from the data with constraints of orthogonality and the direction of variation. There are examples of eigenvectors representing
real expression programs but that does not mean eigenvectors will always be biologically meaningful. Sometimes a combination of them might make more sense in biology than single eigenvectors. This is also the same for the other matrix factorization techniques we describe below.

\hypertarget{other-matrix-factorization-methods-for-dimensionality-reduction}{%
\subsection{Other matrix factorization methods for dimensionality reduction}\label{other-matrix-factorization-methods-for-dimensionality-reduction}}

We must mention a few other techniques that are similar to SVD in spirit. Remember, we mentioned that every matrix can be decomposed to other matrices where matrix multiplication operations reconstruct the original matrix, which is in general called ``matrix factorization''\index{matrix factorization}. In the case of SVD/PCA, the constraint is that eigenvectors/arrays are orthogonal, however, there are other decomposition algorithms with other constraints.

\hypertarget{independent-component-analysis-ica}{%
\subsubsection{Independent component analysis (ICA)}\label{independent-component-analysis-ica}}

We will first start with independent component analysis (ICA)\index{Independent component analysis} which is an extension of PCA. ICA algorithm decomposes a given matrix \(X\) as follows: \(X=SA\) \citep{hyvarinen2013independent}. The rows of \(A\) could be interpreted similar to the eigengenes and columns of \(S\) could be interpreted as eigenarrays. These components are sometimes called metagenes and metasamples in the literature. Traditionally, \(S\) is called the source matrix and \(A\) is called mixing matrix. ICA is developed for a problem called ``blind-source separation''. In this problem, multiple microphones record sound from multiple instruments, and the task is to disentangle sounds from original instruments since each microphone is recording a combination of sounds. In this respect, the matrix \(S\) contains the original signals (sounds from different instruments) and their linear combinations identified by the weights in \(A\), and the product of \(A\) and \(S\) makes up the matrix \(X\), which is the observed signal from different microphones. With this interpretation in mind, if the interest is strictly expression patterns that represent the hidden expression programs, we see that the genes-by-samples matrix is transposed to a samples-by-genes matrix, so that the columns of \(S\) represent these expression patterns, here referred to as ``metagenes'', hopefully representing distinct expression programs (Figure \ref{fig:ICAcartoon} ). \index{independent component analyis (ICA)}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{images/ICAcartoon} 

}

\caption{Independent Component Analysis (ICA)}\label{fig:ICAcartoon}
\end{figure}

ICA requires that the columns of the \(S\) matrix, the ``metagenes'' in our example above, are statistically independent. This is a stronger constraint than uncorrelatedness. In this case, there should be no relationship between non-linear transformation of the data either. There are different ways of ensuring this statistical indepedence and this is the main constraint when finding the optimal \(A\) and \(S\) matrices. The various ICA algorithms use different proxies for statistical independence, and the definition of that proxy is the main difference between many ICA algorithms. The algorithm we are going to use requires that metagenes or sources in the \(S\) matrix are non-Gaussian (non-normal) as possible. Non-Gaussianity is shown to be related to statistical independence \citep{hyvarinen2013independent}. Below, we are using the \texttt{fastICA::fastICA()} function to extract 2 components and plot the rows of matrix \(A\) which represents metagenes shown in Figure \ref{fig:fastICAex}. This way, we can visualize samples in a 2D plot. If we wanted to plot the relationship between genes we would use the columns of matrix \(S\).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(fastICA)}
\NormalTok{ica.res=}\KeywordTok{fastICA}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat),}\DataTypeTok{n.comp=}\DecValTok{2}\NormalTok{) }\CommentTok{# apply ICA}

\CommentTok{# plot reduced dimensions}
\KeywordTok{plot}\NormalTok{(ica.res}\OperatorTok{$}\NormalTok{S[,}\DecValTok{1}\NormalTok{],ica.res}\OperatorTok{$}\NormalTok{S[,}\DecValTok{2}\NormalTok{],}\DataTypeTok{col=}\KeywordTok{as.factor}\NormalTok{(annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{04-unsupervisedLearning_files/figure-latex/fastICAex-1} 

}

\caption{Leukemia gene expression values per patient on reduced dimensions by ICA.}\label{fig:fastICAex}
\end{figure}

\hypertarget{non-negative-matrix-factorization-nmf}{%
\subsubsection{Non-negative matrix factorization (NMF)}\label{non-negative-matrix-factorization-nmf}}

Non-negative matrix factorization
\index{non-negative matrix factorization (NMF)}algorithms are series of algorithms that aim to decompose the matrix \(X\) into the product of matrices \(W\) and \(H\), \(X=WH\) (Figure \ref{fig:NMFcartoon}) \citep{lee2001algorithms}. The constraint is that \(W\) and \(H\) must contain non-negative values, so must \(X\). This is well suited for data sets that cannot contain negative values such as gene expression. This also implies additivity of components or latent factors. This is in line with the idea that expression pattern of a gene across samples is the weighted sum of multiple metagenes. Unlike ICA and SVD/PCA, the metagenes can never be combined in a subtractive way. In this sense, expression programs potentially captured by metagenes are combined additively.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/NMFcartoon} 

}

\caption{Non-negative matrix factorization summary}\label{fig:NMFcartoon}
\end{figure}

The algorithms that compute NMF try to minimize the cost function \(D(X,WH)\), which is the distance between \(X\) and \(WH\). The early algorithms just use the Euclidean distance, which translates to \(\sum(X-WH)^2\); this is also known as the Frobenius norm and you will see in the literature it is written as :\(\||X-WH||_{F}\).
However, this is not the only distance metric; other distance metrics are also used in NMF algorithms. In addition, there could be other parameters to optimize that relates to sparseness of the \(W\) and \(H\) matrices. With sparse \(W\) and \(H\), each entry in the \(X\) matrix is expressed as the sum of a small number of components. This makes the interpretation easier, if the weights are \(0\) then there is no contribution from the corresponding factors.

Below, we are plotting the values of metagenes (rows of \(H\)) for components 1 and 3, shown in Figure \ref{fig:nmfCode}. In this context, these values can also be interpreted as the relationship between samples. If we wanted to plot the relationship between genes we would plot the columns of the \(W\) matrix.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(NMF)}
\NormalTok{res=NMF}\OperatorTok{::}\KeywordTok{nmf}\NormalTok{(mat,}\DataTypeTok{rank=}\DecValTok{3}\NormalTok{,}\DataTypeTok{seed=}\StringTok{"nndsvd"}\NormalTok{) }\CommentTok{# nmf with 3 components/factors}
\NormalTok{w <-}\StringTok{ }\KeywordTok{basis}\NormalTok{(res) }\CommentTok{# get W}
\NormalTok{h <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(res)  }\CommentTok{# get H}

\CommentTok{# plot 1st factor against 3rd factor}
\KeywordTok{plot}\NormalTok{(h[}\DecValTok{1}\NormalTok{,],h[}\DecValTok{3}\NormalTok{,],}\DataTypeTok{col=}\KeywordTok{as.factor}\NormalTok{(annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType),}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{04-unsupervisedLearning_files/figure-latex/nmfCode-1} 

}

\caption{Leukemia gene expression values per patient on reduced dimensions by NMF. Components 1 and 3 is used for the plot.}\label{fig:nmfCode}
\end{figure}

We should add the note that, due to random starting points of the optimization algorithm, NMF is usually run multiple times and a consensus clustering approach is used when clustering samples. This simply means that samples are clustered together if they cluster together in multiple runs of the NMF. The NMF package we used above has built-in ways to achieve this. In addition, NMF is a family of algorithms. The choice of cost function to optimize the difference between \(X\) and \(WH\), and the methods used for optimization create multiple variants of NMF. The ``method'' parameter in the above \texttt{nmf()} function controls the algorithm choice for NMF. \index{R Packages!\texttt{NMF}}

\hypertarget{choosing-the-number-of-components-and-ranking-components-in-importance}{%
\subsubsection{Choosing the number of components and ranking components in importance}\label{choosing-the-number-of-components-and-ranking-components-in-importance}}

In both ICA and NMF, there is no well-defined way to rank components or to select the number of components. There are a couple of approaches that might suit both ICA and NMF for ranking components. One can use the norms of columns/rows in mixing matrices. This could simply mean take the sum of absolute values in mixing matrices. For our ICA example above, we would take the sum of the absolute values of the rows of \(A\) since we transposed the input matrix \(X\) before ICA. And for the NMF, we would use the columns of \(W\). These ideas assume that the larger coefficients in the weight or mixing matrices indicate more important components.

For selecting the optimal number of components, the NMF package provides different strategies. One way is to calculate the RSS for each \(k\), the number of components, and take the \(k\) where the RSS curve starts to stabilize. However, these strategies require that you run the algorithm with multiple possible component numbers. The \texttt{nmf} function will run these automatically when the \texttt{rank} argument is a vector of numbers. For ICA there is no straightforward way to choose the right number of components. A common strategy is to start with as many components as variables and try to rank them by their usefulness.

\begin{rmdtip}
\begin{rmdtip}

\textbf{Want to know more ?}

The NMF package vignette has extensive information on how to run NMF to get stable results and an estimate of components: \url{https://cran.r-project.org/web/packages/NMF/vignettes/NMF-vignette.pdf}

\end{rmdtip}
\end{rmdtip}

\hypertarget{multi-dimensional-scaling}{%
\subsection{Multi-dimensional scaling}\label{multi-dimensional-scaling}}

MDS is a set of data analysis techniques that displays the structure of distance data in a high-dimensional space into a lower dimensional space without much loss of information \citep{cox2000multidimensional}. The overall goal of MDS is to faithfully represent these distances with the lowest possible dimensions. The so-called ``classical multi-dimensional scaling'' algorithm, tries to minimize the following function:\index{Multi-dimensional scaling (MDS)}

\({\displaystyle Stress_{D}(z_{1},z_{2},...,z_{N})={\Biggl (}{\frac {\sum _{i,j}{\bigl (}d_{ij}-\|z_{i}-z_{j}\|{\bigr )}^{2}}{\sum _{i,j}d_{ij}^{2}}}{\Biggr )}^{1/2}}\)

Here the function compares the new data points on the lower dimension \((z_{1},z_{2},...,z_{N})\) to the input distances between data points or distance between samples in our gene expression example. It turns out, this problem can be efficiently solved with SVD/PCA on the scaled distance matrix, the projection on eigenvectors will be the most optimal solution for the equation above. Therefore, classical MDS is sometimes called Principal Coordinates Analysis in the literature. However, later variants improve on classical MDS by using this as a starting point and optimize a slightly different cost function that again measures how well the low-dimensional distances correspond to high-dimensional distances. This variant is called non-metric MDS and due to the nature of the cost function, it assumes a less stringent relationship between the low-dimensional distances \$\textbar z\_\{i\}-z\_\{j\}\textbar{} and input distances \(d_{ij}\). Formally, this procedure tries to optimize the following function.

\({\displaystyle Stress_{D}(z_{1},z_{2},...,z_{N})={\Biggl (}{\frac {\sum _{i,j}{\bigl (}\|z_{i}-z_{j}\|-\theta(d_{ij}){\bigr )}^{2}}{\sum _{i,j}\|z_{i}-z_{j}\|^{2}}}{\Biggr )}^{1/2}}\)

The core of a non-metric MDS algorithm is a two-fold optimization process. First the optimal monotonic transformation of the distances has to be found, which is shown in the above formula as \(\theta(d_{ij})\). Secondly, the points on a low dimension configuration have to be optimally arranged, so that their distances match the scaled distances as closely as possible. These two steps are repeated until some convergence criteria is reached. This usually means that the cost function does not improve much after certain number of iterations. The basic steps in a non-metric MDS algorithm are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Find a random low-dimensional configuration of points, or in the variant we will be using below we start with the configuration returned by classical MDS.
\item
  Calculate the distances between the points in the low dimension \(\|z_{i}-z_{j}\|\), \(z_{i}\) and \(z_{j}\) are vector of positions for samples \(i\) and \(j\).
\item
  Find the optimal monotonic transformation of the input distance, \({\textstyle \theta(d_{ij})}\), to approximate input distances to low-dimensional distances. This is achieved by isotonic regression, where a monotonically increasing free-form function is fit. This step practically ensures that ranking of low-dimensional distances are similar to rankings of input distances.
\item
  Minimize the stress function by re-configuring low-dimensional space and keeping \(\theta\) function constant.
\item
  Repeat from Step 2 until convergence.
\end{enumerate}

We will now demonstrate both classical MDS and Kruskal's isometric MDS.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mds=}\KeywordTok{cmdscale}\NormalTok{(}\KeywordTok{dist}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat)))}
\NormalTok{isomds=MASS}\OperatorTok{::}\KeywordTok{isoMDS}\NormalTok{(}\KeywordTok{dist}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## initial  value 15.907414 
## final  value 13.462986 
## converged
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the patients in the 2D space}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(mds,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{col=}\KeywordTok{as.factor}\NormalTok{(annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType),}
     \DataTypeTok{main=}\StringTok{"classical MDS"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(isomds}\OperatorTok{$}\NormalTok{points,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{col=}\KeywordTok{as.factor}\NormalTok{(annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType),}
     \DataTypeTok{main=}\StringTok{"isotonic MDS"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{04-unsupervisedLearning_files/figure-latex/mds2-1} 

}

\caption{Leukemia gene expression values per patient on reduced dimensions by classical MDS and isometric MDS.}\label{fig:mds2}
\end{figure}

The resulting plot is shown in Figure \ref{fig:mds2}. In this example, there is not much difference between isotonic MDS and classical MDS. However, there might be cases where different MDS methods provide visible changes in the scatter plots.

\hypertarget{t-distributed-stochastic-neighbor-embedding-t-sne}{%
\subsection{t-Distributed Stochastic Neighbor Embedding (t-SNE)}\label{t-distributed-stochastic-neighbor-embedding-t-sne}}

t-SNE maps the distances in high-dimensional space to lower dimensions and it is similar to the MDS method in this respect. But the benefit of this particular method is that it tries to preserve the local structure of the data so the distances and grouping of the points we observe in lower dimensions such as a 2D scatter plot is as close as possible to the distances we observe in the high-dimensional space \citep{maaten2008visualizing}. As with other dimension reduction methods, you can choose how many lower dimensions you need. The main difference of t-SNE, as mentiones above, is that it tries to preserve the local structure of the data. This kind of local structure embedding is missing in the MDS algorithm, which also has a similar goal. MDS tries to optimize the distances as a whole, whereas t-SNE optimizes the distances with the local structure in mind. This is defined by the ``perplexity'' parameter in the arguments. This parameter controls how much the local structure influences the distance calculation. The lower the value, the more the local structure is taken into account. Similar to MDS, the process is an optimization algorithm. Here, we also try to minimize the divergence between observed distances and lower dimension distances. However, in the case of t-SNE, the observed distances and lower dimensional distances are transformed using a probabilistic framework with their local variance in mind.\index{t-Distributed Stochastic Neighbor Embedding (t-SNE)}

From here on, we will provide a bit more detail on how the algorithm works in case the conceptual description above is too shallow. In t-SNE the Euclidean distances between data points are transformed into a conditional similarity between points. This is done by assuming a normal distribution on each data point with a variance calculated ultimately by the use of the ``perplexity'' parameter. The perplexity parameter is, in a sense, a guess about the number of the closest neighbors each point has. Setting it to higher values gives more weight to global structure. Given \(d_{ij}\) is the Euclidean distance between point \(i\) and \(j\), the similarity score \(p_{ij}\) is calculated as shown below.

\[p_{j | i} = \frac{\exp(-\|d_{ij}\|^2 / 2 σ_i^2)}{∑_{k \neq i} \exp(-\|d_{ik}\|^2 / 2 σ_i^2)}\]

This distance is symmetrized by incorporating \(p_{i | j}\) as shown below.

\[p_{i j}=\frac{p_{j|i} + p_{i|j}}{2n}\]

For the distances in the reduced dimension, we use t-distribution with one degree of freedom. In the formula below, \(| y_i-y_j\|^2\) is Euclidean distance between points \(i\) and \(j\) in the reduced dimensions.

\[
q_{i j} = \frac{(1+ \| y_i-y_j\|^2)^{-1}}{(∑_{k \neq l} 1+ \| y_k-y_l\|^2)^{-1} }
\]

As most of the algorithms we have seen in this section, t-SNE is an optimization process in essence. In every iteration the points along lower dimensions are re-arranged to minimize the formulated difference between the observed joint probabilities (\(p_{i j}\)) and low-dimensional joint probabilities (\(q_{i j}\)). Here we are trying to compare probability distributions. In this case, this is done using a method called Kullback-Leibler divergence, or KL-divergence. In the formula below, since the \(p_{i j}\) is pre-defined using original distances, the only way to optimize is to play with \(q_{i j}\) because it depends on the configuration of points in the lower dimensional space. This configuration is optimized to minimize the KL-divergence between \(p_{i j}\) and \(q_{i j}\).

\[
KL(P||Q) = \sum_{i, j} p_{ij} \, \log \frac{p_{ij}}{q_{ij}}.
\]
Strictly speaking, KL-divergence measures how well the distribution \(P\) which is observed using the original data points can be approximated by distribution \(Q\), which is modeled using points on the lower dimension. If the distributions are identical, KL-divergence would be \(0\). Naturally, the more divergent the distributions are, the higher the KL-divergence will be.

We will now show how to use t-SNE on our gene expression data set using the \texttt{Rtsne} package \index{R Packages!\texttt{Rtsne}}. We are setting the random seed because again, the t-SNE optimization algorithm has random starting points and this might create non-identical results in every run. After calculating the t-SNE lower dimension embeddings we plot the points in a 2D scatter plot, shown in Figure \ref{fig:tsne}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"Rtsne"}\NormalTok{)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{) }\CommentTok{# Set a seed if you want reproducible results}
\NormalTok{tsne_out <-}\StringTok{ }\KeywordTok{Rtsne}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat),}\DataTypeTok{perplexity =} \DecValTok{10}\NormalTok{) }\CommentTok{# Run TSNE}
 \CommentTok{#image(t(as.matrix(dist(tsne_out$Y))))}
\CommentTok{# Show the objects in the 2D tsne representation}
\KeywordTok{plot}\NormalTok{(tsne_out}\OperatorTok{$}\NormalTok{Y,}\DataTypeTok{col=}\KeywordTok{as.factor}\NormalTok{(annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType),}
     \DataTypeTok{pch=}\DecValTok{19}\NormalTok{)}

\CommentTok{# create the legend for the Leukemia types}
\KeywordTok{legend}\NormalTok{(}\StringTok{"bottomleft"}\NormalTok{,}
       \DataTypeTok{legend=}\KeywordTok{unique}\NormalTok{(annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType),}
       \DataTypeTok{fill =}\KeywordTok{palette}\NormalTok{(}\StringTok{"default"}\NormalTok{),}
       \DataTypeTok{border=}\OtherTok{NA}\NormalTok{,}\DataTypeTok{box.col=}\OtherTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{04-unsupervisedLearning_files/figure-latex/tsne-1} 

}

\caption{t-SNE of leukemia expression dataset}\label{fig:tsne}
\end{figure}

As you might have noticed, we set again a random seed with the \texttt{set.seed()} function. The optimization algorithm starts with random configuration of points in the lower dimension space, and in each iteration it tries to improve on the previous lower dimension conflagration, which is why starting points can result in different final outcomes.

\begin{rmdtip}
\begin{rmdtip}

\textbf{Want to know more ?}

\begin{itemize}
\tightlist
\item
  How perplexity affects t-sne, interactive examples: \url{https://distill.pub/2016/misread-tsne/}
\item
  More on perplexity: \url{https://blog.paperspace.com/dimension-reduction-with-t-sne/}
\item
  Intro to t-SNE: \url{https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm}
\end{itemize}

\end{rmdtip}
\end{rmdtip}

\hypertarget{exercises-2}{%
\section{Exercises}\label{exercises-2}}

For this set of exercises we will be using the expression data shown below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{expFile=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                    \StringTok{"leukemiaExpressionSubset.rds"}\NormalTok{,}
                    \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{mat=}\KeywordTok{readRDS}\NormalTok{(expFile)}
\end{Highlighting}
\end{Shaded}

\hypertarget{clustering}{%
\subsection{Clustering}\label{clustering}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We want to observe the effect of data transformation in this exercise. Scale the expression matrix with the \texttt{scale()} function. In addition, try taking the logarithm of the data with the \texttt{log2()} function prior to scaling. Make box plots of the unscaled and scaled data sets using the \texttt{boxplot()} function. {[}Difficulty: \textbf{Beginner/Intermediate}{]}
\item
  For the same problem above using the unscaled data and different data transformation strategies, use the \texttt{ward.d} distance in hierarchical clustering and plot multiple heatmaps. You can try to use the \texttt{pheatmap} library or any other library that can plot a heatmap with a dendrogram. Which data-scaling strategy provides more homogeneous clusters with respect to disease types? {[}Difficulty: \textbf{Beginner/Intermediate}{]}
\item
  For the transformed and untransformed data sets used in the exercise above, use the silhouette for deciding number of clusters using hierarchical clustering. {[}Difficulty: \textbf{Intermediate/Advanced}{]}
\item
  Now, use the Gap Statistic for deciding the number of clusters in hierarchical clustering. Is it the same number of clusters identified by two methods? Is it similar to the number of clusters obtained using the k-means algorithm in the chapter. {[}Difficulty: \textbf{Intermediate/Advanced}{]}
\end{enumerate}

\hypertarget{dimension-reduction}{%
\subsection{Dimension reduction}\label{dimension-reduction}}

We will be using the leukemia expression data set again. You can use it as shown in the clustering exercises.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Do PCA on the expression matrix using the \texttt{princomp()} function and then use the \texttt{screeplot()} function to visualize the explained variation by eigenvectors. How many top components explain 95\% of the variation? {[}Difficulty: \textbf{Beginner}{]}
\item
  Our next tasks are to remove eigenvectors and reconstruct the matrix using SVD, then calculate the reconstruction error as the difference between original and reconstructed matrix. HINT: You have to use the \texttt{svd()} function and equalize eigenvalue to \(0\) for the component you want to remove. {[}Difficulty: \textbf{Intermediate/Advanced}{]}
\item
  Produce a 10-component ICA from the expression data set. Remove each component and measure the reconstruction error without that component. Rank the components by decreasing reconstruction-error. {[}Difficulty: \textbf{Advanced}{]}
\item
  In this exercise we use the \texttt{Rtsne()} function on the leukemia expression data set. Try to increase and decrease perplexity t-sne, and describe the observed changes in 2D plots. {[}Difficulty: \textbf{Beginner}{]}
\end{enumerate}

\hypertarget{supervisedLearning}{%
\chapter{Predictive Modeling with Supervised Machine Learning}\label{supervisedLearning}}

In this chapter we will introduce supervised machine learning applications for predictive modeling. In genomics, we are often faced with biological questions to answer using lots of data. Some of those questions can easily fit in the domain of machine learning, where algorithms will learn a mathematical model of the input data in order to make decisions about similar data, previously unseen by the model. Often we are trying to predict a medical or biological variable of interest using molecular signatures obtained via genomics methods. To give you a better idea, we listed some of the machine learning applications in genomics:

\begin{itemize}
\tightlist
\item
  Predicting gene expression from epigenetic modifications \citep{pmid22950368}.
\item
  Predicting gene locations \citep{pmid12364589}.
\item
  Predicting enhancer or other regulatory regions \citep{pmid22328731}.
\item
  Predicting drug response based on genomics \citep{pmid21428770}.
\item
  Predicting healthy/disease status or disease subtypes based on genomics \citep{pmid25750696}.
\item
  Predicting the effect of SNPs on gene regulation \citep{pmid26301843}.
\item
  Calling SNPs \citep{pmid30247488}.
\end{itemize}

Apart from prediction of an outcome, machine learning can be used to understand which predictor variables are the most important for prediction performance. This often gives insights into the biology as well. Many machine learning algorithms have either built-in variable importance assessment or can be wrapped around a model-agnostic variable importance method. For example, we may want to find which epigenetic modifications are most important for gene expression prediction. Although decades of molecular biology gives a pretty good idea for this, we could arrive at similar conclusions by building a machine learning model to predict gene expression using histone modifications H3K27ac, H3K27me, H3K4me1, H3K4me3, and DNA methylation. \index{histone modification} \index{DNA methylation} We can then check which of these are most important for gene expression prediction using variable importance metrics.

In this chapter, we will show how to use supervised machine learning models to solve problems in genomics. We will go over general steps in machine learning applications. In addition, we will introduce how to use some of the most popular supervised machine learning models in practice.

\hypertarget{how-are-machine-learning-models-fit}{%
\section{How are machine learning models fit?}\label{how-are-machine-learning-models-fit}}

We already have quite an insight on how machine learning models are fit. We have previously seen clustering methods, which are unsupervised machine learning models, and we have seen linear regression which is a simple machine learning model if we disregard its objectives for statistical inference.

Machine learning models are optimization methods at their core. They all depend on defining a ``cost'' or ``loss'' function to minimize\index{loss function}\index{cost function}. For example, in linear regression the difference between the predicted and the original values are being minimized. When we have a data set with the correct answer such as original values or class labels, this is called supervised learning. We use the structure in the data to predict a value, and optimization methods help us use the right structure or patterns in the data. The supervised machine learning methods use predictor variables such as gene expression values or other genomic scores to build a mathematical function, or a mapping method if you will. This function maps a predictor variable vector or matrix from a given sample to the response variable: labels/classes or numeric values. The response variable is also called the ``dependent variable''. Then, the predictions are simply output of mathematical functions, \(f(X)\). These functions take predictor variables, \(X\), as input. The variables in \(X\) are also \index{independent variables} called ``independent variables'', ``explanatory variables'' or ``features''. \index{explanatory variables}The functions also have internal parameters that help map \(X\) to the predicted values. The optimization works on the parameters of \(f(X)\) and tries to minimize the difference between the function output and original response variables (\(Y\)): \(\sum(Y-f(X))^2\). Now, this is just a simplification of the actual ``cost'' or ``loss'' function. Especially in classification problems, cost functions can take different forms, but the idea is the same. You have a mathematical expression you can minimize by searching for the optimal parameter values. The core ingredients of a machine learning algorithm are the same and they are listed as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Define a prediction function or method \(f(X)\).
\item
  Devise a function (called the loss or cost function) to optimize the difference between your predictions and observed values, such as \(\sum (Y-f(X))^2\).
\item
  Apply mathematical optimization methods to find the best parameter values for \(f(X)\) in relation to the cost/loss function.\index{optimization}
\end{enumerate}

Similarly, clustering and dimension reduction techniques can use optimization methods, but they do so without having a correct answer to predict or train with. In this case, they find patterns or structure in the data without trying to estimate a correct answer. These patterns are groupings of samples or variables, such as common gene expression patterns, that can be obtained from dimension reduction techniques such as PCA. In general, dimension reduction algorithms can be thought of as optimization procedures that are trying to minimize \(X-WH\). Here, \(X\) is our original data set and \(WH\) is the product of potentially two lower dimension matrices, \(W\) and \(H\). In this case, the optimization procedure hopefully gives us the lower-dimensional space so that we can represent our data without losing too much information.

\hypertarget{machine-learning-vs.-statistics}{%
\subsection{Machine learning vs.~statistics}\label{machine-learning-vs.-statistics}}

Machine learning and statistics are related and sometimes overlapping fields. Statistical inference is the main purpose of statistics. The aim of inference is to find statistical properties of the underlying data and to estimate the uncertainty about those properties. However, while doing so, the field of statistics developed dimension reduction and regression techniques that are the cornerstone of machine learning applications.

Both machine learning and statistics share the same overarching goal, which is learning from the data.
The difference between the two is that machine learning emphasizes optimization and performance over statistical inference. Statistics is also concerned about performance but would like to calculate the uncertainty associated with parameters of the model. It will try to model the population statistics from the sample data points to assess that uncertainty. Having said that, many machine learning algorithms, including a couple we will introduce below, are developed by scientists who will define themselves as statisticians, and work at statistics departments of universities.

\hypertarget{steps-in-supervised-machine-learning}{%
\section{Steps in supervised machine learning}\label{steps-in-supervised-machine-learning}}

There are many methods to use for supervised learning problems. However, there are similar steps that you will need to follow whatever machine learning method you choose to train. These steps are briefly described below and we will get back to these in detail later in the chapter:

\begin{itemize}
\tightlist
\item
  Pre-processing data: We might have to use normalization and data transformation procedures.
\item
  Training and test data split: Decide which strategy you want to use for evaluation purposes. You need to use a test set to evaluate your model later on.
\item
  Training the model: This is where your choice of supervised learning algorithm becomes relevant. ``Training'' generally means your data set is used in optimization of the loss function to find parameters for \(f(x)\).
\item
  Estimating performance of the model: This is about which metrics to use to evaluate performance and how to calculate those metrics.
\item
  Model tuning and selection: We try different parameters and select the best model.
\end{itemize}

Many of these steps are identical for different supervised learning methods. Therefore, we will use the \href{http://topepo.github.io/caret/index.html}{\texttt{caret}} package to\index{R Packages!\texttt{caret}} perform these steps, which streamlines the steps and provides a similar interface for different supervised learning methods. There are other similar packages, such as \href{https://mlr.mlr-org.com/}{\texttt{mlr}}, \index{R Packages!\texttt{mlr}}that can provide similar functionality. For now, we will focus on classification models, which is a subset of supervised learning models. In these types of models, we try to predict a categorical response variable, such as if a patient has the disease or not, or what type of disease the patient has based on predictor variables.

\hypertarget{use-case-disease-subtype-from-genomics-data}{%
\section{Use case: Disease subtype from genomics data}\label{use-case-disease-subtype-from-genomics-data}}

We will start our illustration of machine learning using a real dataset from tumor biopsies. We will use the gene expression data of glioblastoma tumor samples from The Cancer Genome Atlas\index{The Cancer Genome Atlas (TCGA)} project. We will try to predict the subtype of this disease using molecular markers. \index{CpG island}This subtype is characterized by large-scale epigenetic alterations called the ``CpG island methylator phenotype'' or ``CIMP'' \citep{pmid20399149}; half of the patients in our data set have this subtype and the rest do not, and we will try to predict which ones have the CIMP subtype. There two data objects we need for this exercise, one for gene expression values per tumor sample and the other one is subtype annotation per patient. In the expression data set, every row is a patient and every column is a gene expression value\index{gene expression}. There are 184 tumor samples. This data set might be a bit small for real-world applications, however it is very relevant for the genomics focus of this book and the small datasets take less time to train, which is useful for reproducibility purposes. We will read these data sets from the \textbf{compGenomRData} package now with the \texttt{readRDS()} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get file paths}
\NormalTok{fileLGGexp=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                      \StringTok{"LGGrnaseq.rds"}\NormalTok{,}
                      \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{fileLGGann=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                      \StringTok{"patient2LGGsubtypes.rds"}\NormalTok{,}
                      \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\CommentTok{# gene expression values}
\NormalTok{gexp=}\KeywordTok{readRDS}\NormalTok{(fileLGGexp)}
\KeywordTok{head}\NormalTok{(gexp[,}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       TCGA-CS-4941 TCGA-CS-4944 TCGA-CS-5393 TCGA-CS-5394 TCGA-CS-5395
## A1BG       72.2326      24.7132      46.3789      37.9659      19.5162
## A1CF        0.0000       0.0000       0.0000       0.0000       0.0000
## A2BP1     524.4997     105.4092     323.5828      19.7390     299.5375
## A2LD1     144.0856      18.0154      29.0942       7.5945     202.1231
## A2ML1     521.3941     159.3746     164.6157      63.5664     953.4106
## A2M     17944.7205   10894.9590   16480.1130    9217.7919   10801.8461
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(gexp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 20501   184
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# patient annotation}
\NormalTok{patient=}\KeywordTok{readRDS}\NormalTok{(fileLGGann)}
\KeywordTok{head}\NormalTok{(patient)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              subtype
## TCGA-FG-8185    CIMP
## TCGA-DB-5276    CIMP
## TCGA-P5-A77X    CIMP
## TCGA-IK-8125    CIMP
## TCGA-DU-A5TR    CIMP
## TCGA-E1-5311    CIMP
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(patient)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 184   1
\end{verbatim}

\hypertarget{data-preprocessing}{%
\section{Data preprocessing}\label{data-preprocessing}}

We will have to preprocess the data before we start training. This might include exploratory data analysis to see how variables and samples relate to each other. For example, we might want to check the correlation between predictor variables and keep only one variable from that group. In addition, some training algorithms might be sensitive to data scales or outliers\index{outliers}. We should deal with those issues in this step. In some cases, the data might have missing values. We can choose to remove the samples that have missing values or try to impute them. Many machine learning algorithms will not be able to deal with missing values.

We will show how to do this in practice using the \texttt{caret::preProcess()} function and base R functions. Please note that there are more preprocessing options available than we will show here. There are more possibilities in \texttt{caret::preProcess()}function and base R functions, we are just going to cover a few basics in this section.

\hypertarget{data-transformation}{%
\subsection{Data transformation}\label{data-transformation}}

The first thing we will do is data normalization and transformation. We have to take care of data scale issues that might come from how the experiments are performed and the potential problems that might occur during data collection. Ideally, each tumor sample has a similar distribution of gene expression values. Systematic differences between tumor samples must be corrected. We check if there are such differences using box plots.
We will only plot the first 50 tumor samples so that the figure is not too
squished. The resulting boxplot is shown in Figure \ref{fig:boxML}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boxplot}\NormalTok{(gexp[,}\DecValTok{1}\OperatorTok{:}\DecValTok{50}\NormalTok{],}\DataTypeTok{outline=}\OtherTok{FALSE}\NormalTok{,}\DataTypeTok{col=}\StringTok{"cornflowerblue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{05-supervisedLearning_files/figure-latex/boxML-1} 

}

\caption{Boxplots for gene expression values.}\label{fig:boxML}
\end{figure}

It seems there was some normalization done on this data. Gene expression values per sample seem to have the same scale. However, it looks like they have long-tailed distributions, so a log transformation may fix that. These long-tailed distributions have outliers \index{outliers}and this might adversely affect the models. Below, we show the effect of log transformation on the gene expression profile of a patient. We add a pseudo count of 1 to avoid \texttt{log(0)}.
The resulting histograms are shown in Figure \ref{fig:logTransform}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{hist}\NormalTok{(gexp[,}\DecValTok{5}\NormalTok{],}\DataTypeTok{xlab=}\StringTok{"gene expression"}\NormalTok{,}\DataTypeTok{main=}\StringTok{""}\NormalTok{,}\DataTypeTok{border=}\StringTok{"blue4"}\NormalTok{,}
     \DataTypeTok{col=}\StringTok{"cornflowerblue"}\NormalTok{)}
\KeywordTok{hist}\NormalTok{(}\KeywordTok{log10}\NormalTok{(gexp}\OperatorTok{+}\DecValTok{1}\NormalTok{)[,}\DecValTok{5}\NormalTok{], }\DataTypeTok{xlab=}\StringTok{"gene expression log scale"}\NormalTok{,}\DataTypeTok{main=}\StringTok{""}\NormalTok{,}
     \DataTypeTok{border=}\StringTok{"blue4"}\NormalTok{,}\DataTypeTok{col=}\StringTok{"cornflowerblue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{05-supervisedLearning_files/figure-latex/logTransform-1} 

}

\caption{Gene expression distribution for the 5th patient (left). Log transformed gene expression distribution for the same patient (right).}\label{fig:logTransform}
\end{figure}

Since taking a log seems to work to tame the extreme values, we do that below and also add \(1\) pseudo-count to be able to deal with \(0\) values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gexp=}\KeywordTok{log10}\NormalTok{(gexp}\OperatorTok{+}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Another thing we can do in combination with this is to winsorize the data, which caps extreme values to the 1st and 99th percentiles or to other user-defined percentiles. But before we go forward, we should transpose our data. In this case, the predictor variables are gene expression values and they should be on the column side. It was OK to leave them on the row side, to check systematic errors with box plots, but machine learning algorithms require that predictor variables are on the column side.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# transpose the data set}
\NormalTok{tgexp <-}\StringTok{ }\KeywordTok{t}\NormalTok{(gexp)}
\end{Highlighting}
\end{Shaded}

\hypertarget{filtering-data-and-scaling}{%
\subsection{Filtering data and scaling}\label{filtering-data-and-scaling}}

We can filter predictor variables which have low variation. They are not likely to have any predictive importance since there is not much variation and they will just slow our algorithms. The more variables, the slower the algorithms will be generally. The \texttt{caret::preProcess()} function can help filter the predictor variables with near zero variance.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\CommentTok{# remove near zero variation for the columns at least}
\CommentTok{# 85% of the values are the same}
\CommentTok{# this function creates the filter but doesn't apply it yet}
\NormalTok{nzv=}\KeywordTok{preProcess}\NormalTok{(tgexp,}\DataTypeTok{method=}\StringTok{"nzv"}\NormalTok{,}\DataTypeTok{uniqueCut =} \DecValTok{15}\NormalTok{)}

\CommentTok{# apply the filter using "predict" function}
\CommentTok{# return the filtered dataset and assign it to nzv_tgexp}
\CommentTok{# variable}
\NormalTok{nzv_tgexp=}\KeywordTok{predict}\NormalTok{(nzv,tgexp)}
\end{Highlighting}
\end{Shaded}

In addition, we can also choose arbitrary cutoffs for variability. For example, we can choose to take the top 1000 variable predictors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SDs=}\KeywordTok{apply}\NormalTok{(tgexp,}\DecValTok{2}\NormalTok{,sd )}
\NormalTok{topPreds=}\KeywordTok{order}\NormalTok{(SDs,}\DataTypeTok{decreasing =} \OtherTok{TRUE}\NormalTok{)[}\DecValTok{1}\OperatorTok{:}\DecValTok{1000}\NormalTok{]}
\NormalTok{tgexp=tgexp[,topPreds]}
\end{Highlighting}
\end{Shaded}

We can also center the data, which as we have seen in Chapter 4, is subtracting the mean. Following this, the predictor variables will have zero means. In addition, we can scale the data. When we scale, each value of the predictor
variable is divided by its standard deviation. Therefore predictor variables will have the same standard deviation. These manipulations are generally used to improve the numerical stability of some calculations. In distance-based metrics, it could be beneficial to at least center the data. We will now center the data using the \texttt{preProcess()} function. This is more practical than the \texttt{scale()} function because when we get a new data point, we can use the \texttt{predict()} function and \texttt{processCenter} object to process it just like we did for the training samples.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\NormalTok{processCenter=}\KeywordTok{preProcess}\NormalTok{(tgexp, }\DataTypeTok{method =} \KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{))}
\NormalTok{tgexp=}\KeywordTok{predict}\NormalTok{(processCenter,tgexp)}
\end{Highlighting}
\end{Shaded}

We will next filter the predictor variables that are highly correlated. You may choose not to do this as some methods can handle correlation between predictor variables.\index{collinearity} However, the fewer predictor variables we have, the faster the model fitting can be done.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a filter for removing higly correlated variables}
\CommentTok{# if two variables are highly correlated only one of them}
\CommentTok{# is removed}
\NormalTok{corrFilt=}\KeywordTok{preProcess}\NormalTok{(tgexp, }\DataTypeTok{method =} \StringTok{"corr"}\NormalTok{,}\DataTypeTok{cutoff =} \FloatTok{0.9}\NormalTok{)}
\NormalTok{tgexp=}\KeywordTok{predict}\NormalTok{(corrFilt,tgexp)}
\end{Highlighting}
\end{Shaded}

\hypertarget{dealing-with-missing-values}{%
\subsection{Dealing with missing values}\label{dealing-with-missing-values}}

In real-life situations, there will be missing values in our data. In genomics, we might not have values for certain genes or genomic locations due to technical problems during experiments. We have to be able to deal with these missing values\index{missing values}. For demonstration purposes, we will now introduce NA values in our data, the ``NA'' value is normally used to encode missing values in R. We then show how to check and deal with those. One way is to impute them; here, we again use a machine learning algorithm to guess the missing values. Another option is to discard the samples with missing values or discard the predictor variables with missing values. First, we replace one of the values as NA and check if it is there.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{missing_tgexp=tgexp}
\NormalTok{missing_tgexp[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]=}\OtherTok{NA}
\KeywordTok{anyNA}\NormalTok{(missing_tgexp) }\CommentTok{# check if there are NA values}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

Next, we will try to remove that gene from the set. Removing genes or samples both have downsides. You might be removing a predictor variable that could be important for the prediction. Removing samples with missing values will decrease the number of samples in the training set. The code below checks which values are NA in the matrix, then runs a column sum and keeps everything where the column sum is equal to 0. The column sums where there are NA values will be higher than 0 depending on how many NA values there are in a column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gexpnoNA=missing_tgexp[ , }\KeywordTok{colSums}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(missing_tgexp)) }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

We will next try to impute the missing value(s). Imputation can be as simple as assigning missing values to the mean or median value of the variable, or assigning the mean/median of values from nearest neighbors of the sample having the missing value. We will show both using the \texttt{caret::preProcess()} function. First, let us run the median imputation.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\NormalTok{mImpute=}\KeywordTok{preProcess}\NormalTok{(missing_tgexp,}\DataTypeTok{method=}\StringTok{"medianImpute"}\NormalTok{)}
\NormalTok{imputedGexp=}\KeywordTok{predict}\NormalTok{(mImpute,missing_tgexp)}
\end{Highlighting}
\end{Shaded}

Another imputation\index{imputation} method that is more precise than the median imputation is to impute the missing values based on the nearest neighbors of the samples. In this case, the algorithm finds samples that are most similar to the sample vector with NA values. Next, the algorithm averages the non-missing values from those neighbors and replaces the missing value with that value.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(RANN)}
\NormalTok{knnImpute=}\KeywordTok{preProcess}\NormalTok{(missing_tgexp,}\DataTypeTok{method=}\StringTok{"knnImpute"}\NormalTok{)}
\NormalTok{knnimputedGexp=}\KeywordTok{predict}\NormalTok{(knnImpute,missing_tgexp) }
\end{Highlighting}
\end{Shaded}

\hypertarget{splitting-the-data}{%
\section{Splitting the data}\label{splitting-the-data}}

At this point we might choose to split the data into the test and the training partitions. The reason for this is that we need an independent test we did not train on. This will become clearer in the following sections, but without having a separate test set, we cannot assess the performance of our model or tune it properly.

\hypertarget{holdout-test-dataset}{%
\subsection{Holdout test dataset}\label{holdout-test-dataset}}

There are multiple data split strategies. For starters, we will split 30\% of the data as the test. This method is the gold standard for testing performance of our model. By doing this, we have a separate data set that the model has never seen. First, we create a single data frame with predictors and response variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tgexp=}\KeywordTok{merge}\NormalTok{(patient,tgexp,}\DataTypeTok{by=}\StringTok{"row.names"}\NormalTok{)}

\CommentTok{# push sample ids back to the row names}
\KeywordTok{rownames}\NormalTok{(tgexp)=tgexp[,}\DecValTok{1}\NormalTok{]}
\NormalTok{tgexp=tgexp[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Now that the response variable or the class label is merged with our dataset, we can split it into test and training sets with the \texttt{caret::createPartition()} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{3031}\NormalTok{) }\CommentTok{# set the random number seed for reproducibility }

\CommentTok{# get indices for 70% of the data set}
\NormalTok{intrain <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ tgexp[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{p=} \FloatTok{0.7}\NormalTok{)[[}\DecValTok{1}\NormalTok{]]}

\CommentTok{# seperate test and training sets}
\NormalTok{training <-}\StringTok{ }\NormalTok{tgexp[intrain,]}
\NormalTok{testing <-}\StringTok{ }\NormalTok{tgexp[}\OperatorTok{-}\NormalTok{intrain,]}
\end{Highlighting}
\end{Shaded}

\hypertarget{cross-validation}{%
\subsection{Cross-validation}\label{cross-validation}}

In some cases, we might have too few data points and it might be too costly to set aside a significant portion of the data set as a holdout test set. In these cases a resampling-based technique such as cross-validation may be useful.\index{cross-validation}

Cross-validation works by splitting the data into randomly sampled \(k\) subsets, called k-folds. So, for example, in the case of 5-fold cross-validation with 100 data points, we would create 5 folds, each containing 20 data points. We would then build models and estimate errors 5 times. Each time, four of the groups are combined (resulting in 80 data points) and used to train your model. Then the 5th group of 20 points that was not used to construct the model is used to estimate the test error. In the case of 5-fold cross-validation, we would have 5 error estimates that could be averaged to obtain a more robust estimate of the test error.

An extreme case of k-fold cross-validation, is to equalize the \(k\) to the number of data points or in our case, the number of tumor samples. This is called leave-one-out cross-validation (LOOCV). This could be better than k-fold cross-validation but it takes too much time to train that many models if the number of data points is large.

The \texttt{caret} package\index{R Packages!\texttt{caret}} has built-in cross-validation functionality for all the machine learning methods and we will be using that in the later sections.

\hypertarget{bootstrap-resampling}{%
\subsection{Bootstrap resampling}\label{bootstrap-resampling}}

Another method to estimate the prediction error is to use bootstrap resampling. This is a general method we have already introduced in Chapter \ref{stats}. It can be used to estimate variability of any statistical parameter. In this case, that parameter is the test error or test accuracy.\index{bootstrap resampling}

The training set is drawn from the original set with replacement (same size as the original set), then we build a model with this bootstrap resampled set. Next, we take the data points that are not selected for the random sample and predict labels for them. These data points are called the ``out-of-the-bag (OOB) sample''. We repeat this process many times and record the error for the OOB samples. We can take the average of the OOB errors to estimate the real test error. This is a powerful method that is not only used to estimate test error but incorporated into the training part of some machine learning methods such as random forests. Normally, we should repeat the process hundreds or up to a thousand times to get good estimates. However, the limiting factor would be the time it takes to construct and test that many models. Twenty to 30 repetitions might be enough if the time cost of training is too high. Again, the \texttt{caret} \index{R Packages!\texttt{caret}} package provides the bootstrap interface for many machine learning models for sampling before training and estimating the error on OOB samples.

\hypertarget{predicting-the-subtype-with-k-nearest-neighbors}{%
\section{Predicting the subtype with k-nearest neighbors}\label{predicting-the-subtype-with-k-nearest-neighbors}}

One of the easiest things to wrap our heads around when we are trying to predict a label such as disease subtype is to look for similar samples and assign the labels of those similar samples to our sample.

Conceptually, k-nearest neighbors (k-NN) is very similar to\index{k-nearest neighbors (k-NN)} clustering algorithms we have seen earlier. If we have a measure of distance between the samples, we can find the nearest \(k\) samples to our new sample and use a voting method to decide on the label of our new sample.

Let us run the k-NN algorithm with our cancer data. For illustrative purposes, we provide the same data set for training and test data. Providing the training data as the test data shows us the training error or accuracy, which is how the model is doing on the data it is trained with. Below we are running k-NN with the \texttt{caret:knn3()} function. The most important argument is \texttt{k}, which is the number of nearest neighbors to consider. In this case, we set it to 5. We will later discuss how to find the best \texttt{k}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\NormalTok{knnFit=}\KeywordTok{knn3}\NormalTok{(}\DataTypeTok{x=}\NormalTok{training[,}\OperatorTok{-}\DecValTok{1}\NormalTok{], }\CommentTok{# training set}
            \DataTypeTok{y=}\NormalTok{training[,}\DecValTok{1}\NormalTok{], }\CommentTok{# training set class labels}
            \DataTypeTok{k=}\DecValTok{5}\NormalTok{)}
\CommentTok{# predictions on the test set}
\NormalTok{trainPred=}\KeywordTok{predict}\NormalTok{(knnFit,training[,}\OperatorTok{-}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\hypertarget{assessing-the-performance-of-our-model}{%
\section{Assessing the performance of our model}\label{assessing-the-performance-of-our-model}}

We have to define some metrics to see if our model worked. The algorithm is trying to reduce the classification error, or in other words it is trying to increase the training accuracy. For the assessment of performance, there are other different metrics to consider. All the metrics for 2-class classification depend on the table below, which shows the number of true positives (TP), false positives (FP), true negatives (TN) and false negatives (FN), similar to a table we used in the hypothesis testing section in the statistics chapter previously.

\begin{longtable}[]{@{}lcc@{}}
\toprule
\begin{minipage}[b]{0.20\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.27\columnwidth}\centering
Actual CIMP\strut
\end{minipage} & \begin{minipage}[b]{0.28\columnwidth}\centering
Actual noCIMP\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.20\columnwidth}\raggedright
Predicted as
CIMP\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\centering
True Positives (TP)\strut
\end{minipage} & \begin{minipage}[t]{0.28\columnwidth}\centering
False Positive (FP)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.20\columnwidth}\raggedright
Predicted as
noCIMP\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\centering
False Positives (FN)\strut
\end{minipage} & \begin{minipage}[t]{0.28\columnwidth}\centering
True negatives (TN)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Accuracy is the first metric to look at. This metric is simply
\((TP+TN)/(TP+TN+FP+FN)\) and shows the proportion of times we were right. There are other accuracy metrics that are important and output by \texttt{caret} functions. We will go over some of them here.\index{accuracy}

Precision, \(TP/(TP+FP)\), is about the confidence we have on our CIMP calls. If our method is very precise, we will have low false positives. That means every time we call a CIMP event, we would be relatively certain it is not a false positive.\index{precision}

Sensitivity, \(TP/(TP+FN)\), is how often we miss CIMP cases and call them as noCIMP. Making fewer mistakes in noCIMP cases will increase our sensitivity. You can think of sensitivity also in a sick/healthy context. A highly sensitive method will be good at classifying sick people when they are indeed sick.\index{sensitivity}

Specificity, \(TN/(TN+FP)\), is about how sure we are when we call something noCIMP. If our method is not very specific, we would call many patients CIMP, while in fact, they did not have the subtype. In the sick/healthy context, a highly specific method will be good at not calling healthy people sick.\index{specificity}

An alternative to accuracy we showed earlier is ``balanced accuracy''. Accuracy does not perform well when classes have very different numbers of samples (class imbalance). For example, if you have 90 CIMP cases and 10 noCIMP cases, classifying all the samples as CIMP gives 0.9 accuracy score by default. Using the ``balanced accuracy'' metric can help in such situations. This is simply \((Precision+Sensitivity)/2\). In this case above with the class imbalance scenario, the ``balanced accuracy'' would be 0.5. Another metric that takes into account accuracy that could be generated by chance is the ``Kappa statistic'' or ``Cohen's Kappa''. This metric includes expected accuracy, which is affected by class imbalance in the training set and provides a metric corrected by that.

In the k-NN example above, we trained and tested on the same data. The model returned the predicted labels for our training. We can calculate the accuracy metrics using the \texttt{caret::confusionMatrix()} function. This is sometimes called training accuracy. If you take \(1-accuracy\), it will be the ``training error''.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get k-NN prediction on the training data itself, with k=5}
\NormalTok{knnFit=}\KeywordTok{knn3}\NormalTok{(}\DataTypeTok{x=}\NormalTok{training[,}\OperatorTok{-}\DecValTok{1}\NormalTok{], }\CommentTok{# training set}
            \DataTypeTok{y=}\NormalTok{training[,}\DecValTok{1}\NormalTok{], }\CommentTok{# training set class labels}
            \DataTypeTok{k=}\DecValTok{5}\NormalTok{)}

\CommentTok{# predictions on the training set}
\NormalTok{trainPred=}\KeywordTok{predict}\NormalTok{(knnFit,training[,}\OperatorTok{-}\DecValTok{1}\NormalTok{],}\DataTypeTok{type=}\StringTok{"class"}\NormalTok{)}

\CommentTok{# compare the predicted labels to real labels}
\CommentTok{# get different performance metrics}
\KeywordTok{confusionMatrix}\NormalTok{(}\DataTypeTok{data=}\NormalTok{training[,}\DecValTok{1}\NormalTok{],}\DataTypeTok{reference=}\NormalTok{trainPred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction CIMP noCIMP
##     CIMP     65      0
##     noCIMP    2     63
##                                           
##                Accuracy : 0.9846          
##                  95% CI : (0.9455, 0.9981)
##     No Information Rate : 0.5154          
##     P-Value [Acc > NIR] : <2e-16          
##                                           
##                   Kappa : 0.9692          
##                                           
##  Mcnemar's Test P-Value : 0.4795          
##                                           
##             Sensitivity : 0.9701          
##             Specificity : 1.0000          
##          Pos Pred Value : 1.0000          
##          Neg Pred Value : 0.9692          
##              Prevalence : 0.5154          
##          Detection Rate : 0.5000          
##    Detection Prevalence : 0.5000          
##       Balanced Accuracy : 0.9851          
##                                           
##        'Positive' Class : CIMP            
## 
\end{verbatim}

Now, let us see what our test set accuracy looks like again using the \texttt{knn} function and the \texttt{confusionMatrix()} function on the predicted and real classes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# predictions on the test set, return class labels}
\NormalTok{testPred=}\KeywordTok{predict}\NormalTok{(knnFit,testing[,}\OperatorTok{-}\DecValTok{1}\NormalTok{],}\DataTypeTok{type=}\StringTok{"class"}\NormalTok{)}

\CommentTok{# compare the predicted labels to real labels}
\CommentTok{# get different performance metrics}
\KeywordTok{confusionMatrix}\NormalTok{(}\DataTypeTok{data=}\NormalTok{testing[,}\DecValTok{1}\NormalTok{],}\DataTypeTok{reference=}\NormalTok{testPred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction CIMP noCIMP
##     CIMP     27      0
##     noCIMP    2     25
##                                           
##                Accuracy : 0.963           
##                  95% CI : (0.8725, 0.9955)
##     No Information Rate : 0.537           
##     P-Value [Acc > NIR] : 2.924e-12       
##                                           
##                   Kappa : 0.9259          
##                                           
##  Mcnemar's Test P-Value : 0.4795          
##                                           
##             Sensitivity : 0.9310          
##             Specificity : 1.0000          
##          Pos Pred Value : 1.0000          
##          Neg Pred Value : 0.9259          
##              Prevalence : 0.5370          
##          Detection Rate : 0.5000          
##    Detection Prevalence : 0.5000          
##       Balanced Accuracy : 0.9655          
##                                           
##        'Positive' Class : CIMP            
## 
\end{verbatim}

Test set accuracy is not as good as the training accuracy, which is usually the case. That is why the best way to evaluate performance is to use test data that is not used by the model for training. That gives you an idea about real-world performance where the model will be used to predict data that is not previously seen.

\hypertarget{receiver-operating-characteristic-roc-curves}{%
\subsection{Receiver Operating Characteristic (ROC) curves}\label{receiver-operating-characteristic-roc-curves}}

One important and popular metric when evaluating performance is looking at receiver operating characteristic (ROC) curves.\index{receiver operating characteristic (ROC) curve} The ROC curve is created by evaluating the class probabilities for the model across a continuum of thresholds. Typically, in the case of two-class classification, the methods return a probability for one of the classes. If that probability is higher than \(0.5\), you call the label, for example, class A. If less than \(0.5\), we call the label class B. However, we can move that threshold and change what we call class A or B. For each candidate threshold, the resulting sensitivity and 1-specificity are plotted against each other. The best possible prediction would result in a point in the upper left corner, representing 100\% sensitivity (no false negatives) and 100\% specificity (no false positives). For the best model, the curve will be almost like a square. Since this is important information, area under the curve (AUC) is \index{area under the curve (AUC)}calculated. This is a quantity between 0 and 1, and the closer to 1, the better the performance of your classifier in terms of sensitivity and specificity. For an uninformative classification model, AUC will be \(0.5\). Although, ROC curves are initially designed for two-class problems, later extensions made it possible to use ROC curves for multi-class problems.

ROC curves can also be used to determine alternate cutoffs for class probabilities for two-class problems. However, this will always result in a trade-off between sensitivity and specificity. Sometimes it might be desirable to limit the number of false positives because making such mistakes would be too costly for the individual cases. For example, if predicted with a certain disease, you might be recommended to have surgery. However, if your classifier has a relatively high false positive rate, low specificity, you might have surgery for no reason. Typically, you want your classification model to have high specificity and sensitivity, which may not always be possible in the real world. You might have to choose what is more important for a specific problem and try to increase that.

Next, we will show how to use ROC curves for our k-NN application\index{k-nearest neighbors (k-NN)}. The method requires classification probabilities in the format where 0 probability denotes class ``noCIMP'' and probability 1 denotes class ``CIMP''. This way the ROC curve can be drawn by varying the probability cutoff for calling class a ``noCIMP'' or ``CIMP''. Below we are getting a similar probability from k-NN, but we have to transform it to the format we described above. Then, we feed those class probabilities to the \texttt{pROC::roc()} function to calculate the ROC curve and the area-under-the-curve. The resulting ROC curve is shown in Figure \ref{fig:ROC}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(pROC)}

\CommentTok{# get k-NN class probabilities}
\CommentTok{# prediction probabilities on the test set}
\NormalTok{testProbs=}\KeywordTok{predict}\NormalTok{(knnFit,testing[,}\OperatorTok{-}\DecValTok{1}\NormalTok{])}

\CommentTok{# get the roc curve}
\NormalTok{rocCurve <-}\StringTok{ }\NormalTok{pROC}\OperatorTok{::}\KeywordTok{roc}\NormalTok{(}\DataTypeTok{response =}\NormalTok{ testing[,}\DecValTok{1}\NormalTok{],}
                \DataTypeTok{predictor =}\NormalTok{ testProbs[,}\DecValTok{1}\NormalTok{],}
              \CommentTok{## This function assumes that the second}
              \CommentTok{## class is the class of interest, so we}
              \CommentTok{## reverse the labels.}
              \DataTypeTok{levels =} \KeywordTok{rev}\NormalTok{(}\KeywordTok{levels}\NormalTok{(testing[,}\DecValTok{1}\NormalTok{])))}
\CommentTok{# plot the curve}
\KeywordTok{plot}\NormalTok{(rocCurve, }\DataTypeTok{legacy.axes =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{05-supervisedLearning_files/figure-latex/ROC-1} 

}

\caption{ROC curve for k-NN.}\label{fig:ROC}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# return area under the curve}
\NormalTok{pROC}\OperatorTok{::}\KeywordTok{auc}\NormalTok{(rocCurve)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Area under the curve: 0.976
\end{verbatim}

\hypertarget{model-tuning-and-avoiding-overfitting}{%
\section{Model tuning and avoiding overfitting}\label{model-tuning-and-avoiding-overfitting}}

How can we know that we picked the best \(k\)? One straightforward way is that we can try many different \(k\) values and check the accuracy of our model. We will first check the effect of different \(k\) values on training accuracy. Below, we will go through many \(k\) values and calculate the training accuracy for each.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}
\NormalTok{k=}\DecValTok{1}\OperatorTok{:}\DecValTok{12} \CommentTok{# set k values}
\NormalTok{trainErr=}\KeywordTok{c}\NormalTok{() }\CommentTok{# set vector for training errors}
\ControlFlowTok{for}\NormalTok{( i }\ControlFlowTok{in}\NormalTok{ k)\{}
  
\NormalTok{  knnFit=}\KeywordTok{knn3}\NormalTok{(}\DataTypeTok{x=}\NormalTok{training[,}\OperatorTok{-}\DecValTok{1}\NormalTok{], }\CommentTok{# training set}
              \DataTypeTok{y=}\NormalTok{training[,}\DecValTok{1}\NormalTok{], }\CommentTok{# training set class labels}
              \DataTypeTok{k=}\NormalTok{i)}

  \CommentTok{# predictions on the training set}
\NormalTok{  class.res=}\KeywordTok{predict}\NormalTok{(knnFit,training[,}\OperatorTok{-}\DecValTok{1}\NormalTok{],}\DataTypeTok{type=}\StringTok{"class"}\NormalTok{)}

  \CommentTok{# training error}
\NormalTok{  err=}\DecValTok{1}\OperatorTok{-}\KeywordTok{confusionMatrix}\NormalTok{(training[,}\DecValTok{1}\NormalTok{],class.res)}\OperatorTok{$}\NormalTok{overall[}\DecValTok{1}\NormalTok{]}
\NormalTok{  trainErr[i]=err}
\NormalTok{\}}

\CommentTok{# plot training error vs k}
\KeywordTok{plot}\NormalTok{(k,trainErr,}\DataTypeTok{type=}\StringTok{"p"}\NormalTok{,}\DataTypeTok{col=}\StringTok{"#CC0000"}\NormalTok{,}\DataTypeTok{pch=}\DecValTok{20}\NormalTok{)}

\CommentTok{# add a smooth line for the trend}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{loess.smooth}\NormalTok{(}\DataTypeTok{x=}\NormalTok{k, trainErr,}\DataTypeTok{degree=}\DecValTok{2}\NormalTok{),}\DataTypeTok{col=}\StringTok{"#CC0000"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{05-supervisedLearning_files/figure-latex/trainingErrork-1} 

}

\caption{Training error for k-NN classification of glioma tumor samples.}\label{fig:trainingErrork}
\end{figure}

The resulting training error plot is shown in Figure \ref{fig:trainingErrork}. We can see the effect of \(k\) in the training error; as \(k\) increases the model tends to be a bit worse on training. This makes sense because with large \(k\) we take into account more and more neighbors, and at some point we start considering data points from the other classes as well and that decreases our accuracy.

However, looking at the training accuracy is not the right way to test the model as we have mentioned. The models are generally tested on the datasets that are not used when building model. There are different strategies to do this. We have already split part of our dataset as test set, so let us see how we do it on the test data using the code below. The resulting plot is shown in Figure \ref{fig:testTrainErr}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{31}\NormalTok{)}
\NormalTok{k=}\DecValTok{1}\OperatorTok{:}\DecValTok{12}
\NormalTok{testErr=}\KeywordTok{c}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{( i }\ControlFlowTok{in}\NormalTok{ k)\{}

\NormalTok{  knnFit=}\KeywordTok{knn3}\NormalTok{(}\DataTypeTok{x=}\NormalTok{training[,}\OperatorTok{-}\DecValTok{1}\NormalTok{], }\CommentTok{# training set}
              \DataTypeTok{y=}\NormalTok{training[,}\DecValTok{1}\NormalTok{], }\CommentTok{# training set class labels}
              \DataTypeTok{k=}\NormalTok{i)}

  \CommentTok{# predictions on the training set}
\NormalTok{  class.res=}\KeywordTok{predict}\NormalTok{(knnFit,testing[,}\OperatorTok{-}\DecValTok{1}\NormalTok{],}\DataTypeTok{type=}\StringTok{"class"}\NormalTok{)}
\NormalTok{  testErr[i]=}\DecValTok{1}\OperatorTok{-}\KeywordTok{confusionMatrix}\NormalTok{(testing[,}\DecValTok{1}\NormalTok{],}
\NormalTok{                                 class.res)}\OperatorTok{$}\NormalTok{overall[}\DecValTok{1}\NormalTok{]}
 
\NormalTok{\}}

\CommentTok{# plot training error}
\KeywordTok{plot}\NormalTok{(k,trainErr,}\DataTypeTok{type=}\StringTok{"p"}\NormalTok{,}\DataTypeTok{col=}\StringTok{"#CC0000"}\NormalTok{,}
     \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\FloatTok{0.000}\NormalTok{,}\FloatTok{0.08}\NormalTok{),}
     \DataTypeTok{ylab=}\StringTok{"prediction error (1-accuracy)"}\NormalTok{,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{)}
\CommentTok{# add a smooth line for the trend}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{loess.smooth}\NormalTok{(}\DataTypeTok{x=}\NormalTok{k, trainErr,}\DataTypeTok{degree=}\DecValTok{2}\NormalTok{), }\DataTypeTok{col=}\StringTok{"#CC0000"}\NormalTok{)}

\CommentTok{# plot test error}
\KeywordTok{points}\NormalTok{(k,testErr,}\DataTypeTok{col=}\StringTok{"#00CC66"}\NormalTok{,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{) }
\KeywordTok{lines}\NormalTok{(}\KeywordTok{loess.smooth}\NormalTok{(}\DataTypeTok{x=}\NormalTok{k,testErr,}\DataTypeTok{degree=}\DecValTok{2}\NormalTok{), }\DataTypeTok{col=}\StringTok{"#00CC66"}\NormalTok{)}
\CommentTok{# add legend}
\KeywordTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{,}\DataTypeTok{fill=}\KeywordTok{c}\NormalTok{(}\StringTok{"#CC0000"}\NormalTok{,}\StringTok{"#00CC66"}\NormalTok{),}
       \DataTypeTok{legend=}\KeywordTok{c}\NormalTok{(}\StringTok{"training"}\NormalTok{,}\StringTok{"test"}\NormalTok{),}\DataTypeTok{bty=}\StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{05-supervisedLearning_files/figure-latex/testTrainErr-1} 

}

\caption{Training and test error for k-NN classification of glioma tumor samples.}\label{fig:testTrainErr}
\end{figure}

The test data show a different thing, of course. It is not the best strategy to increase the \(k\) indefinitely. The test error rate increases after a while. Increasing \(k\) results in too many data points influencing the decision about the class of the new sample, this may not be desirable since this strategy might include points from other classes eventually. On the other hand, if we set \(k\) too low, we are restricting the model to only look for a few neighbors.

In addition, \(k\) values that give the best performance for the training set are not the best \(k\) for the test set. In fact, if we stick with \(k=1\) as the best \(k\) obtained from the training set, we would obtain a worse performance on the test set. In this case, we can talk about the concept of overfitting. This happens when our models fit the data in the training set extremely well but cannot perform well in the test data; in other words, they cannot generalize. Similarly, underfitting could occur when our models do not learn well from the training data and they are overly simplistic. Ideally, we should use methods that help us estimate the real test error when tuning the models such as cross-validation, bootstrap or holdout test set.

\hypertarget{model-complexity-and-bias-variance-trade-off}{%
\subsection{Model complexity and bias variance trade-off}\label{model-complexity-and-bias-variance-trade-off}}

The case of over- and underfitting is closely related to the model complexity and the related bias-variance trade-off.\index{overfitting} We will introduce these concepts now. First, let us point out that prediction error depends on the real value of the class label of the test case and predicted value. The test case label or value is not dependent on the prediction; the only thing that is variable here is the model. Therefore, if we could train multiple models with different data sets for the same problem, our predictions for the test set would vary. That means our prediction error would also vary. Now, with this setting we can talk about expected prediction error for a given machine learning model. This is the average error you would get for a test set if you were able to train multiple models. This expected prediction error can largely be decomposed into the variability of the predictions due to the model variability (variance) and the difference between the expected prediction values and the correct value of the response (bias). Formally, the expected prediction error, \(E[Error]\) is decomposed as follows:

\[
 E[Error]=Bias^2 + Variance + \sigma_e^2
\]
Note that in the above equation \(\sigma_e^2\) is the irreducible error. This is the noise term that cannot fundamentally be accounted for by any model. The bias is formally the difference between the expected prediction value and the correct response value, \(Y\): \(Bias=(Y-E[PredictedValue])\). The variance is simply the variability of the prediction values when we construct models multiple times with different training sets for the same problem: \(Variance=E[(PredictedValue-E[PredictedValue])^2]\). Note that this value of the variance does not depend of the correct value of the test cases.

The models that have high variance are generally more complex models that have many knobs or parameters than can fit the training data well. These models, due to their flexibility, can fit training data too much that it creates poor prediction performance in a new data set. On the other hand, simple, less complex models do not have the flexibility to fit every data set that well, so they can avoid overfitting. However, they can underfit if they are not flexible enough to model or at least approximate the true relationship between predictors and the response variable. The bias term is mostly about the general model performance (expected or average value of predictions) that can be attributed to approximating a real-life problem with simpler models. These simple models can have less variability in their predictions, so the prediction error will be mostly composed of the bias term.

In reality, there is always a trade-off between bias and variance (See Figure \ref{fig:varBias}). Increasing the variance with complex models will decrease the bias, but that might overfit. Conversely, simple models will increase the bias at the expense of the model variance, and that might underfit. There is an optimal point for model complexity, a balance between overfitting and underfitting.\index{overfitting} \index{underfitting} In practice, there is no analytical way to find this optimal complexity. Instead we must use an accurate measure of prediction error and explore different levels of model complexity and choose the complexity level that minimizes the overall error. Another approach to this is to use ``the one standard error rule''. Instead of choosing the parameter that minimizes the error estimate, we can choose the simplest model whose error estimate is within one standard error of the best model (see Chapter 7 of \citep{friedman2001elements}). The rationale behind that is to choose a simple model with the hope that it would perform better in the unseen data since its performance is not different from the best model in a statistically significant way. You might see the option to choose the ``one-standard-error'' model in some machine learning packages.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/Variance-bias} 

}

\caption{Variance-bias trade-off visualized as components of total prediction error in relation to model complexity.}\label{fig:varBias}
\end{figure}

In our k-NN example\index{k-nearest neighbors (k-NN)}, lower \(k\) values create a more flexible model. This might be counterintuitive, but as we have explained before having small \(k\) values will fit the data in a very data-specific manner. It will probably not generalize well. Therefore in this respect, lower \(k\) values will result in more complex models with high variance\index{model complexity}. On the other hand, higher \(k\) values will result in less variance but higher bias. Figure \ref{fig:kNNboundary} shows the decision boundary for two different k-NN models with \(k=2\) and \(k=12\). To be able to plot this in 2D we ran the model on principal component 1 and 2 of the training data set, and predicted the class label of many points in this 2D space. As you can see, \(k=2\) creates a more variable model which tries aggressively to include all training samples in the correct class. This creates a high-variance model because the model could change drastically from dataset to dataset. On the other hand, setting \(k=12\) creates a model with a smoother decision boundary. This model will have less variance since it considers many points for a decision, and therefore the decision boundary is smoother.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/knnDecisionBoundPCA} 

}

\caption{Decision boundary for different k values in k-NN models. k=12 creates a smooth decision boundary and ignores certain data points on either side of the boundary. k=2 is less smooth and more variable.}\label{fig:kNNboundary}
\end{figure}

\hypertarget{data-split-strategies-for-model-tuning-and-testing}{%
\subsection{Data split strategies for model tuning and testing}\label{data-split-strategies-for-model-tuning-and-testing}}

The data split strategy is essential for accurate prediction of the test error. As we have seen in the model complexity/bias-variance discussion\index{model complexity}, estimating the prediction error is central for model tuning in order to find the model with the right complexity. Therefore, we will revisit this and show how to build and test models, and measure their prediction error in practice.

\hypertarget{training-validation-test}{%
\subsubsection{Training-validation-test}\label{training-validation-test}}

This data split strategy creates three partitions of the dataset, training, validation, and test sets. In this strategy, the training set is used to train the data and the validation set is used to tune the model to the best possible model. The final partition, ``test'', is only used for the final test and should not be used to tune the model. This is regarded as the real-world prediction error for your model. This strategy works when you have a lot of data to do a three-way split. The test set we used above is most likely too small to measure the prediction error with just using a test set. In such cases, bootstrap or cross-validation should yield more stable results.

\hypertarget{cross-validation-1}{%
\subsubsection{Cross-validation}\label{cross-validation-1}}

A more realistic approach when you do not have a lot of data to do the three-way split is cross-validation. You can use\index{cross-validation} cross-validation in the model-tuning phase as well, instead of going with a single train-validation split. As with the three-way split, the final prediction error could be estimated with the test set. In other words, we can separate 80\% of the data for model building with cross-validation, and the final model performance will be measured on the test set.

We have already split our glioma dataset into training and test sets. Now, we will show how to run a k-NN \index{k-nearest neighbors (k-NN)}model with cross-validation using the \texttt{caret::train()} function. This function will use cross-validation to train models for different \(k\) values. Every \(k\) value will be trained and tested with cross-validation to estimate prediction performance for each \(k\). We will then plot the cross-validation error and the resulting plot is shown in Figure \ref{fig:kknCv}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{17}\NormalTok{)}
\CommentTok{# this method controls everything about training}
\CommentTok{# we will just set up 10-fold cross validation}
\NormalTok{trctrl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{,}\DataTypeTok{number=}\DecValTok{10}\NormalTok{)}

\CommentTok{# we will now train k-NN model}
\NormalTok{knn_fit <-}\StringTok{ }\KeywordTok{train}\NormalTok{(subtype}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ training, }
                 \DataTypeTok{method =} \StringTok{"knn"}\NormalTok{,}
                 \DataTypeTok{trControl=}\NormalTok{trctrl,}
                 \DataTypeTok{tuneGrid =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{k=}\DecValTok{1}\OperatorTok{:}\DecValTok{12}\NormalTok{))}

\CommentTok{# best k value by cross-validation accuracy}
\NormalTok{knn_fit}\OperatorTok{$}\NormalTok{bestTune}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   k
## 4 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot k vs prediction error}
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x=}\DecValTok{1}\OperatorTok{:}\DecValTok{12}\NormalTok{,}\DecValTok{1}\OperatorTok{-}\NormalTok{knn_fit}\OperatorTok{$}\NormalTok{results[,}\DecValTok{2}\NormalTok{],}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}
     \DataTypeTok{ylab=}\StringTok{"prediction error"}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"k"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{loess.smooth}\NormalTok{(}\DataTypeTok{x=}\DecValTok{1}\OperatorTok{:}\DecValTok{12}\NormalTok{,}\DecValTok{1}\OperatorTok{-}\NormalTok{knn_fit}\OperatorTok{$}\NormalTok{results[,}\DecValTok{2}\NormalTok{],}\DataTypeTok{degree=}\DecValTok{2}\NormalTok{),}
      \DataTypeTok{col=}\StringTok{"#CC0000"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{05-supervisedLearning_files/figure-latex/kknCv-1} 

}

\caption{Cross-validated estimate of prediction error of k in k-NN models.}\label{fig:kknCv}
\end{figure}

Based on Figure \ref{fig:kknCv} the cross-validation accuracy reveals that \(k=5\) is the best \(k\) value. On the other hand, we can also try bootstrap resampling and check the prediction error that way. We will again use the \texttt{caret::trainControl()} function to do the bootstrap sampling and estimate OOB-based error. However, for a small number of samples like we have in our example, the difference between the estimated and the true value of the prediction error can be large. Below we show how to use bootstrapping for the k-NN model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{17}\NormalTok{)}
\CommentTok{# this method controls everything about training}
\CommentTok{# we will just set up 100 bootstrap samples and for each }
\CommentTok{# bootstrap OOB samples to test the error}
\NormalTok{trctrl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"boot"}\NormalTok{,}\DataTypeTok{number=}\DecValTok{20}\NormalTok{,}
                       \DataTypeTok{returnResamp=}\StringTok{"all"}\NormalTok{)}

\CommentTok{# we will now train k-NN model}
\NormalTok{knn_fit <-}\StringTok{ }\KeywordTok{train}\NormalTok{(subtype}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ training, }
                 \DataTypeTok{method =} \StringTok{"knn"}\NormalTok{,}
                 \DataTypeTok{trControl=}\NormalTok{trctrl,}
                 \DataTypeTok{tuneGrid =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{k=}\DecValTok{1}\OperatorTok{:}\DecValTok{12}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{variable-importance}{%
\section{Variable importance}\label{variable-importance}}

Another important purpose of machine learning models could be to learn which variables are more important for the prediction. This information could lead to potential biological insights or could help design better data collection methods or experiments.\index{variable importance}

Variable importance metrics can be separated into two groups: those that are model dependent and those that are not. Many machine-learning methods come with built-in variable importance measures. These may be able to incorporate the correlation structure between the predictors into the importance calculation. Model-independent methods are not able to use any internal model data. We will go over some model-independent strategies below. The model-dependent importance measures will be mentioned when we introduce machine learning methods that have built-in variable importance measures.

One simple method for variable importance is to correlate or apply statistical tests to test the association of the predictor variable with the response variable. Variables can be ranked based on the strength of those associations. For classification problems, ROC curves can be computed by thresholding the predictor variable, and for each variable an AUC can be computed. The variables can be ranked based on these values. However, these methods completely ignore how variables would behave in the presence of other variables. The \texttt{caret::filterVarImp()} function implements some of these strategies.

If a variable is important for prediction, removing that variable before model training will cause a drop in performance. With this understanding, we can remove the variables one by one and train models without them and rank them by the loss of performance. The most important variables must cause the largest loss of performance. This strategy requires training and testing models as many times as the number of predictor variables. This will consume a lot of time. A related but more practical approach has been put forward to measure variable importance in a model-independent manner but without re-training \citep{dalex, mcr}. In this case, instead of removing the variables at training, variables are permuted at the test phase. The loss in prediction performance is calculated by comparing the labels/values from the original response variable to the labels/values obtained by running the permuted test data through the model. This is called ``variable dropout loss''. In this case, we are not really dropping out variables, but by permuting them, we destroy their relationship to the response variable. The dropout loss is compared to the ``worst case'' scenario where the response variable is permuted and compared against the original response variables, which is called ``baseline loss''. The algorithm ranks the variables by their variable dropout loss or by their ratio of variable dropout to baseline loss. Both quantities are proportional but the second one contains information about the baseline loss.
\index{variable importance}
Below, we run the \texttt{DALEX::explain()} function to do the permutation drop-out strategy for the variables. The function needs the machine learning model, and new data and its labels to do the permutation-based dropout strategy. In this case, we are feeding the function with the data we used for training.
For visualization we can use the \texttt{DALEX::feature\_importance()} function which plots the loss. Although, in this case we are not plotting the results. In the following sections, we will discuss method-specific variable importance measures.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(DALEX)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{102}\NormalTok{)}
\CommentTok{# do permutation drop-out}
\NormalTok{explainer_knn<-}\StringTok{ }\NormalTok{DALEX}\OperatorTok{::}\KeywordTok{explain}\NormalTok{(knn_fit, }
                               \DataTypeTok{label=}\StringTok{"knn"}\NormalTok{, }
                               \DataTypeTok{data =}\NormalTok{training[,}\OperatorTok{-}\DecValTok{1}\NormalTok{], }
                               \DataTypeTok{y =} \KeywordTok{as.numeric}\NormalTok{(training[,}\DecValTok{1}\NormalTok{]))}

\NormalTok{viknn=}\KeywordTok{feature_importance}\NormalTok{(explainer_knn,}\DataTypeTok{n_sample=}\DecValTok{50}\NormalTok{,}\DataTypeTok{type=}\StringTok{"difference"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(viknn)}
\end{Highlighting}
\end{Shaded}

Although the variable drop-out strategy will still be slow if you have a lot of variables, the upside is that you can use any black-box model as long as you have access to the model to run new predictions. Later sections in this chapter will show methods with built-in variable importance metrics, since these are calculated during training it comes with less of an additional compute cost.

\hypertarget{how-to-deal-with-class-imbalance}{%
\section{How to deal with class imbalance}\label{how-to-deal-with-class-imbalance}}

A common hurdle in many applications of machine learning on genomic data is the large class imbalance. The imbalance refers to relative difference in the sizes of the groups being classified. For example, if we had class imbalance in our example data set we could have much more CIMP samples in the training than noCIMP samples, or the other way around. Another example with severe class imbalance would be enhancer prediction \citep{enhancerImbalance}. Depending on which training data set you use, you can have a couple of hundred to thousands of positive examples for enhancer locations in the human genome. In either case, the negative set, ``not enhancer'', will overwhelm the training, because the human genome is 3 billion base-pairs long and most of that does not overlap with an enhancer annotation. In whatever strategy you pick to build a negative set, it will contain many more data points than the positive set. As we have mentioned in the model performance section above, if we have a severe imbalance in the class sizes, the training algorithm may get better accuracy just by calling everything one class. This will be evident in specificity and sensitivity metrics, and the related balanced accuracy metric. Below, we will discuss a couple of techniques that might help when the training set has class imbalance.

\hypertarget{sampling-for-class-balance}{%
\subsection{Sampling for class balance}\label{sampling-for-class-balance}}

If we think class imbalance is a problem based on looking at the relative sizes of the classes and relevant accuracy metrics of a model, there are a couple of things that might help. First, we can try sampling or ``stratified'' sampling when we are constructing our training set. This simply means that before training we can we build the classification model with samples of the data so we have the same size classes. This could be down-sampling the classes with too many data points. For this purpose, you can simply use the \texttt{sample()} or \texttt{caret::downSample()} function and create your training set prior to modeling. In addition, the minority class could be up-sampled for the missing number of data points using sampling with replacement similar to bootstrap sampling with the \texttt{caret::upSample()} function. There are more advanced up-sampling methods such as the synthetic up-sampling method SMOTE \citep{smote}. In this method, each data point from the minority class is up-sampled synthetically by adding variability to the predictor variable vector from one of the k-nearest neighbors of the data point. Specifically, one neighbor is randomly chosen and the difference between predictor variables of the neighbor and the original data point is added to the original predictor variables after multiplying the difference values with a random number between \(0\) and \(1\). This creates synthetic data points that are similar to original data points but not identical. This method and other similar methods of synthetic sampling are available at \href{https://cran.r-project.org/web/packages/smotefamily/index.html}{\texttt{smotefamily}} package \index{R Packages!\texttt{smotefamily}} in CRAN.

In addition to the strategies above, some methods can do sampling during training to cope with the effects of class imbalance. For example, random forests has a sampling step during training, and this step can be altered to do stratified sampling. We will be introducing random forests later in the chapter.

However, even if we are doing the sampling on the training set to avoid problems, the test set proportions should have original class label proportions to evaluate the performance in a real-world situation.

\hypertarget{altering-case-weights}{%
\subsection{Altering case weights}\label{altering-case-weights}}

For some methods, we can use different case weights proportional to the imbalance suffered by the minority class. This means cases from the minority class will have higher case weights, which causes an effect as if we are up-sampling the minority class. Logistic regression-based methods and boosting methods are examples of algorithms that can utilize case weights, both of which will be introduced later.

\hypertarget{selecting-different-classification-score-cutoffs}{%
\subsection{Selecting different classification score cutoffs}\label{selecting-different-classification-score-cutoffs}}

Another simple approach for dealing with class imbalance is to select a prediction score cutoff that minimizes the excess true positives or false positives depending on the direction of the class imbalance. This can simply be done using ROC curves. For example, the classical prediction cutoff for a 2-class classification problems is 0.5. We can alter this cutoff to optimize sensitivity and specificity.

\hypertarget{dealing-with-correlated-predictors}{%
\section{Dealing with correlated predictors}\label{dealing-with-correlated-predictors}}

Highly correlated predictors can lead to collinearity issues and this can greatly increase the model variance, especially in the context of regression. In some cases, there could be relationships between multiple predictor variables and this is called multicollinearity. Having correlated variables will result in unnecessarily complex models with more than necessary predictor variables. From a data collection point of view, spending time and money for collecting correlated variables could be a waste of effort. In terms of linear regression or the models that are based on regression, the collinearity problem is more severe because it creates unstable models where statistical inference becomes difficult or unreliable. On the other hand, correlation between variables may not be a problem for the predictive performance if the correlation structure in the training and the future tests data sets are the same. However, more often, correlated structures within the training set might lead to overfitting.

Here are couple of things to do if collinearity \index{collinearity} is a problem:

\begin{itemize}
\item
  We can do PCA on the training data, which creates new variables removing the collinearity between them. We can then train models on these new dimensions. The downside is that it is harder to interpret these variables. They are now linear combinations of original variables. The variable importance would be harder to interpret.
\item
  As we have already shown in the data preprocessing section, we can try variable filtering and reduce the number of correlated variables. However, this may not address the multicollinearity issue where linear combinations of variables might be correlated while they are not directly correlated themselves.
\item
  Method-specific techniques such as regularization \index{regularization}can decrease the effects of collinearity. Regularization, as we will see in the later chapter, is a technique that is used to prevent overfitting and it can also dampen the effects of collinearity. In addition, decision-tree-based methods could suffer less from the effects of collinearity.
\end{itemize}

\hypertarget{trees-and-forests-random-forests-in-action}{%
\section{Trees and forests: Random forests in action}\label{trees-and-forests-random-forests-in-action}}

\hypertarget{decision-trees}{%
\subsection{Decision trees}\label{decision-trees}}

Decision trees are a popular method for various machine learning tasks mostly because their interpretability is very high. A decision tree is a series of filters on the predictor variables. The series of filters end up in a class prediction. Each filter is a binary yes/no question, which creates bifurcations in the series of filters thus leading to a treelike structure. The filters are dependent on the type of predictor variables. If the variables are categorical, such as gender, then the filters could be ``is gender female'' type of questions. If the variables are continuous, such as gene expression, the filter could be ``is PIGX expression larger than 210?''. Every point where we filter samples based on these questions are called ``decision nodes''. The tree-fitting algorithm finds the best variables at decision nodes depending on how well they split the samples into classes after the application of the decision node. Decision trees handle both categorical and numeric predictor variables, they are easy to interpret, and they can deal with missing variables. Despite their advantages, decision trees tend to overfit if they are grown very deep and can learn irregular patterns.
\index{decision tree}
There are many variants of tree-based machine learning algorithms. However, most algorithms construct decision nodes in a top down manner. They select the best variables to use in decision nodes based on how homogeneous the sample sets are after the split. One measure of homogeneity is ``Gini impurity''. This measure is calculated for each subset after the split and later summed up as a weighted average. For a decision node that splits the data perfectly in a two-class problem, the gini impurity will be \(0\), and for a node that splits the data into a subset that has 50\% class A and 50\% class B the impurity will be \(0.5\). Formally, the gini impurity, \({I}_{G}(p)\), of a set of samples with known class labels for \(K\) classes is the following, where \(p_{i}\) is the probability of observing class \(i\) in the subset:

\[
{\displaystyle {I}_{G}(p)=\sum _{i=1}^{K}p_{i}(1-p_{i})=\sum _{i=1}^{K}p_{i}-\sum _{i=1}^{K}{p_{i}}^{2}=1-\sum _{i=1}^{K}{p_{i}}^{2}}
\]

For example, if a subset of data after split has 75\% class A and 25\% class B for that subset, the impurity would be \(1-(0.75^2+0.25^2)=0.375\). If the other subset had 5\% class A and 95\% class B, its impurity would be \(1-(0.95^2+0.05^2)=0.095\). If the subset sizes after the split were equal, total weighted impurity would be \(0.5*0.375+0.5*0.095= 0.235\). These calculations will be done for each potential variable and the split, and every node will be constructed based on gini impurity decrease. If the variable is continuous, the cutoff value will be decided based on the best impurity. For example, gene expression values will have splits such as ``PIGX expression \textless{} 2.1''. Here \(2.1\) is the cutoff value that produces the best impurity. There are other homogeneity measures, however gini impurity is the one that is used for random forests, which we will introduce next.

\hypertarget{trees-to-forests}{%
\subsection{Trees to forests}\label{trees-to-forests}}

Random forests are devised to counter the shortcomings of decision trees. They are simply ensembles of decision trees. Each tree is trained with a different randomly selected part of the data with randomly selected predictor variables. The goal of introducing randomness is to reduce the variance of the model so it does not overfit, at the expense of a small increase in the bias and some loss of interpretability. This strategy generally boosts the performance of the final model.\index{random forest}

The random forests algorithm tries to decorrelate the trees so that they learn different things about the data. It does this by selecting a random subset of variables. If one or a few predictor variables are very strong predictors for the response variable, these features will be selected in many of the trees, causing them to become correlated. Random subsampling of predictor variables ensures that not always the best predictors overall are selected for every tree and, the model does
have a chance to learn other features of the data.

Another sampling method introduced when building random forest models is bootstrap resampling before constructing each tree. This brings the advantage of out-of-the-bag (OOB) error prediction. In this case, the prediction error can be estimated for training samples that were OOB, meaning they were not used in the training, for some percentage of the trees. The prediction error for each sample can be estimated from the trees where that sample was OOB. OOB estimates claimed to be a good alternative to cross-validation estimated errors \citep{breiman2001random}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/ml-random-forest-features} 

}

\caption{Random forest concept. Individual decision trees are built with sampling strategies. Votes from each tree define the final class.}\label{fig:RFcartoon}
\end{figure}

For demonstration purposes, we will use the \texttt{caret} \index{R Packages!\texttt{caret}}package interface to the \texttt{ranger} random forest package\index{R Packages!\texttt{ranger}}. This is a fast implementation of the original random forest algorithm. For random forests, we have two critical arguments. One of the most critical arguments for random forest is the number of predictor variables to sample in each split of the tree. This parameter controls the independence between the trees, and as explained before, this limits overfitting. Below, we are going to fit a random forest model to our tumor subtype problem. We will set \texttt{mtry=100} and not perform the training procedure to find the best \texttt{mtry} value for simplicity. However, it is good practice
to run the model with cross-validation and let it pick the best parameters based on the cross-validation performance. It defaults to the square root of number of predictor variables. Another variable we can tune is the minimum node size of terminal nodes in the trees (\texttt{min.node.size}). This controls the depth of the trees grown. Setting this to larger numbers might cost a small loss in accuracy but the algorithm will run faster.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{17}\NormalTok{)}

\CommentTok{# we will do no resampling based prediction error}
\CommentTok{# although it is advised to do so even for random forests}
\NormalTok{trctrl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"none"}\NormalTok{)}

\CommentTok{# we will now train random forest model}
\NormalTok{rfFit <-}\StringTok{ }\KeywordTok{train}\NormalTok{(subtype}\OperatorTok{~}\NormalTok{., }
               \DataTypeTok{data =}\NormalTok{ training, }
               \DataTypeTok{method =} \StringTok{"ranger"}\NormalTok{,}
               \DataTypeTok{trControl=}\NormalTok{trctrl,}
               \DataTypeTok{importance=}\StringTok{"permutation"}\NormalTok{, }\CommentTok{# calculate importance}
               \DataTypeTok{tuneGrid =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{mtry=}\DecValTok{100}\NormalTok{,}
                                     \DataTypeTok{min.node.size =} \DecValTok{1}\NormalTok{,}
                                     \DataTypeTok{splitrule=}\StringTok{"gini"}\NormalTok{)}
\NormalTok{               )}
\CommentTok{# print OOB error}
\NormalTok{rfFit}\OperatorTok{$}\NormalTok{finalModel}\OperatorTok{$}\NormalTok{prediction.error}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.01538462
\end{verbatim}

\hypertarget{variable-importance-1}{%
\subsection{Variable importance}\label{variable-importance-1}}

Random forests come with built-in variable importance metrics. One of the metrics is similar to the ``variable dropout metric'' where the predictor variables are permuted. In this case, OOB samples are used and the variables are permuted one at a time. Every time, the samples with the permuted variables are fed to the network and the decrease in accuracy is measured. Using this quantity, the variables can be ranked. \index{variable importance}

A less costly method with similar performance is to use gini impurity. Every time a variable is used in a tree to make a split, the gini impurity is less than the parent node. This method adds up these gini impurity decreases for each individual variable across the trees and divides it by the number of the trees in the forest. This metric is often consistent with the permutation importance measure \citep{breiman2001random}. Below, we are going to plot the permutation-based importance metric. This metric has been calculated during the run of the model above. We will use the \texttt{caret::varImp()} function to access the importance values and plot them using the \texttt{plot()} function; the result is shown in Figure \ref{fig:RFvarImp}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{varImp}\NormalTok{(rfFit),}\DataTypeTok{top=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{05-supervisedLearning_files/figure-latex/RFvarImp-1} 

}

\caption{Top 10 important variables based on permutation-based method for the random forest classification.}\label{fig:RFvarImp}
\end{figure}

\hypertarget{logistic-regression-and-regularization}{%
\section{Logistic regression and regularization}\label{logistic-regression-and-regularization}}

Logistic regression \index{logistic regression} is a statistical method that is used to model a binary response variable based on predictor variables. Although initially devised for two-class or binary response problems, this method can be generalized to multiclass problems. However, our example tumor sample data is a binary response or two-class problem, therefore we will not go into the multiclass case in this chapter.

Logistic regression is very similar to linear regression as a concept and it can be thought of as a ``maximum likelihood estimation'' problem where we are trying to find statistical parameters that maximize the likelihood of the observed data being sampled from the statistical distribution of interest. This is also very related to the general cost/loss function approach we see in supervised machine learning algorithms. In the case of binary response variables, the simple linear regression model, such as \(y_i \sim \beta _{0}+\beta _{1}x_i\), would be a poor choice because it can easily generate values outside of the \(0\) to \(1\) boundary. What we need is a
model that restricts the lower bound of the prediction to zero and an upper
bound to \(1\). The first thing towards this requirement is to formulate the problem differently. If \(y_i\) can only be \(0\) or \(1\), we can formulate \(y_i\) as a realization of a random variable that can take the values one and zero with probabilities \(p_i\) and \(1-{p_i}\), respectively. This random variable follows the Bernoulli distribution, and instead of predicting the binary variable we can formulate the problem as \(p_i \sim \beta _{0}+\beta _{1}x_i\). However, our initial problem still stands, simple linear regression will still result in values that are beyond \(0\) and \(1\) boundaries. A model that satisfies the boundary requirement is the logistic equation shown below.

\[
{\displaystyle p_i={\frac {e^{(\beta _{0}+\beta _{1}x_i)}}{1+e^{(\beta _{0}+\beta_{1}x_i)}}}}
\]

This equation can be linearized by the following transformation

\[
{\displaystyle \operatorname{logit} (p_i)=\ln \left({\frac {p_i}{1-p_i}}\right)=\beta _{0}+\beta _{1}x_i}
\]
The left-hand side is termed the logit, which stands for ``logistic unit''. It is also known as the log odds. In this case, our model will produce values on the log scale and with the logistic equation above, we can transform the values to the \(0-1\) range. Now, the question remains: ``What are the best parameter estimates for our training set''. Within the maximum likelihood framework\index{maximum likelihood estimation} we have touched upon in Chapter \ref{stats}, the best parameter estimates are the ones that maximize the likelihood of the statistical model actually producing the observed data. You can think of this fitting as a probability distribution to an observed data set. The parameters of the probability distribution should maximize the likelihood that the observed data came from the distribution in question. If we were using a Gaussian distribution we would change the mean and variance parameters until the observed data was more plausible to be drawn from that specific Gaussian distribution.

In logistic regression, \index{logistic regression}the response variable is modeled with a binomial distribution or its special case Bernoulli distribution. The value of each response variable, \(y_i\), is 0 or 1, and we need to figure out parameter \(p_i\) values that could generate such a distribution of 0s and 1s. If we can find the best \(p_i\) values for each tumor sample \(i\), we would be maximizing the log-likelihood function of the model over the observed data. The maximum log-likelihood function for our binary response variable case is shown as Equation \eqref{eq:logLik}.

\begin{equation}
 \operatorname{\ln} (L)=\sum_{i=1}^N\bigg[{\ln(1-p_i)+y_i\ln \left({\frac {p_i}{1-p_i}}\right)\bigg]}
 \label{eq:logLik}
\end{equation}

In order to maximize this equation we have to find optimum \(p_i\) values which are dependent on parameters \(\beta_0\) and \(\beta_1\), and also dependent on the values of predictor variables \(x_i\). We can rearrange the equation replacing \(p_i\) with the logistic equation. In addition, many optimization functions minimize rather than maximize. Therefore, we will be using negative log likelihood, which is also called the ``log loss'' or ``logistic loss'' function. The function below is the ``log loss'' function. We substituted \(p_i\) with the logistic equation and simplified the expression.

\begin{equation}
\operatorname L_{log}=-{\ln}(L)=-\sum_{i=1}^N\bigg[-{\ln(1+e^{(\beta _{0}+\beta _{1}x_i)})+y_i \left(\beta _{0}+\beta _{1}x_i\right)\bigg]}
 \label{eq:llog}
\end{equation}

Now, let us see how this works in practice. First, as in the example above we will use one predictor variable, the expression of one gene to classify tumor samples to ``CIMP'' and ``noCIMP'' subtypes. We will be using PDPN gene expression, which was one of the most important variables in our random forest model. We will use the formula interface in \texttt{caret}, where we will supply the names of the response and predictor variables in a formula. In this case, we will be using a core R function, \texttt{glm()}, from the \texttt{stats} package.\index{R Packages!\texttt{stats}} ``glm'' stands for generalized linear models, and it is the main interface for different types of regression
in R.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit logistic regression model}
\CommentTok{# method and family defines the type of regression}
\CommentTok{# in this case these arguments mean that we are doing logistic}
\CommentTok{# regression}
\NormalTok{lrFit =}\StringTok{ }\KeywordTok{train}\NormalTok{(subtype }\OperatorTok{~}\StringTok{ }\NormalTok{PDPN,  }
               \DataTypeTok{data=}\NormalTok{training, }\DataTypeTok{trControl=}\KeywordTok{trainControl}\NormalTok{(}\StringTok{"none"}\NormalTok{),}
               \DataTypeTok{method=}\StringTok{"glm"}\NormalTok{, }\DataTypeTok{family=}\StringTok{"binomial"}\NormalTok{)}

\CommentTok{# create data to plot the sigmoid curve}
\NormalTok{newdat <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{PDPN=}\KeywordTok{seq}\NormalTok{(}\KeywordTok{min}\NormalTok{(training}\OperatorTok{$}\NormalTok{PDPN),}
                               \KeywordTok{max}\NormalTok{(training}\OperatorTok{$}\NormalTok{PDPN),}\DataTypeTok{len=}\DecValTok{100}\NormalTok{))}

\CommentTok{# predict probabilities for the simulated data}
\NormalTok{newdat}\OperatorTok{$}\NormalTok{subtype =}\StringTok{ }\KeywordTok{predict}\NormalTok{(lrFit, }\DataTypeTok{newdata=}\NormalTok{newdat, }\DataTypeTok{type=}\StringTok{"prob"}\NormalTok{)[,}\DecValTok{1}\NormalTok{]}

\CommentTok{# plot the sigmoid curve and the training data}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(subtype}\OperatorTok{==}\StringTok{"CIMP"}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{) }\OperatorTok{~}\StringTok{ }\NormalTok{PDPN, }
     \DataTypeTok{data=}\NormalTok{training, }\DataTypeTok{col=}\StringTok{"red4"}\NormalTok{,}
     \DataTypeTok{ylab=}\StringTok{"subtype as 0 or 1"}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"PDPN expression"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(subtype }\OperatorTok{~}\StringTok{ }\NormalTok{PDPN, newdat, }\DataTypeTok{col=}\StringTok{"green4"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{05-supervisedLearning_files/figure-latex/logReg1-1} 

}

\caption{Sigmoid curve for prediction of subtype based on one predictor variable.}\label{fig:logReg1}
\end{figure}

Figure \ref{fig:logReg1} shows the sigmoidal curve that is fitted by the logistic regression. ``noCIMP'' subtype has higher expression of the PDPN gene than the ``CIMP'' subtype. In other words, the higher the values of PDPN, the more likely that the tumor sample will be classified as ``noCIMP''. We can also assess the performance of our model with the test set and the training set. Let us try to do that again with the \texttt{caret::predict()} and \texttt{caret::confusionMatrix()} functions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# training accuracy }
\NormalTok{class.res=}\KeywordTok{predict}\NormalTok{(lrFit,training[,}\OperatorTok{-}\DecValTok{1}\NormalTok{])}
\KeywordTok{confusionMatrix}\NormalTok{(training[,}\DecValTok{1}\NormalTok{],class.res)}\OperatorTok{$}\NormalTok{overall[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Accuracy 
## 0.9461538
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test accuracy }
\NormalTok{class.res=}\KeywordTok{predict}\NormalTok{(lrFit,testing[,}\OperatorTok{-}\DecValTok{1}\NormalTok{])}
\KeywordTok{confusionMatrix}\NormalTok{(testing[,}\DecValTok{1}\NormalTok{],class.res)}\OperatorTok{$}\NormalTok{overall[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Accuracy 
## 0.9259259
\end{verbatim}

The test accuracy\index{accuracy} is slightly worse than the training accuracy. Overall this is not as good as k-NN\index{k-nearest neighbors (k-NN)}, but remember we used only one predictor variable. We have thousands of genes as predictor variables. Now we will try to use all of them in the classification problem. After fitting the model, we will check training and test accuracy. We fit the model again with the \texttt{caret::train()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lrFit2 =}\StringTok{ }\KeywordTok{train}\NormalTok{(subtype }\OperatorTok{~}\StringTok{ }\NormalTok{.,  }
                \DataTypeTok{data=}\NormalTok{training, }
                \CommentTok{# no model tuning with sampling}
                \DataTypeTok{trControl=}\KeywordTok{trainControl}\NormalTok{(}\StringTok{"none"}\NormalTok{),}
                \DataTypeTok{method=}\StringTok{"glm"}\NormalTok{, }\DataTypeTok{family=}\StringTok{"binomial"}\NormalTok{)}

\CommentTok{# training accuracy }
\NormalTok{class.res=}\KeywordTok{predict}\NormalTok{(lrFit2,training[,}\OperatorTok{-}\DecValTok{1}\NormalTok{])}
\KeywordTok{confusionMatrix}\NormalTok{(training[,}\DecValTok{1}\NormalTok{],class.res)}\OperatorTok{$}\NormalTok{overall[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Accuracy 
##        1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test accuracy }
\NormalTok{class.res=}\KeywordTok{predict}\NormalTok{(lrFit2,testing[,}\OperatorTok{-}\DecValTok{1}\NormalTok{])}
\KeywordTok{confusionMatrix}\NormalTok{(testing[,}\DecValTok{1}\NormalTok{],class.res)}\OperatorTok{$}\NormalTok{overall[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Accuracy 
## 0.4259259
\end{verbatim}

Training accuracy is \(1\), so training error is \(0\), and nothing is misclassified in the training set. However, test accuracy/error is close to terrible. It does only little better than a random guess. If we randomly assigned class labels we would get 0.5 accuracy. The test set accuracy is 0.55 despite the 100\% training accuracy. This is because the model overfits to the training data. There are too many variables in the model. The number of predictor variables is \textasciitilde6.5 times more than the number of samples. The excess of predictor variables makes the model very flexible (high variance), and this leads to overfitting.

\hypertarget{regularization-in-order-to-avoid-overfitting}{%
\subsection{Regularization in order to avoid overfitting}\label{regularization-in-order-to-avoid-overfitting}}

If \index{regularization}we can limit the flexibility of the model, this might help with performance on the unseen, new data sets. Generally, any modification of the learning method to improve performance on the unseen datasets is called regularization. We need regularization to introduce bias to the model and to decrease the variance. This can be achieved by modifying the loss function with a penalty term which effectively shrinks the estimates of the coefficients. Therefore these types of methods within the framework of regression are also called ``shrinkage'' methods or ``penalized regression'' methods.\index{overfitting}

One way to ensure shrinkage is to add the penalty term, \(\lambda\sum{\beta_j}^2\), to the loss function. This penalty term is also known as the L2 norm or L2 penalty. It is calculated as the square root of the sum of the squared vector values. This term will help shrink the coefficients in the regression towards zero. The new loss function is as follows, where \(j\) is the number of parameters/coefficients in the model and \(L_{log}\) is the log loss function in Eq. \eqref{eq:llog}.

\begin{equation}
L_{log}+\lambda\sum_{j=1}^p{\beta_j}^2
\label{eq:L2norm}
\end{equation}

This penalized loss function is called ``ridge regression'' \citep{hoerl1970ridge}.\index{ridge regression} When we add the penalty, the only way the optimization procedure keeps the overall loss function minimum is to assign smaller values to the coefficients. The \(\lambda\) parameter controls how much emphasis is given to the penalty term. The higher the \(\lambda\) value, the more coefficients in the regression will be pushed towards zero. However, they will never be exactly zero. This is not desirable if we want the model to select important variables. A small modification to the penalty is to use the absolute values of \(B_j\) instead of squared values. This penalty is called the ``L1 norm'' or ``L1 penalty''. The regression method that uses the L1 penalty is known as ``Lasso regression''\index{lasso regression} \citep{tibshirani1996regression}.

\[
L_{log}+\lambda\sum_{j=1}^p{|\beta_j}|
\]
However, the L1 penalty tends to pick one variable at random when predictor variables are correlated. In this case, it looks like one of the variables is not important although it might still have predictive power. The Ridge regression on the other hand shrinks coefficients of correlated variables towards each other, keeping all of them. It has been shown that both Lasso and Ridge regression have their drawbacks and advantages \citep{friedman2010regularization}. More recently, a method called ``elastic net'' \index{elastic net}was proposed to include the best of both worlds \citep{zou2005regularization}. This method uses both L1 and L2 penalties. The equation below shows the modified loss function by this penalty. As you can see the \(\lambda\) parameter still controls the weight that is given to the penalty. This time the additional parameter \(\alpha\) controls the weight given to L1 or L2 penalty and it is a value between 0 and 1.
\[
L_{log}+\lambda\sum_{j=1}^p{(\alpha\beta_j^2+(1-\alpha)|\beta_j}|)
\]

We have now got the concept behind regularization and we can see how it works in practice. We are going to use elastic net on our tumor subtype prediction problem. We will let cross-validation select the best \(\lambda\) and we will fix the \(\alpha\) parameter at \(0.5\).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{17}\NormalTok{)}
\KeywordTok{library}\NormalTok{(glmnet)}

\CommentTok{# this method controls everything about training}
\CommentTok{# we will just set up 10-fold cross validation}
\NormalTok{trctrl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{,}\DataTypeTok{number=}\DecValTok{10}\NormalTok{)}

\CommentTok{# we will now train elastic net model}
\CommentTok{# it will try}
\NormalTok{enetFit <-}\StringTok{ }\KeywordTok{train}\NormalTok{(subtype}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ training, }
                 \DataTypeTok{method =} \StringTok{"glmnet"}\NormalTok{,}
                 \DataTypeTok{trControl=}\NormalTok{trctrl,}
                 \CommentTok{# alpha and lambda paramters to try}
                 \DataTypeTok{tuneGrid =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{alpha=}\FloatTok{0.5}\NormalTok{,}
                                       \DataTypeTok{lambda=}\KeywordTok{seq}\NormalTok{(}\FloatTok{0.1}\NormalTok{,}\FloatTok{0.7}\NormalTok{,}\FloatTok{0.05}\NormalTok{)))}

\CommentTok{# best alpha and lambda values by cross-validation accuracy}
\NormalTok{enetFit}\OperatorTok{$}\NormalTok{bestTune}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   alpha lambda
## 1   0.5    0.1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test accuracy }
\NormalTok{class.res=}\KeywordTok{predict}\NormalTok{(enetFit,testing[,}\OperatorTok{-}\DecValTok{1}\NormalTok{])}
\KeywordTok{confusionMatrix}\NormalTok{(testing[,}\DecValTok{1}\NormalTok{],class.res)}\OperatorTok{$}\NormalTok{overall[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Accuracy 
## 0.9814815
\end{verbatim}

As you can see regularization worked, the tuning step selected \(\lambda=1\), and we were able to get a satisfactory test set accuracy with the best model.

\hypertarget{variable-importance-2}{%
\subsection{Variable importance}\label{variable-importance-2}}

The variable importance\index{variable importance} of the penalized regression, especially for lasso and elastic net, is more or less out of the box. As discussed, these methods will set regression coefficients for irrelevant variables to zero. This provides a system for selecting important variables but it does not necessarily provide a way to rank them. Using the size of the regression coefficients is a way to rank predictor variables, however if the data is not normalized, you will get different scales for different variables. In our case, we normalized the data and we know that the variables have the same scale before they went into the training. We can use this fact and rank them based on the regression coefficients. The \texttt{caret::varImp()} function uses the coefficients to rank the variables from the elastic net model. Below, were going to plot the top 10 important variables which are normalized to the importance of the most important variable.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{varImp}\NormalTok{(enetFit),}\DataTypeTok{top=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{05-supervisedLearning_files/figure-latex/varImpEnet-1} 

}

\caption{Variable importance metric for elastic net. This metric uses regression coefficients as importance.}\label{fig:varImpEnet}
\end{figure}

\begin{rmdtip}
\begin{rmdtip}

\textbf{Want to know more ?}

\begin{itemize}
\tightlist
\item
  Lecture by Trevor Hastie on regularized regression. You probably need to understand the basics of regression and its terminology to follow this. However, the lecture is not very heavy on math. \url{https://youtu.be/BU2gjoLPfDc}
\end{itemize}

\end{rmdtip}
\end{rmdtip}

\hypertarget{other-supervised-algorithms}{%
\section{Other supervised algorithms}\label{other-supervised-algorithms}}

We will next introduce a couple of other supervised algorithms for completeness but in less detail. These algorithms are also as popular as the others we introduced above and people who are interested in computational genomics see them used in the field for different problems. These algorithms also fit to the general framework of optimization of a cost/loss function. However, the approaches to the construction of the cost function and the cost function itself are different in each case.

\hypertarget{gradient-boosting}{%
\subsection{Gradient boosting}\label{gradient-boosting}}

Gradient boosting is a prediction model that uses an ensemble of decision trees similar to random forest. However, the decision trees are added sequentially, which is why these models are also called ``Multiple Additive Regression Trees (MART)'' \citep{friedman2003mart}. Apart from this, you will see similar methods called ``Gradient boosting machines (GBM)''\citep{friedman2001gbm} or ``Boosted regression trees (BRT)'' \citep{elith2008brt} in the literature.

Generally, ``boosting'' \index{gradient boosting} refers to an iterative learning approach where each new model tries to focus on data points where the previous ensemble of simple models did not predict well. Gradient boosting is an improvement over that, where each new model tries to focus on the residual errors (prediction error for the current ensemble of models) of the previous model. Specifically in gradient boosting, the simple models are trees. As in random forests, many trees are grown but in this case, trees are sequentially grown and each tree focuses on fixing the shortcomings of the previous trees. Figure \ref{fig:GBMcartoon} shows this concept. One of the most widely used algorithms for gradient boosting is \texttt{XGboost} which stands for ``extreme gradient boosting'' \citep{chen2016xgboost}. Below we will demonstrate how to use this on our problem. \texttt{XGboost}\index{R Packages!\texttt{XGboost}} as well as other gradient boosting methods has many parameters to regularize and optimize the complexity of the model. Finding the best parameters for your problem might take some time. However, this flexibility comes with benefits; methods depending on \texttt{XGboost} have won many machine learning competitions \citep{chen2016xgboost}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/ml-GBM-features} 

}

\caption{Gradient boosting machines concept. Individual decision trees are built sequentially in order to fix the errors from the previous trees.}\label{fig:GBMcartoon}
\end{figure}

The most important parameters are number of trees (\texttt{nrounds}), tree depth (\texttt{max\_depth}), and learning rate or shrinkage (\texttt{eta}). Generally, the more trees we have, the better the algorithm will learn because each tree tries to fix classification errors that the previous tree ensemble could not perform. Having too many trees might cause overfitting. However, the learning rate parameter, eta, combats that by shrinking the contribution of each new tree. This can be set to lower values if you have many trees. You can either set a large number of trees and then tune the model with the learning rate parameter or set the learning rate low, say to \(0.01\) or \(0.1\) and tune the number of trees. Similarly, tree depth also controls for overfitting. The deeper the tree, the more usually it will overfit. This has to be tuned as well; the default is at 6. You can try to explore a range around the default. Apart from these, as in random forests, you can subsample the training data and/or the predictive variables. These strategies can also help you counter overfitting.

We are now going to use \texttt{XGboost} with the caret package on our cancer subtype classification problem. We are going to try different learning rate parameters. In this instance, we also subsample the dataset before we train each tree. The ``subsample'' parameter controls this and we set this to be 0.5, which means that before we train a tree we will sample 50\% of the data and use only that portion to train the tree.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(xgboost)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{17}\NormalTok{)}

\CommentTok{# we will just set up 5-fold cross validation}
\NormalTok{trctrl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{,}\DataTypeTok{number=}\DecValTok{5}\NormalTok{)}

\CommentTok{# we will now train elastic net model}
\CommentTok{# it will try}
\NormalTok{gbFit <-}\StringTok{ }\KeywordTok{train}\NormalTok{(subtype}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ training, }
                 \DataTypeTok{method =} \StringTok{"xgbTree"}\NormalTok{,}
                 \DataTypeTok{trControl=}\NormalTok{trctrl,}
                 \CommentTok{# paramters to try}
                 \DataTypeTok{tuneGrid =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{nrounds=}\DecValTok{200}\NormalTok{,}
                                       \DataTypeTok{eta=}\KeywordTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{,}\FloatTok{0.1}\NormalTok{,}\FloatTok{0.3}\NormalTok{),}
                                       \DataTypeTok{max_depth=}\DecValTok{4}\NormalTok{,}
                                       \DataTypeTok{gamma=}\DecValTok{0}\NormalTok{,}
                                       \DataTypeTok{colsample_bytree=}\DecValTok{1}\NormalTok{,}
                                       \DataTypeTok{subsample=}\FloatTok{0.5}\NormalTok{,}
                                       \DataTypeTok{min_child_weight=}\DecValTok{1}\NormalTok{))}
                                       

\CommentTok{# best parameters by cross-validation accuracy}
\NormalTok{gbFit}\OperatorTok{$}\NormalTok{bestTune}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   nrounds max_depth eta gamma colsample_bytree min_child_weight subsample
## 2     200         4 0.1     0                1                1       0.5
\end{verbatim}

Similar to random forests, we can estimate the variable importance for gradient boosting using the improvement in gini impurity or other performance-related metrics every time a variable is selected in a tree. Again, the \texttt{caret::varImp()} function can be used to plot the importance metrics.

\begin{rmdtip}
\begin{rmdtip}

\textbf{Want to know more ?}

\begin{itemize}
\tightlist
\item
  More background on gradient boosting and XGboost: (\url{https://xgboost.readthedocs.io/en/latest/tutorials/model.html}). This explains the cost/loss function and regularization in more detail.
\item
  Lecture on Gradient boosting and random forests by Trevor Hastie: (\url{https://youtu.be/wPqtzj5VZus})
\end{itemize}

\end{rmdtip}
\end{rmdtip}

\hypertarget{support-vector-machines-svm}{%
\subsection{Support Vector Machines (SVM)}\label{support-vector-machines-svm}}

Support vector machines (SVM) \index{Support vector machines (SVM)} were popularized in the 90s due the efficiency and the performance of the algorithm \citep{boser1992svm}. The algorithm works by identifying the optimal decision boundary that separates the data points into different groups (or classes), and then predicts the class of new observations based on this separation boundary. Depending on the situation, the different groups might be separable by a linear straight line or by a non-linear boundary line or plane. If you review k-NN decision boundaries in Figure \ref{fig:kNNboundary}, you can see that the decision boundary is not linear. SVM can deal with linear or non-linear decision boundaries.

First, SVM can map the data to higher dimensions where the decision boundary can be linear. This is achieved by applying certain mathematical functions, called ``kernel functions'', to the predictor variable space. For example, a second-degree polynomial can be applied to predictor variables which creates new variables and in this new space the problem is linearly separable. Figure \ref{fig:SVMcartoon} demonstrates this concept where points in feature space are mapped to quadratic space where linear separation is possible.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/kernelSVM} 

}

\caption{Support vector machine concept. With the help of a kernel function,points in feature space are mapped to higher dimensions where linear separation is possible.}\label{fig:SVMcartoon}
\end{figure}

Second, SVM not only tries to find a decision boundary, but tries to find the boundary with the largest buffer zone on the sides of the boundary. Having a boundary with a large buffer or ``margin'', as it is formally called, will perform better for the new data points not used in the model training (margin is marked in Figure \ref{fig:SVMcartoon} ). In addition, SVM calculates the decision boundary with some error toleration. As we have seen it may not always be possible to find a linear boundary that perfectly separates the classes. SVM tolerates some degree of error, as in data points on the wrong side of the decision boundary.

Another important feature of the algorithm is that SVM decides on the decision boundary by only relying on the ``landmark'' data points, formally known as ``support vectors''. These are points that are closest to the decision boundary and harder to classify. By keeping track of such points only for decision boundary creation, the computational complexity of the algorithm is reduced. However, this depends on the margin or the buffer zone. If we have a large margin then there are many landmark points. The extent of the margin is also related to the variance-bias trade-off. If the allowed margin is small the classification will try to find a boundary that makes fewer errors in the training set therefore might overfit. If the margin is larger, it will tolerate more errors in the training set and might generalize better. Practically, this is controlled by the ``C'' or ``Cost'' parameter in the SVM example we will show below. Another important choice we will make is the kernel function. Below we use the radial basis kernel function. This function provides an extra predictor dimension where the problem is linearly separable. The model we will use has only one parameter, which is ``C''. It is recommended that \(C\) is in the form of \(2^k\) where \(k\) is in the range of -5 and 15 \citep{hsu2003practical}. Another parameter that can be tuned is related to the radial basis function called ``sigma''. A smaller sigma means less bias and more variance, while a larger sigma means less variance and more bias. Again, exponential sequences are recommended for tuning that \citep{hsu2003practical}. We will set it to 1 for demonstration purposes below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#svm code here}
\KeywordTok{library}\NormalTok{(kernlab)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{17}\NormalTok{)}

\CommentTok{# we will just set up 5-fold cross validation}
\NormalTok{trctrl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{,}\DataTypeTok{number=}\DecValTok{5}\NormalTok{)}

\CommentTok{# we will now train elastic net model}
\CommentTok{# it will try}
\NormalTok{svmFit <-}\StringTok{ }\KeywordTok{train}\NormalTok{(subtype}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ training, }
                \CommentTok{# this SVM used radial basis function}
                 \DataTypeTok{method =} \StringTok{"svmRadial"}\NormalTok{, }
                 \DataTypeTok{trControl=}\NormalTok{trctrl,}
                \DataTypeTok{tuneGrid=}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{C=}\KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{,}\FloatTok{0.5}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                    \DataTypeTok{sigma=}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{rmdtip}
\begin{rmdtip}

\textbf{Want to know more ?}

\begin{itemize}
\tightlist
\item
  MIT lecture by Patrick Winston on SVM: \url{https://youtu.be/_PwhiWxHK8o}. This lecture explains the concept with some mathematical background. It is not hard to follow. You should be able to follow this if you know what vectors are and if you have some knowledge on derivatives and basic algebra.
\item
  Online demo for SVM: (\url{https://cs.stanford.edu/people/karpathy/svmjs/demo/}). You can play with sigma and C parameters for radial basis SVM and see how they affect the decision boundary.
\end{itemize}

\end{rmdtip}
\end{rmdtip}

\hypertarget{neural-networks-and-deep-versions-of-it}{%
\subsection{Neural networks and deep versions of it}\label{neural-networks-and-deep-versions-of-it}}

Neural networks \index{neural network} are another popular machine learning method which is recently regaining popularity. The earlier versions of the algorithm were popularized in the 80s and 90s. The advantage of neural networks is like SVM, they can model non-linear decision boundaries. The basic idea of neural networks is to combine the predictor variables in order to model the response variable as a non-linear function. In a neural network, input variables pass through several layers that combine the variables and transform those combinations and recombine outputs depending on how many layers the network has. In the conceptual example in Figure \ref{fig:neuralNetDiagram} the input nodes receive predictor variables and make linear combinations of them in the form of \(\sum ( w_ixi +b)\). Simply put, the variables are multiplied with weights and summed up. This is what we call ``linear combination''. These quantities are further fed into another layer called the hidden layer where an activation function is applied on the sums. And these results are further fed into an output node which outputs class probabilities assuming we are working on a classification algorithm. There could be many more hidden layers that will even further combine the output from hidden layers before them. The algorithm in the end also has a cost function similar to the logistic regression cost function, but it now has to estimate all the weight parameters: \(w_i\). This is a more complicated problem than logistic regression because of the number of parameters to be estimated but neural networks are able to fit complex functions due their parameter space flexibility as well.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/neuralNetDiagram} 

}

\caption{Diagram for a simple neural network, their combinations pass through hidden layers and are combined again for the output. Predictor variables are fed to the network and weights are adjusted to optimize the cost function.}\label{fig:neuralNetDiagram}
\end{figure}

In a practical sense, the number of nodes in the hidden layer (size) and some regularization on the weights can be applied to control for overfitting. This is called the calculated (decay) parameter controls for overfitting.

We will train a simple neural network on our cancer data set. In this simple example, the network architecture is somewhat fixed. We can only the choose number of nodes (denoted by ``size'') in the hidden layer and a regularization parameter (denoted by ``decay''). Increasing the number of nodes in the hidden layer or in other implementations increasing the number of the hidden layers, will help model non-linear relationships but can overfit. One way to combat that is to limit the number of nodes in the hidden layer; another way is to regularize the weights. The decay parameter does just that, it penalizes the loss function by \(decay(weigths^2)\). In the example below, we try 1 or 2 nodes in the hidden layer in the interest of simplicity and run-time. In addition, we set \texttt{decay=0}, which will correspond to not doing any regularization.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#svm code here}
\KeywordTok{library}\NormalTok{(nnet)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{17}\NormalTok{)}

\CommentTok{# we will just set up 5-fold cross validation}
\NormalTok{trctrl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{,}\DataTypeTok{number=}\DecValTok{5}\NormalTok{)}

\CommentTok{# we will now train neural net model}
\CommentTok{# it will try}
\NormalTok{nnetFit <-}\StringTok{ }\KeywordTok{train}\NormalTok{(subtype}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ training, }
                 \DataTypeTok{method =} \StringTok{"nnet"}\NormalTok{,}
                 \DataTypeTok{trControl=}\NormalTok{trctrl,}
                 \DataTypeTok{tuneGrid=}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{size=}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DataTypeTok{decay=}\DecValTok{0}
\NormalTok{                                      ),}
                 \CommentTok{# this is maximum number of weights}
                 \CommentTok{# needed for the nnet method}
                 \DataTypeTok{MaxNWts=}\DecValTok{2000}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

The example we used above is a bit outdated. The modern ``deep'' neural networks provide much more flexibility in the number of nodes, number of layers and regularization options. In many areas, especially computer vision deep neural networks are the state-of-the-art \citep{lecun2015deep}. These modern implementations of neural networks are available in R via the \texttt{keras}\index{R Packages!\texttt{keras}} package and can also be trained via the \texttt{caret}\index{R Packages!\texttt{caret}} package with the similar interface we have shown until now.

\begin{rmdtip}
\begin{rmdtip}

\textbf{Want to know more ?}

\begin{itemize}
\tightlist
\item
  Deep neural networks in R: (\url{https://keras.rstudio.com/}). There are examples and background information on deep neural networks.
\item
  Online demo for neural networks: (\url{https://cs.stanford.edu/~karpathy/svmjs/demo/demonn.html}). You can see the effect of the number of hidden layers and number of nodes on the decision boundary.
\end{itemize}

\end{rmdtip}
\end{rmdtip}

\hypertarget{ensemble-learning}{%
\subsection{Ensemble learning}\label{ensemble-learning}}

Ensemble learning \index{ensemble learning}models are simply combinations of different machine learning models. By now, we already introduced the concept of ensemble learning in random forests and gradient boosting. However, this concept can be generalized to combining all kinds of different models. ``Random forests'' is an ensemble of the same type of models, decision trees. We can also have ensembles of different types of models. For example, we can combine random forest, k-NN and elastic net models, and make class predictions based on the votes from those different models. Below, we are showing how to do this. We are going to get predictions for three different models on the test set, use majority voting to decide on the class label, and then check performance using \texttt{caret::confusionMatrix()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# predict with k-NN model}
\NormalTok{knnPred=}\KeywordTok{as.character}\NormalTok{(}\KeywordTok{predict}\NormalTok{(knnFit,testing[,}\OperatorTok{-}\DecValTok{1}\NormalTok{],}\DataTypeTok{type=}\StringTok{"class"}\NormalTok{))}
\CommentTok{# predict with elastic Net model}
\NormalTok{enetPred=}\KeywordTok{as.character}\NormalTok{(}\KeywordTok{predict}\NormalTok{(enetFit,testing[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]))}
\CommentTok{# predict with random forest model}
\NormalTok{rfPred=}\KeywordTok{as.character}\NormalTok{(}\KeywordTok{predict}\NormalTok{(rfFit,testing[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]))}

\CommentTok{# do voting for class labels}
\CommentTok{# code finds the most frequent class label per row}
\NormalTok{votingPred=}\KeywordTok{apply}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(knnPred,enetPred,rfPred),}\DecValTok{1}\NormalTok{,}
                 \ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{names}\NormalTok{(}\KeywordTok{which.max}\NormalTok{(}\KeywordTok{table}\NormalTok{(x))))}

\CommentTok{# check accuracy}
\KeywordTok{confusionMatrix}\NormalTok{(}\DataTypeTok{data=}\NormalTok{testing[,}\DecValTok{1}\NormalTok{],}
                \DataTypeTok{reference=}\KeywordTok{as.factor}\NormalTok{(votingPred))}\OperatorTok{$}\NormalTok{overall[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Accuracy 
## 0.9814815
\end{verbatim}

In the test set, we were able to obtain perfect accuracy after voting. More complicated and accurate ways to build ensembles exist. We could also use the mean of class probabilities instead of voting for final class predictions. We can even combine models in a regression-based scheme to assign weights to the votes or to the predicted class probabilities of each model. In these cases, the prediction performance of the ensembles can also be tested with sampling techniques such as cross-validation. You can think of this as another layer of optimization or modeling for combining results from different models. We will not pursue this further in this chapter but packages such as \href{https://cran.r-project.org/web/packages/caretEnsemble/}{\texttt{caretEnsemble}}, \href{https://cran.r-project.org/web/packages/SuperLearner/index.html}{\texttt{SuperLearner}} or \href{https://mlr.mlr-org.com/}{\texttt{mlr}} can combine models in various ways described above. \index{R Packages!\texttt{caretEnsemble}}
\index{R Packages!\texttt{SuperLearner}}
\index{R Packages!\texttt{mlr}}

\hypertarget{predicting-continuous-variables-regression-with-machine-learning}{%
\section{Predicting continuous variables: Regression with machine learning}\label{predicting-continuous-variables-regression-with-machine-learning}}

Until now, we only considered methods that can help us predict class labels. However, all the methods we have shown can also be used to predict continuous variables. In this case, the methods will try to optimize the prediction in error which is usually in the form of the sum of squared errors (SSE): \(SSE=\sum (Y-f(X))^2\), where \(Y\) is the continuous response variable and \(f(X)\) is the outcome of the machine learning model. \index{sum of squared errors (SSE)}

In this section, we are going to show how to use a supervised learning method for regression. All the methods we have introduced previously in the context of classification can also do regression. Technically, this is just a simple change in the cost function format and the optimization step still tries to optimize the parameters of the cost function. In many cases, if your response variable is numeric, methods in the \texttt{caret} package will automatically apply regression.

\hypertarget{use-case-predicting-age-from-dna-methylation}{%
\subsection{Use case: Predicting age from DNA methylation}\label{use-case-predicting-age-from-dna-methylation}}

We will demonstrate random forest regression using a different data set which has a continuous response variable. This time we are going to try to predict the age of individuals from their DNA methylation \index{DNA methylation} levels. Methylation is a DNA modification which has implications in gene regulation and cell state. We have introduced DNA methylation in depth in Chapters \ref{intro} and \ref{bsseq}, however for now, what we need to know is that there are about 24 million CpG dinucleotides in the human genome. Their methylation status can be measured with quantitative assays and the value is between 0 and 1. If it is 0, the CpG is not methylated in any of the cells in the sample, and if it is 1, the CpG is methylated in all the cells of the sample. It has been shown that methylation is predictive of the age of the individual that the sample is taken from \citep{numata2012dna, horvath2013dna}. Now, we will try to test that with a data set containing hundreds of individuals, their age, and methylation values for \textasciitilde27000 CpGs. We first read in the files and construct a training set.

\hypertarget{reading-and-processing-the-data}{%
\subsection{Reading and processing the data}\label{reading-and-processing-the-data}}

Let us first read in the data. When we run the summary and histogram we see that the methylation values are between \(0\) and \(1\) and there are \(108\) samples (see Figure \ref{fig:readMethAge} ). Typically, methylation values have bimodal distribution. In this case many of them have values around \(0\) and the second-most frequent value bracket is around \(0.9\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# file path for CpG methylation and age}
\NormalTok{fileMethAge=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                      \StringTok{"CpGmeth2Age.rds"}\NormalTok{,}
                      \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}

\CommentTok{# read methylation-age table}
\NormalTok{ameth=}\KeywordTok{readRDS}\NormalTok{(fileMethAge)}
\KeywordTok{dim}\NormalTok{(ameth)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]   108 27579
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(ameth[,}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       Age            cg26211698        cg03790787     
##  Min.   :-0.4986   Min.   :0.01223   Min.   :0.05001  
##  1st Qu.:-0.4027   1st Qu.:0.01885   1st Qu.:0.07818  
##  Median :18.8466   Median :0.02269   Median :0.08964  
##  Mean   :25.9083   Mean   :0.02483   Mean   :0.09300  
##  3rd Qu.:49.6110   3rd Qu.:0.02888   3rd Qu.:0.10423  
##  Max.   :83.6411   Max.   :0.04883   Max.   :0.16271
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot histogram of methylation values}
\KeywordTok{hist}\NormalTok{(}\KeywordTok{unlist}\NormalTok{(ameth[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]),}\DataTypeTok{border=}\StringTok{"white"}\NormalTok{,}
      \DataTypeTok{col=}\StringTok{"cornflowerblue"}\NormalTok{,}\DataTypeTok{main=}\StringTok{""}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"methylation values"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{05-supervisedLearning_files/figure-latex/readMethAge-1} 

}

\caption{Histogram of methylation values in the training set for age prediction.}\label{fig:readMethAge}
\end{figure}

There are \(~27000\) predictor variables. We can remove the ones that have low variation across samples. In this case, the methylation values are between \(0\) and \(1\). The CpGs that have low variation are not likely to have any association with age; they could simply be technical variation of the experiment. We will remove CpGs that have less than 0.1 standard deviation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ameth=ameth[,}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{,matrixStats}\OperatorTok{::}\KeywordTok{colSds}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(ameth[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]))}\OperatorTok{>}\FloatTok{0.1}\NormalTok{)]}
\KeywordTok{dim}\NormalTok{(ameth)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  108 2290
\end{verbatim}

\hypertarget{running-random-forest-regression}{%
\subsection{Running random forest regression}\label{running-random-forest-regression}}

Now we can use random forest regression to predict the age from methylation values. We are then going to plot the predicted vs.~observed ages and see how good our predictions are. The resulting plots are shown in Figure \ref{fig:predictAge}. \index{random forest regression}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{18}\NormalTok{)}

\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}

\CommentTok{# we are not going to do any cross-validatin}
\CommentTok{# and rely on OOB error}
\NormalTok{trctrl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"none"}\NormalTok{)}

\CommentTok{# we will now train random forest model}
\NormalTok{rfregFit <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Age}\OperatorTok{~}\NormalTok{., }
               \DataTypeTok{data =}\NormalTok{ ameth, }
               \DataTypeTok{method =} \StringTok{"ranger"}\NormalTok{,}
               \DataTypeTok{trControl=}\NormalTok{trctrl,}
               \CommentTok{# calculate importance}
               \DataTypeTok{importance=}\StringTok{"permutation"}\NormalTok{, }
               \DataTypeTok{tuneGrid =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{mtry=}\DecValTok{50}\NormalTok{,}
                                     \DataTypeTok{min.node.size =} \DecValTok{5}\NormalTok{,}
                                     \DataTypeTok{splitrule=}\StringTok{"variance"}\NormalTok{)}
\NormalTok{               )}
\CommentTok{# plot Observed vs OOB predicted values from the model}
\KeywordTok{plot}\NormalTok{(ameth}\OperatorTok{$}\NormalTok{Age,rfregFit}\OperatorTok{$}\NormalTok{finalModel}\OperatorTok{$}\NormalTok{predictions,}
     \DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"observed Age"}\NormalTok{,}
     \DataTypeTok{ylab=}\StringTok{"OOB predicted Age"}\NormalTok{)}
\KeywordTok{mtext}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"R-squared"}\NormalTok{,}
            \KeywordTok{format}\NormalTok{(rfregFit}\OperatorTok{$}\NormalTok{finalModel}\OperatorTok{$}\NormalTok{r.squared,}\DataTypeTok{digits=}\DecValTok{2}\NormalTok{)))}

\CommentTok{# plot residuals}
\KeywordTok{plot}\NormalTok{(ameth}\OperatorTok{$}\NormalTok{Age,(rfregFit}\OperatorTok{$}\NormalTok{finalModel}\OperatorTok{$}\NormalTok{predictions}\OperatorTok{-}\NormalTok{ameth}\OperatorTok{$}\NormalTok{Age),}
     \DataTypeTok{pch=}\DecValTok{18}\NormalTok{,}\DataTypeTok{ylab=}\StringTok{"residuals (predicted-observed)"}\NormalTok{,}
     \DataTypeTok{xlab=}\StringTok{"observed Age"}\NormalTok{,}\DataTypeTok{col=}\StringTok{"blue3"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{h=}\DecValTok{0}\NormalTok{,}\DataTypeTok{col=}\StringTok{"red4"}\NormalTok{,}\DataTypeTok{lty=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{05-supervisedLearning_files/figure-latex/predictAge-1} 

}

\caption{Observed vs. predicted age (Left). Residual plot showing that for older people the error increases (Right).}\label{fig:predictAge}
\end{figure}

In this instance, we are using OOB errors and \(R^2\) value which shows how the model performs on OOB samples. The model can capture the general trend and it has acceptable OOB performance. It is not perfect as it makes errors on average close to 10 years when predicting the age, and the errors are more severe for older people (Figure \ref{fig:predictAge}). This could be due to having fewer older people to model or missing/inadequate predictor variables. However, everything we discussed in classification applies here. We had even fewer data points than the classification problem, so we did not do a split for a test data set. However, this should also be done for regression problems, especially when we are going to compare the performance of different models or want to have a better idea of the real-world performance of our model. We might also be interested in which variables are most important as in the classification problem; we can use the \texttt{caret:varImp()} function to get access to random-forest-specific variable importance metrics.

\hypertarget{exercises-3}{%
\section{Exercises}\label{exercises-3}}

\hypertarget{classification}{%
\subsection{Classification}\label{classification}}

For this set of exercises we will be using the gene expression and patient annotation data from the glioblastoma patient. You can read the data as shown below:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(compGenomRData)}
\CommentTok{# get file paths}
\NormalTok{fileLGGexp=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                      \StringTok{"LGGrnaseq.rds"}\NormalTok{,}
                      \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{fileLGGann=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                      \StringTok{"patient2LGGsubtypes.rds"}\NormalTok{,}
                      \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\CommentTok{# gene expression values}
\NormalTok{gexp=}\KeywordTok{readRDS}\NormalTok{(fileLGGexp)}

\CommentTok{# patient annotation}
\NormalTok{patient=}\KeywordTok{readRDS}\NormalTok{(fileLGGann)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Our first task is to not use any data transformation and do classification. Run the k-NN classifier on the data without any transformation or scaling. What is the effect on classification accuracy for k-NN predicting the CIMP and noCIMP status of the patient? {[}Difficulty: \textbf{Beginner}{]}
\item
  Bootstrap resampling can be used to measure the variability of the prediction error. Use bootstrap resampling with k-NN for the prediction accuracy. How different is it from cross-validation for different \(k\)s? {[}Difficulty: \textbf{Intermediate}{]}
\item
  There are a number of ways to get variable importance for a classification problem. Run random forests on the classification problem above. Compare the variable importance metrics from random forest and the one obtained from DALEX. How many variables are the same in the top 10? {[}Difficulty: \textbf{Advanced}{]}
\item
  Come up with a unified importance score by normalizing importance scores from random forests and DALEX, followed by taking the average of those scores. {[}Difficulty: \textbf{Advanced}{]}
\end{enumerate}

\hypertarget{regression}{%
\subsection{Regression}\label{regression}}

For this set of problems we will use the regression data set where we tried to predict the age of the sample from the methylation values. The data can be loaded as shown below:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# file path for CpG methylation and age}
\NormalTok{fileMethAge=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                      \StringTok{"CpGmeth2Age.rds"}\NormalTok{,}
                      \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}

\CommentTok{# read methylation-age table}
\NormalTok{ameth=}\KeywordTok{readRDS}\NormalTok{(fileMethAge)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Run random forest regression and plot the importance metrics. {[}Difficulty: \textbf{Beginner}{]}
\item
  Split 20\% of the methylation-age data as test data and run elastic net regression on the training portion to tune parameters and test it on the test portion. {[}Difficulty: \textbf{Intermediate}{]}
\item
  Run an ensemble model for regression using the \textbf{caretEnsemble} or \textbf{mlr} package and compare the results with the elastic net and random forest model. Did the test accuracy increase?
  \textbf{HINT:} You need to install these extra packages and learn how to use them in the context of ensemble models. {[}Difficulty: \textbf{Advanced}{]}
\end{enumerate}

\hypertarget{genomicIntervals}{%
\chapter{Operations on Genomic Intervals and Genome Arithmetic}\label{genomicIntervals}}

Considerable time in computational genomics is spent on overlapping different
features of the genome. Each feature can be represented with a genomic interval
within the chromosomal coordinate system. In addition, each interval can carry
different sorts of information. An interval may for instance represent exon coordinates or a transcription factor binding site. On the other hand,
you can have base-pair resolution, continuous scores over the genome such as read coverage, or
scores that could be associated with only certain bases such as in the case of CpG
methylation (see Figure \ref{fig:gintsum} ).
Typically, you will need to overlap intervals of interest with other features of
the genome, again represented as intervals. For example, you may want to overlap
transcription factor binding sites with CpG islands or promoters to quantify what percentage of binding sites overlap with your regions of interest. Overlapping mapped reads from high-throughput sequencing experiments with genomic features such as exons, promoters, and enhancers can also be classified as operations on genomic intervals. You can think of a million other ways that involve overlapping two sets of different features on the genome. This chapter aims to show how to do analysis involving operations on genomic intervals.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{images/genomeIntervalSummary} 

}

\caption{Summary of genomic intervals with different kinds of information.}\label{fig:gintsum}
\end{figure}

\hypertarget{operations-on-genomic-intervals-with-genomicranges-package}{%
\section{\texorpdfstring{Operations on genomic intervals with \texttt{GenomicRanges} package}{Operations on genomic intervals with GenomicRanges package}}\label{operations-on-genomic-intervals-with-genomicranges-package}}

The \href{http://bioconductor.org}{Bioconductor} project has a dedicated package called \href{http://www.bioconductor.org/packages/release/bioc/html/GenomicRanges.html}{\texttt{GenomicRanges}} to deal with genomic intervals. In this section, we will provide use cases involving operations on genomic intervals. The main reason we will stick to this package is that it provides tools to do overlap operations. However, the package requires that users operate on specific data types that are conceptually similar to a tabular data structure implemented in a way that makes overlapping and related operations easier. The main object we will be using is called the \texttt{GRanges} object and we will also see some other related objects from the \texttt{GenomicRanges} package.\index{R Packages!\texttt{GenomicRanges}}

\hypertarget{how-to-create-and-manipulate-a-granges-object}{%
\subsection{How to create and manipulate a GRanges object}\label{how-to-create-and-manipulate-a-granges-object}}

\texttt{GRanges} (from \texttt{GenomicRanges} package) is the main object that holds the genomic intervals and extra information about those intervals. Here we will show how to create one. Conceptually, it is similar to a data frame and some operations such as using \texttt{{[}\ {]}} notation to subset the table will also work on \texttt{GRanges}, but keep in mind that not everything that works for data frames will work on \texttt{GRanges} objects.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(GenomicRanges)}
\NormalTok{gr=}\KeywordTok{GRanges}\NormalTok{(}\DataTypeTok{seqnames=}\KeywordTok{c}\NormalTok{(}\StringTok{"chr1"}\NormalTok{,}\StringTok{"chr2"}\NormalTok{,}\StringTok{"chr2"}\NormalTok{),}
           \DataTypeTok{ranges=}\KeywordTok{IRanges}\NormalTok{(}\DataTypeTok{start=}\KeywordTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{150}\NormalTok{,}\DecValTok{200}\NormalTok{),}
                          \DataTypeTok{end=}\KeywordTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{200}\NormalTok{,}\DecValTok{300}\NormalTok{)),}
           \DataTypeTok{strand=}\KeywordTok{c}\NormalTok{(}\StringTok{"+"}\NormalTok{,}\StringTok{"-"}\NormalTok{,}\StringTok{"-"}\NormalTok{)}
\NormalTok{)}
\NormalTok{gr}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## GRanges object with 3 ranges and 0 metadata columns:
##       seqnames    ranges strand
##          <Rle> <IRanges>  <Rle>
##   [1]     chr1    50-100      +
##   [2]     chr2   150-200      -
##   [3]     chr2   200-300      -
##   -------
##   seqinfo: 2 sequences from an unspecified genome; no seqlengths
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# subset like a data frame}
\NormalTok{gr[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## GRanges object with 2 ranges and 0 metadata columns:
##       seqnames    ranges strand
##          <Rle> <IRanges>  <Rle>
##   [1]     chr1    50-100      +
##   [2]     chr2   150-200      -
##   -------
##   seqinfo: 2 sequences from an unspecified genome; no seqlengths
\end{verbatim}

As you can see, it looks a bit like a data frame. Also, note that the peculiar second argument ``ranges'' basically contains the start and end positions of the genomic intervals. However, you cannot just give start and end positions, you actually have to provide another object of \texttt{IRanges}. Do not let this confuse you; \texttt{GRanges} actually depends on another object that is very similar to itself called \texttt{IRanges} and you have to provide the ``ranges'' argument as an \texttt{IRanges} object. In its simplest form, an \texttt{IRanges} object can be constructed by providing start and end positions to the \texttt{IRanges()} function. Think of it as something you just have to provide in order to construct the \texttt{GRanges} object.

\texttt{GRanges} can also contain other information about the genomic interval such as scores, names, etc. You can provide extra information at the time of the construction or you can add it later. Here is how you can do that:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gr=}\KeywordTok{GRanges}\NormalTok{(}\DataTypeTok{seqnames=}\KeywordTok{c}\NormalTok{(}\StringTok{"chr1"}\NormalTok{,}\StringTok{"chr2"}\NormalTok{,}\StringTok{"chr2"}\NormalTok{),}
           \DataTypeTok{ranges=}\KeywordTok{IRanges}\NormalTok{(}\DataTypeTok{start=}\KeywordTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{150}\NormalTok{,}\DecValTok{200}\NormalTok{),}
                          \DataTypeTok{end=}\KeywordTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{200}\NormalTok{,}\DecValTok{300}\NormalTok{)),}
           \DataTypeTok{names=}\KeywordTok{c}\NormalTok{(}\StringTok{"id1"}\NormalTok{,}\StringTok{"id3"}\NormalTok{,}\StringTok{"id2"}\NormalTok{),}
           \DataTypeTok{scores=}\KeywordTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{90}\NormalTok{,}\DecValTok{50}\NormalTok{)}
\NormalTok{)}
\CommentTok{# or add it later (replaces the existing meta data)}
\KeywordTok{mcols}\NormalTok{(gr)=}\KeywordTok{DataFrame}\NormalTok{(}\DataTypeTok{name2=}\KeywordTok{c}\NormalTok{(}\StringTok{"pax6"}\NormalTok{,}\StringTok{"meis1"}\NormalTok{,}\StringTok{"zic4"}\NormalTok{),}
                    \DataTypeTok{score2=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{))}

\NormalTok{gr=}\KeywordTok{GRanges}\NormalTok{(}\DataTypeTok{seqnames=}\KeywordTok{c}\NormalTok{(}\StringTok{"chr1"}\NormalTok{,}\StringTok{"chr2"}\NormalTok{,}\StringTok{"chr2"}\NormalTok{),}
           \DataTypeTok{ranges=}\KeywordTok{IRanges}\NormalTok{(}\DataTypeTok{start=}\KeywordTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{150}\NormalTok{,}\DecValTok{200}\NormalTok{),}
                          \DataTypeTok{end=}\KeywordTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{200}\NormalTok{,}\DecValTok{300}\NormalTok{)),}
           \DataTypeTok{names=}\KeywordTok{c}\NormalTok{(}\StringTok{"id1"}\NormalTok{,}\StringTok{"id3"}\NormalTok{,}\StringTok{"id2"}\NormalTok{),}
           \DataTypeTok{scores=}\KeywordTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{90}\NormalTok{,}\DecValTok{50}\NormalTok{)}
\NormalTok{)}

\CommentTok{# or appends to existing meta data}
\KeywordTok{mcols}\NormalTok{(gr)=}\KeywordTok{cbind}\NormalTok{(}\KeywordTok{mcols}\NormalTok{(gr),}
                          \KeywordTok{DataFrame}\NormalTok{(}\DataTypeTok{name2=}\KeywordTok{c}\NormalTok{(}\StringTok{"pax6"}\NormalTok{,}\StringTok{"meis1"}\NormalTok{,}\StringTok{"zic4"}\NormalTok{)) )}
\NormalTok{gr}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## GRanges object with 3 ranges and 3 metadata columns:
##       seqnames    ranges strand |       names    scores       name2
##          <Rle> <IRanges>  <Rle> | <character> <numeric> <character>
##   [1]     chr1    50-100      * |         id1       100        pax6
##   [2]     chr2   150-200      * |         id3        90       meis1
##   [3]     chr2   200-300      * |         id2        50        zic4
##   -------
##   seqinfo: 2 sequences from an unspecified genome; no seqlengths
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# elementMetadata() and values() do the same things}
\KeywordTok{elementMetadata}\NormalTok{(gr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## DataFrame with 3 rows and 3 columns
##         names    scores       name2
##   <character> <numeric> <character>
## 1         id1       100        pax6
## 2         id3        90       meis1
## 3         id2        50        zic4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{values}\NormalTok{(gr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## DataFrame with 3 rows and 3 columns
##         names    scores       name2
##   <character> <numeric> <character>
## 1         id1       100        pax6
## 2         id3        90       meis1
## 3         id2        50        zic4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# you may also add metadata using the $ operator, as for data frames}
\NormalTok{gr}\OperatorTok{$}\NormalTok{name3 =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{,}\StringTok{"C"}\NormalTok{, }\StringTok{"B"}\NormalTok{)}
\NormalTok{gr}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## GRanges object with 3 ranges and 4 metadata columns:
##       seqnames    ranges strand |       names    scores       name2       name3
##          <Rle> <IRanges>  <Rle> | <character> <numeric> <character> <character>
##   [1]     chr1    50-100      * |         id1       100        pax6           A
##   [2]     chr2   150-200      * |         id3        90       meis1           C
##   [3]     chr2   200-300      * |         id2        50        zic4           B
##   -------
##   seqinfo: 2 sequences from an unspecified genome; no seqlengths
\end{verbatim}

\hypertarget{getting-genomic-regions-into-r-as-granges-objects}{%
\subsection{Getting genomic regions into R as GRanges objects}\label{getting-genomic-regions-into-r-as-granges-objects}}

There are multiple ways you can read your genomic features into R and create a \texttt{GRanges} object. Most genomic interval data comes in a tabular format that has the basic information about the location of the interval and some other information. We already showed how to read BED files as a data frame in Chapter \ref{Rintro}. Now we will show how to convert it to the \texttt{GRanges} object. This is one way of doing it, but there are more convenient ways described further in the text.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read CpGi data set}
\NormalTok{filePath=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                      \StringTok{"cpgi.hg19.chr21.bed"}\NormalTok{,}
                      \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{cpgi.df =}\StringTok{ }\KeywordTok{read.table}\NormalTok{(filePath, }\DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{,}
                     \DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{) }
\CommentTok{# remove chr names with "_"}
\NormalTok{cpgi.df =cpgi.df [}\KeywordTok{grep}\NormalTok{(}\StringTok{"_"}\NormalTok{,cpgi.df[,}\DecValTok{1}\NormalTok{],}\DataTypeTok{invert=}\OtherTok{TRUE}\NormalTok{),]}

\NormalTok{cpgi.gr=}\KeywordTok{GRanges}\NormalTok{(}\DataTypeTok{seqnames=}\NormalTok{cpgi.df[,}\DecValTok{1}\NormalTok{],}
                \DataTypeTok{ranges=}\KeywordTok{IRanges}\NormalTok{(}\DataTypeTok{start=}\NormalTok{cpgi.df[,}\DecValTok{2}\NormalTok{],}
                              \DataTypeTok{end=}\NormalTok{cpgi.df[,}\DecValTok{3}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

You may need to do some pre-processing before/after reading in the BED file\index{BED file}. Below is an example of getting transcription start sites from BED files containing RefSeq transcript locations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read refseq file}
\NormalTok{filePathRefseq=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                      \StringTok{"refseq.hg19.chr21.bed"}\NormalTok{,}
                      \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}


\NormalTok{ref.df =}\StringTok{ }\KeywordTok{read.table}\NormalTok{(filePathRefseq, }\DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{,}
                     \DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{) }
\NormalTok{ref.gr=}\KeywordTok{GRanges}\NormalTok{(}\DataTypeTok{seqnames=}\NormalTok{ref.df[,}\DecValTok{1}\NormalTok{],}
               \DataTypeTok{ranges=}\KeywordTok{IRanges}\NormalTok{(}\DataTypeTok{start=}\NormalTok{ref.df[,}\DecValTok{2}\NormalTok{],}
                              \DataTypeTok{end=}\NormalTok{ref.df[,}\DecValTok{3}\NormalTok{]),}
               \DataTypeTok{strand=}\NormalTok{ref.df[,}\DecValTok{6}\NormalTok{],}\DataTypeTok{name=}\NormalTok{ref.df[,}\DecValTok{4}\NormalTok{])}
\CommentTok{# get TSS}
\NormalTok{tss.gr=ref.gr}
\CommentTok{# end of the + strand genes must be equalized to start pos}
\KeywordTok{end}\NormalTok{(tss.gr[}\KeywordTok{strand}\NormalTok{(tss.gr)}\OperatorTok{==}\StringTok{"+"}\NormalTok{,])  =}\KeywordTok{start}\NormalTok{(tss.gr[}\KeywordTok{strand}\NormalTok{(tss.gr)}\OperatorTok{==}\StringTok{"+"}\NormalTok{,])}
\CommentTok{# startof the - strand genes must be equalized to end pos}
\KeywordTok{start}\NormalTok{(tss.gr[}\KeywordTok{strand}\NormalTok{(tss.gr)}\OperatorTok{==}\StringTok{"-"}\NormalTok{,])=}\KeywordTok{end}\NormalTok{(tss.gr[}\KeywordTok{strand}\NormalTok{(tss.gr)}\OperatorTok{==}\StringTok{"-"}\NormalTok{,])}
\CommentTok{# remove duplicated TSSes ie alternative transcripts}
\CommentTok{# this keeps the first instance and removes duplicates}
\NormalTok{tss.gr=tss.gr[}\OperatorTok{!}\KeywordTok{duplicated}\NormalTok{(tss.gr),]}
\end{Highlighting}
\end{Shaded}

Another way of doing this from a BED file is to use the \texttt{readTranscriptfeatures()}
function from the \texttt{genomation} package. This function takes care of the steps described in the code chunk above.

Reading the genomic features as text files and converting to \texttt{GRanges} is not the only way to create a \texttt{GRanges} object. With the help of the \href{http://www.bioconductor.org/packages/release/bioc/html/rtracklayer.html}{\texttt{rtracklayer}} package we can directly import BED files.\index{R Packages!\texttt{rtracklayer}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(rtracklayer)}

\CommentTok{# we are reading a BED file, the path to the file}
\CommentTok{# is stored in filePathRefseq variable}
\KeywordTok{import.bed}\NormalTok{(filePathRefseq)}
\end{Highlighting}
\end{Shaded}

Next, we will show how to use other methods to automatically obtain the data in the \texttt{GRanges} format from online databases. But you will not be able to use these methods for every data set, so it is good to know how to read data from flat files as well. We will use the \texttt{rtracklayer} package to download data from the UCSC Genome Browser\index{UCSC Genome Browser}. We will download CpG islands as \texttt{GRanges} objects. The \texttt{rtracklayer} workflow we show below works like using the UCSC table browser. You need to select which species you are working with, then you need to select which dataset you need to download and lastly you download the UCSC dataset or track as a \texttt{GRanges} object.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(rtracklayer)}
\NormalTok{session <-}\StringTok{ }\KeywordTok{browserSession}\NormalTok{(}\StringTok{"UCSC"}\NormalTok{,}\DataTypeTok{url =} \StringTok{'http://genome-euro.ucsc.edu/cgi-bin/'}\NormalTok{)}
\KeywordTok{genome}\NormalTok{(session) <-}\StringTok{ "mm9"}
\CommentTok{## choose CpG island track on chr12}
\NormalTok{query <-}\StringTok{ }\KeywordTok{ucscTableQuery}\NormalTok{(session, }\DataTypeTok{track=}\StringTok{"CpG Islands"}\NormalTok{,}\DataTypeTok{table=}\StringTok{"cpgIslandExt"}\NormalTok{,}
        \DataTypeTok{range=}\KeywordTok{GRangesForUCSCGenome}\NormalTok{(}\StringTok{"mm9"}\NormalTok{, }\StringTok{"chr12"}\NormalTok{))}
\CommentTok{## get the GRanges object for the track}
\KeywordTok{track}\NormalTok{(query)}
\end{Highlighting}
\end{Shaded}

There is also an interface to the Ensembl database called \href{https://bioconductor.org/packages/release/bioc/html/biomaRt.html}{biomaRt}. \index{R Packages!\texttt{biomaRt}}
This package will enable you to access and import all of the datasets included
in Ensembl. Another similar package is \href{https://bioconductor.org/packages/release/bioc/html/AnnotationHub.html}{AnnotationHub}.\index{R Packages!\texttt{AnnotationHub}}
This package is an aggregator for different datasets from various sources.
Using \texttt{AnnotationHub} one can access data sets from the UCSC browser, Ensembl browser
and datasets from genomics consortia such as ENCODE and Roadmap Epigenomics\index{ENCODE}.\index{Roadmap Epigenomics}
We provide examples of using \texttt{Biomart} package further into the chapter. In addition, the \texttt{AnnotationHub} package is used in Chapter \ref{chipseq}.

\hypertarget{frequently-used-file-formats-and-how-to-read-them-into-r-as-a-table}{%
\subsubsection{Frequently used file formats and how to read them into R as a table}\label{frequently-used-file-formats-and-how-to-read-them-into-r-as-a-table}}

There are multiple file formats in genomics but some of them you will see more
frequently than others. We already mentioned some of them. Here is a list of files
and functions that can read them into R as \texttt{GRanges} objects or something coercible to
\texttt{GRanges} objects.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \textbf{BED}: This format is used and popularized by the UCSC browser, and can hold a variety of
  information including exon/intron structure of transcripts in a single line. We will be using BED files in this chapter. In its simplest form, the BED file contains the chromosome name, the start position and end position for a genomic feature of interest.\index{BED file}

  \begin{itemize}
  \tightlist
  \item
    \texttt{genomation::readBed()}
  \item
    \texttt{genomation::readTranscriptFeatures()} good for getting intron/exon/promoters from BED12 files
  \item
    \texttt{rtracklayer::import.bed()}
  \end{itemize}
\item
  \textbf{GFF}: GFF format is a tabular text format for genomic features similar to BED. However,
  it is a more flexible format than BED, which makes it harder to parse at times. Many gene annotation files are in this format.

  \begin{itemize}
  \tightlist
  \item
    \texttt{genomation::gffToGranges()}
  \item
    \texttt{rtracklayer::impot.gff()}
  \end{itemize}
\item
  \textbf{BAM/SAM}: BAM format is a compressed and indexed tabular file format designed for aligned sequencing reads. SAM is the uncompressed version of the BAM file. We will touch upon BAM files in this chapter. The uncompressed SAM file is similar in spirit to a BED file where you have the basic location of chromosomal location information plus additional columns that are related to the quality of alignment or other relevant information. We will introduce this format in detail later in this chapter.\index{BAM file}
  \index{SAM file}

  \begin{itemize}
  \tightlist
  \item
    \texttt{GenomicAlignments::readGAlignments}
  \item
    \texttt{Rsamtools::scanBam} returns a data frame with columns from a SAM/BAM file.
  \end{itemize}
\item
  \textbf{BigWig}: This is used to for storing scores associated with genomic intervals. It is an indexed format. Similar to BAM, this makes it easier to query and only necessary portions
  of the file could be loaded into memory.

  \begin{itemize}
  \tightlist
  \item
    \texttt{rtracklayer::import.bw()}
  \end{itemize}
\item
  \textbf{Generic Text files}: This represents any text file with the minimal information of chromosome, start and end coordinates.

  \begin{itemize}
  \tightlist
  \item
    \texttt{genomation::readGeneric()}
  \end{itemize}
\item
  \textbf{Tabix/Bcf}: These are tabular file formats indexed and compressed similar to
  BAM. The following functions return lists rather than tabular data structures. These
  formats are mostly used to store genomic variation data such as SNPs and indels.\index{SNP}

  \begin{itemize}
  \tightlist
  \item
    \texttt{Rsamtools::scanTabix}
  \item
    \texttt{Rsamtools::scanBcf}
  \end{itemize}
\end{enumerate}

\hypertarget{finding-regions-that-dodo-not-overlap-with-another-set-of-regions}{%
\subsection{Finding regions that do/do not overlap with another set of regions}\label{finding-regions-that-dodo-not-overlap-with-another-set-of-regions}}

This is one of the most common tasks in genomics. Usually, you have a set of regions that you are interested in and you want to see if they overlap with another set of regions or see how many of them overlap. A good example is transcription factor binding sites determined by \href{http://en.wikipedia.org/wiki/ChIP-sequencing}{ChIP-seq} experiments. We will introduce ChIP-seq in more detail in Chapter \ref{chipseq}. However, in these types of experiments and the following analysis, one usually ends up with genomic regions that are bound by transcription factors. One of the standard next questions would be to annotate binding sites with genomic annotations such as promoter, exon, intron and/or CpG islands, which are important for gene regulation. Below is a demonstration of how transcription factor binding sites can be annotated using CpG islands\index{CpG island}. First, we will get the subset of binding sites that overlap with the CpG islands. In this case, binding sites are ChIP-seq peaks.\index{ChIP-seq}

In the code snippet below, we read the ChIP-seq analysis output files using the \texttt{genomation::readBroadPeak()} function. This function directly outputs a \texttt{GRanges} object. These output files are similar to BED files, where the location of the predicted binding sites are written out in a tabular format with some analysis-related scores and/or P-values. After reading the files, we can find the subset of peaks that overlap with the CpG islands using the \texttt{subsetByoverlaps()} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(genomation)}
\NormalTok{filePathPeaks=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,   }
              \StringTok{"wgEncodeHaibTfbsGm12878Sp1Pcr1xPkRep1.broadPeak.gz"}\NormalTok{,}
                      \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}

\CommentTok{# read the peaks from a bed file}
\NormalTok{pk1.gr=}\KeywordTok{readBroadPeak}\NormalTok{(filePathPeaks)}

\CommentTok{# get the peaks that overlap with CpG islands}
\KeywordTok{subsetByOverlaps}\NormalTok{(pk1.gr,cpgi.gr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## GRanges object with 44 ranges and 5 metadata columns:
##        seqnames            ranges strand |        name     score signalValue
##           <Rle>         <IRanges>  <Rle> | <character> <integer>   <numeric>
##    [1]    chr21   9825360-9826582      * |   peak14562        56      183.11
##    [2]    chr21   9968469-9968984      * |   peak14593       947     3064.92
##    [3]    chr21 15755368-15755956      * |   peak14828        90      291.90
##    [4]    chr21 19191579-19192525      * |   peak14840       290      940.03
##    [5]    chr21 26979619-26980048      * |   peak14854        32      104.67
##    ...      ...               ...    ... .         ...       ...         ...
##   [40]    chr21 46237464-46237809      * |   peak15034        32      106.36
##   [41]    chr21 46707702-46708084      * |   peak15037        67      217.02
##   [42]    chr21 46961552-46961875      * |   peak15039        38      124.31
##   [43]    chr21 47743587-47744125      * |   peak15050       353     1141.58
##   [44]    chr21 47878412-47878891      * |   peak15052       104      338.78
##           pvalue    qvalue
##        <integer> <integer>
##    [1]        -1        -1
##    [2]        -1        -1
##    [3]        -1        -1
##    [4]        -1        -1
##    [5]        -1        -1
##    ...       ...       ...
##   [40]        -1        -1
##   [41]        -1        -1
##   [42]        -1        -1
##   [43]        -1        -1
##   [44]        -1        -1
##   -------
##   seqinfo: 23 sequences from an unspecified genome; no seqlengths
\end{verbatim}

For each CpG island, we can count the number of peaks that overlap with a given CpG island with \texttt{GenomicRanges::countOverlaps()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{counts=}\KeywordTok{countOverlaps}\NormalTok{(pk1.gr,cpgi.gr)}
\KeywordTok{head}\NormalTok{(counts)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0 0 0 0 0 0
\end{verbatim}

The \texttt{GenomicRanges::findOverlaps()} function can be used to see one-to-one overlaps between peaks and CpG islands. It returns a matrix showing which peak overlaps which CpG island.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{findOverlaps}\NormalTok{(pk1.gr,cpgi.gr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Hits object with 45 hits and 0 metadata columns:
##        queryHits subjectHits
##        <integer>   <integer>
##    [1]     14562           1
##    [2]     14593           3
##    [3]     14828           8
##    [4]     14840          13
##    [5]     14854          16
##    ...       ...         ...
##   [41]     15034         155
##   [42]     15037         166
##   [43]     15039         176
##   [44]     15050         192
##   [45]     15052         200
##   -------
##   queryLength: 26121 / subjectLength: 205
\end{verbatim}

Another interesting thing would be to look at the distances to the nearest CpG islands for each peak. In addition, just finding the nearest CpG island could also be interesting. Oftentimes, you will need to find the nearest TSS\index{transcription start site (TSS)} or gene to your regions of interest, and the code below is handy for doing that using the \texttt{nearest()} and \texttt{distanceToNearest()} functions, the resulting plot is shown in Figure \ref{fig:findNearest}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# find nearest CpGi to each TSS}
\NormalTok{n.ind=}\KeywordTok{nearest}\NormalTok{(pk1.gr,cpgi.gr)}
\CommentTok{# get distance to nearest}
\NormalTok{dists=}\KeywordTok{distanceToNearest}\NormalTok{(pk1.gr,cpgi.gr,}\DataTypeTok{select=}\StringTok{"arbitrary"}\NormalTok{)}
\NormalTok{dists}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Hits object with 620 hits and 1 metadata column:
##         queryHits subjectHits |  distance
##         <integer>   <integer> | <integer>
##     [1]     14440           1 |    384188
##     [2]     14441           1 |    382968
##     [3]     14442           1 |    381052
##     [4]     14443           1 |    379311
##     [5]     14444           1 |    376978
##     ...       ...         ... .       ...
##   [616]     15055         205 |     26212
##   [617]     15056         205 |     27402
##   [618]     15057         205 |     30468
##   [619]     15058         205 |     31611
##   [620]     15059         205 |     34090
##   -------
##   queryLength: 26121 / subjectLength: 205
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# histogram of the distances to nearest TSS}
\NormalTok{dist2plot=}\KeywordTok{mcols}\NormalTok{(dists)[,}\DecValTok{1}\NormalTok{]}
\KeywordTok{hist}\NormalTok{(}\KeywordTok{log10}\NormalTok{(dist2plot),}\DataTypeTok{xlab=}\StringTok{"log10(dist to nearest TSS)"}\NormalTok{,}
     \DataTypeTok{main=}\StringTok{"Distances"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{06-genomicIntervals_files/figure-latex/findNearest-1} 

}

\caption{Histogram of distances of CpG islands to the nearest TSSes.}\label{fig:findNearest}
\end{figure}

\hypertarget{dealing-with-mapped-high-throughput-sequencing-reads}{%
\section{Dealing with mapped high-throughput sequencing reads}\label{dealing-with-mapped-high-throughput-sequencing-reads}}

The reads from sequencing machines are usually pre-processed and aligned to the genome with the help of specific bioinformatics tools. We have introduced the details of general read processing, quality check and alignment methods in chapter \ref{processingReads}. In this section we will deal with mapped reads. Since each mapped read has a start and end position the genome, mapped reads can be thought of as genomic intervals stored in a file. After mapping, the next task is to quantify the enrichment of those aligned reads in the regions of interest. You may want to count how many reads overlap with your promoter set of interest or you may want to quantify RNA-seq reads\index{RNA-seq} overlap with exons. This is similar to operations on genomic intervals which are described previously. If you can read all your alignments into memory and create a \texttt{GRanges} object, you can apply the previously described operations. However, most of the time we can not read all mapped reads into memory, so we have to use specialized tools to query and quantify alignments on a given set of regions. One of the most common alignment formats is SAM/BAM format, most aligners will produce SAM/BAM output or you will be able to convert your specific alignment format to SAM/BAM format. The BAM format is a binary version of the human-readable SAM format. The SAM format has specific columns that contain different kind of information about the alignment such as mismatches, qualities etc. (see {[}\url{http://samtools.sourceforge.net/SAM1.pdf}{]} for SAM format specification).

\hypertarget{counting-mapped-reads-for-a-set-of-regions}{%
\subsection{Counting mapped reads for a set of regions}\label{counting-mapped-reads-for-a-set-of-regions}}

The \texttt{Rsamtools} package has functions to query BAM files\index{R Packages!\texttt{Rsamtools}}. The function we will use in the first example is the \texttt{countBam()} function, which takes input of the BAM file and param argument. The \texttt{param} argument takes a \texttt{ScanBamParam} object. The object is instantiated using \texttt{ScanBamParam()} and contains parameters for scanning the BAM file. The example below is a simple example where \texttt{ScanBamParam()} only includes regions of interest, promoters on chr21.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{promoter.gr=tss.gr}
\KeywordTok{start}\NormalTok{(promoter.gr)=}\KeywordTok{start}\NormalTok{(promoter.gr)}\OperatorTok{-}\DecValTok{1000}
\KeywordTok{end}\NormalTok{(promoter.gr)  =}\KeywordTok{end}\NormalTok{(promoter.gr)}\OperatorTok{+}\DecValTok{1000}
\NormalTok{promoter.gr=promoter.gr[}\KeywordTok{seqnames}\NormalTok{(promoter.gr)}\OperatorTok{==}\StringTok{"chr21"}\NormalTok{]}

\KeywordTok{library}\NormalTok{(Rsamtools)}
\NormalTok{bamfilePath=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
            \StringTok{"wgEncodeHaibTfbsGm12878Sp1Pcr1xAlnRep1.chr21.bam"}\NormalTok{,}
                      \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}

\CommentTok{# get reads for regions of interest from the bam file}
\NormalTok{param <-}\StringTok{ }\KeywordTok{ScanBamParam}\NormalTok{(}\DataTypeTok{which=}\NormalTok{promoter.gr)}
\NormalTok{counts=}\KeywordTok{countBam}\NormalTok{(bamfilePath, }\DataTypeTok{param=}\NormalTok{param)}
\end{Highlighting}
\end{Shaded}

Alternatively, aligned reads can be read in using the \texttt{GenomicAlignments} package (which on this occasion relies on the \texttt{Rsamtools} package).\index{R Packages!\texttt{GenomicAlignments}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(GenomicAlignments)}
\NormalTok{alns <-}\StringTok{ }\KeywordTok{readGAlignments}\NormalTok{(bamfilePath, }\DataTypeTok{param=}\NormalTok{param)}
\end{Highlighting}
\end{Shaded}

\hypertarget{dealing-with-continuous-scores-over-the-genome}{%
\section{Dealing with continuous scores over the genome}\label{dealing-with-continuous-scores-over-the-genome}}

Most high-throughput data can be viewed as a continuous score over the bases of the genome. In case of RNA-seq or ChIP-seq experiments, the data can be represented as read coverage values per genomic base position\index{RNA-seq}\index{ChIP-seq}. In addition, other information (not necessarily from high-throughput experiments) can be represented this way. The GC content and conservation scores per base are prime examples of other data sets that can be represented as scores over the genome. This sort of data can be stored as a generic text file or can have special formats such as Wig (stands for wiggle) from UCSC, or the bigWig format, which is an indexed binary format of the wig files\index{wig file}\index{bigWig file}. The bigWig format is great for data that covers a large fraction of the genome with varying scores, because the file is much smaller than regular text files that have the same information and it can be queried more easily since it is indexed.

In R/Bioconductor, continuous data can also be represented in a compressed format, called Rle vector, which stands for run-length encoded vector. This gives superior memory performance over regular vectors because repeating consecutive values are represented as one value in the Rle vector (see Figure \ref{fig:Rle}).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/Rle_demo} 

}

\caption{Rle encoding explained.}\label{fig:Rle}
\end{figure}

Typically, for genome-wide data you will have an \texttt{RleList} object, which is a list of Rle vectors per chromosome. You can obtain such vectors by reading the reads in and calling the \texttt{coverage()} function from the \texttt{GenomicRanges} package. Let's try that on the above data set.\index{R Packages!\texttt{GenomicRanges}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{covs=}\KeywordTok{coverage}\NormalTok{(alns) }\CommentTok{# get coverage vectors}
\NormalTok{covs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## RleList of length 24
## $chr1
## integer-Rle of length 249250621 with 1 run
##   Lengths: 249250621
##   Values :         0
## 
## $chr2
## integer-Rle of length 243199373 with 1 run
##   Lengths: 243199373
##   Values :         0
## 
## $chr3
## integer-Rle of length 198022430 with 1 run
##   Lengths: 198022430
##   Values :         0
## 
## $chr4
## integer-Rle of length 191154276 with 1 run
##   Lengths: 191154276
##   Values :         0
## 
## $chr5
## integer-Rle of length 180915260 with 1 run
##   Lengths: 180915260
##   Values :         0
## 
## ...
## <19 more elements>
\end{verbatim}

Alternatively, you can get the coverage from the BAM file directly. Below, we are getting the coverage directly from the BAM file for our previously defined promoters.\index{BAM/SAM file}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{covs=}\KeywordTok{coverage}\NormalTok{(bamfilePath, }\DataTypeTok{param=}\NormalTok{param) }\CommentTok{# get coverage vectors}
\end{Highlighting}
\end{Shaded}

One of the most common ways of storing score data is, as mentioned, the wig or bigWig format. Most of the ENCODE project\index{ENCODE} data can be downloaded in bigWig format. In addition, conservation scores can also be downloaded in the wig/bigWig format. You can import bigWig files into R using the \texttt{import()} function from the \texttt{rtracklayer} package. However, it is generally not advisable to read the whole bigWig file in memory as was the case with BAM files. Usually, you will be interested in only a fraction of the genome, such as promoters, exons etc. So it is best that you extract the data for those regions and read those into memory rather than the whole file. Below we read a bigWig file only for the bases on promoters. The operation returns an \texttt{GRanges} object with the score column which indicates the scores in the bigWig file per genomic region.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rtracklayer)}

\CommentTok{# File from ENCODE ChIP-seq tracks}
\NormalTok{bwFile=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                      \StringTok{"wgEncodeHaibTfbsA549.chr21.bw"}\NormalTok{,}
                      \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{bw.gr=}\KeywordTok{import}\NormalTok{(bwFile, }\DataTypeTok{which=}\NormalTok{promoter.gr) }\CommentTok{# get coverage vectors}
\NormalTok{bw.gr}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## GRanges object with 9205 ranges and 1 metadata column:
##          seqnames            ranges strand |     score
##             <Rle>         <IRanges>  <Rle> | <numeric>
##      [1]    chr21   9825456-9825457      * |         1
##      [2]    chr21   9825458-9825464      * |         2
##      [3]    chr21   9825465-9825466      * |         4
##      [4]    chr21   9825467-9825470      * |         5
##      [5]    chr21           9825471      * |         6
##      ...      ...               ...    ... .       ...
##   [9201]    chr21 48055809-48055856      * |         2
##   [9202]    chr21 48055857-48055858      * |         1
##   [9203]    chr21 48055872-48055921      * |         1
##   [9204]    chr21 48055944-48055993      * |         1
##   [9205]    chr21 48056069-48056118      * |         1
##   -------
##   seqinfo: 1 sequence from an unspecified genome
\end{verbatim}

Following this we can create an \texttt{RleList} object from the \texttt{GRanges} with the \texttt{coverage()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cov.bw=}\KeywordTok{coverage}\NormalTok{(bw.gr,}\DataTypeTok{weight =} \StringTok{"score"}\NormalTok{)}

\CommentTok{# or get this directly from}
\NormalTok{cov.bw=}\KeywordTok{import}\NormalTok{(bwFile, }\DataTypeTok{which=}\NormalTok{promoter.gr,}\DataTypeTok{as =} \StringTok{"RleList"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{extracting-subsections-of-rle-and-rlelist-objects}{%
\subsection{Extracting subsections of Rle and RleList objects}\label{extracting-subsections-of-rle-and-rlelist-objects}}

Frequently, we will need to extract subsections of the Rle vectors or \texttt{RleList} objects.
We will need to do this to visualize that subsection or get some statistics out
of those sections. For example, we could be interested in average coverage per
base for the regions we are interested in. We have to extract those regions
from the \texttt{RleList} object and apply summary statistics. Below, we show how to extract
subsections of the \texttt{RleList} object. We are extracting promoter regions from the ChIP-seq\index{ChIP-seq}
read coverage \texttt{RleList}. Following that, we will plot one of the promoter's coverage values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myViews=}\KeywordTok{Views}\NormalTok{(cov.bw,}\KeywordTok{as}\NormalTok{(promoter.gr,}\StringTok{"IRangesList"}\NormalTok{)) }\CommentTok{# get subsets of coverage}
\CommentTok{# there is a views object for each chromosome}
\NormalTok{myViews}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## RleViewsList object of length 1:
## $chr21
## Views on a 48129895-length Rle subject
## 
## views:
##          start      end width
##   [1] 42218039 42220039  2001 [2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
##   [2] 17441841 17443841  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
##   [3] 17565698 17567698  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
##   [4] 30395937 30397937  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
##   [5] 27542138 27544138  2001 [1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 ...]
##   [6] 27511708 27513708  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
##   [7] 32930290 32932290  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
##   [8] 27542446 27544446  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
##   [9] 28338439 28340439  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
##   ...      ...      ...   ... ...
## [370] 47517032 47519032  2001 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...]
## [371] 47648157 47650157  2001 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...]
## [372] 47603373 47605373  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
## [373] 47647738 47649738  2001 [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 ...]
## [374] 47704236 47706236  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
## [375] 47742785 47744785  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
## [376] 47881383 47883383  2001 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...]
## [377] 48054506 48056506  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
## [378] 48024035 48026035  2001 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myViews[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Views on a 48129895-length Rle subject
## 
## views:
##          start      end width
##   [1] 42218039 42220039  2001 [2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
##   [2] 17441841 17443841  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
##   [3] 17565698 17567698  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
##   [4] 30395937 30397937  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
##   [5] 27542138 27544138  2001 [1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 ...]
##   [6] 27511708 27513708  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
##   [7] 32930290 32932290  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
##   [8] 27542446 27544446  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
##   [9] 28338439 28340439  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
##   ...      ...      ...   ... ...
## [370] 47517032 47519032  2001 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...]
## [371] 47648157 47650157  2001 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...]
## [372] 47603373 47605373  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
## [373] 47647738 47649738  2001 [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 ...]
## [374] 47704236 47706236  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
## [375] 47742785 47744785  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
## [376] 47881383 47883383  2001 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...]
## [377] 48054506 48056506  2001 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]
## [378] 48024035 48026035  2001 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get the coverage vector from the 5th view and plot}
\KeywordTok{plot}\NormalTok{(myViews[[}\DecValTok{1}\NormalTok{]][[}\DecValTok{5}\NormalTok{]],}\DataTypeTok{type=}\StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{06-genomicIntervals_files/figure-latex/getViews-1} 

}

\caption{Coverage vector extracted from the RleList via the Views() function is plotted as a line plot.}\label{fig:getViews}
\end{figure}

Next, we are interested in average coverage per base for the promoters using summary
functions that work on the \texttt{Views} object.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get the mean of the views}
\KeywordTok{head}\NormalTok{(}
  \KeywordTok{viewMeans}\NormalTok{(myViews[[}\DecValTok{1}\NormalTok{]])}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2258871 0.3498251 1.2243878 0.4997501 2.0904548 0.6996502
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get the max of the views}
\KeywordTok{head}\NormalTok{(}
  \KeywordTok{viewMaxs}\NormalTok{(myViews[[}\DecValTok{1}\NormalTok{]])}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  2  4 12  4 21  6
\end{verbatim}

\hypertarget{genomic-intervals-with-more-information-summarizedexperiment-class}{%
\section{Genomic intervals with more information: SummarizedExperiment class}\label{genomic-intervals-with-more-information-summarizedexperiment-class}}

As we have seen, genomic intervals can be mainly contained in a \texttt{GRanges} object.
It can also contain additional columns associated with each interval. Here
you can save information such as read counts or other scores associated with the
interval. However,
genomic data often have many layers. With \texttt{GRanges} you can have a table
associated with the intervals, but what happens if you have many tables and each
table has some metadata associated with it. In addition, rows and columns might
have additional annotation that cannot be contained by row or column names.
For these cases, the \texttt{SummarizedExperiment} class is ideal. It can hold multi-layered
tabular data associated with each genomic interval and the meta-data associated with
rows and columns, or associated with each table. For example, genomic intervals
associated with the \texttt{SummarizedExperiment} object can be gene locations, and
each tabular data structure can be RNA-seq read counts in a time course experiment.
Each table could represent different conditions in which experiments are performed.
The \texttt{SummarizedExperiment} class is outlined in the figure below (Figure \ref{fig:SumExpOv} ).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/Summarized.Experiment} 

}

\caption{Overview of SummarizedExperiment class and functions. Adapted from the SummarizedExperiment package vignette}\label{fig:SumExpOv}
\end{figure}

\hypertarget{create-a-summarizedexperiment-object}{%
\subsection{Create a SummarizedExperiment object}\label{create-a-summarizedexperiment-object}}

Here we show how to create a basic \texttt{SummarizedExperiment} object. We will first
create a matrix of read counts. This matrix will represent read counts from
a series of RNA-seq experiments from different time points. Following that,
we create a \texttt{GRanges} object to represent the locations of the genes, and a table
for column annotations. This will include the names for the columns and any
other value we want to represent. Finally, we will create a \texttt{SummarizedExperiment}
object by combining all those pieces.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# simulate an RNA-seq read counts table}
\NormalTok{nrows <-}\StringTok{ }\DecValTok{200}
\NormalTok{ncols <-}\StringTok{ }\DecValTok{6}
\NormalTok{counts <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{runif}\NormalTok{(nrows }\OperatorTok{*}\StringTok{ }\NormalTok{ncols, }\DecValTok{1}\NormalTok{, }\FloatTok{1e4}\NormalTok{), nrows)}

\CommentTok{# create gene locations}
\NormalTok{rowRanges <-}\StringTok{ }\KeywordTok{GRanges}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"chr1"}\NormalTok{, }\StringTok{"chr2"}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DecValTok{150}\NormalTok{)),}
                     \KeywordTok{IRanges}\NormalTok{(}\KeywordTok{floor}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\DecValTok{200}\NormalTok{, }\FloatTok{1e5}\NormalTok{, }\FloatTok{1e6}\NormalTok{)), }\DataTypeTok{width=}\DecValTok{100}\NormalTok{),}
                     \DataTypeTok{strand=}\KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"+"}\NormalTok{, }\StringTok{"-"}\NormalTok{), }\DecValTok{200}\NormalTok{, }\OtherTok{TRUE}\NormalTok{),}
                     \DataTypeTok{feature_id=}\KeywordTok{paste0}\NormalTok{(}\StringTok{"gene"}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{200}\NormalTok{))}

\CommentTok{# create table for the columns}
\NormalTok{colData <-}\StringTok{ }\KeywordTok{DataFrame}\NormalTok{(}\DataTypeTok{timepoint=}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{,}
                     \DataTypeTok{row.names=}\NormalTok{LETTERS[}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{])}


\CommentTok{# create SummarizedExperiment object}
\NormalTok{se=}\KeywordTok{SummarizedExperiment}\NormalTok{(}\DataTypeTok{assays=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{counts=}\NormalTok{counts),}
                     \DataTypeTok{rowRanges=}\NormalTok{rowRanges, }\DataTypeTok{colData=}\NormalTok{colData)}

\NormalTok{se}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## class: RangedSummarizedExperiment 
## dim: 200 6 
## metadata(0):
## assays(1): counts
## rownames: NULL
## rowData names(1): feature_id
## colnames(6): A B ... E F
## colData names(1): timepoint
\end{verbatim}

\hypertarget{subset-and-manipulate-the-summarizedexperiment-object}{%
\subsection{Subset and manipulate the SummarizedExperiment object}\label{subset-and-manipulate-the-summarizedexperiment-object}}

Now that we have a \texttt{SummarizedExperiment} object, we can subset it and extract/change
parts of it.

\hypertarget{extracting-parts-of-the-object}{%
\subsubsection{Extracting parts of the object}\label{extracting-parts-of-the-object}}

\texttt{colData()} and \texttt{rowData()} extract the column-associated and row-associated
tables. \texttt{metaData()} extracts the meta-data table if there is any table associated.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{colData}\NormalTok{(se) }\CommentTok{# extract column associated data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## DataFrame with 6 rows and 1 column
##   timepoint
##   <integer>
## A         1
## B         2
## C         3
## D         4
## E         5
## F         6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rowData}\NormalTok{(se) }\CommentTok{# extrac row associated data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## DataFrame with 200 rows and 1 column
##      feature_id
##     <character>
## 1         gene1
## 2         gene2
## 3         gene3
## 4         gene4
## 5         gene5
## ...         ...
## 196     gene196
## 197     gene197
## 198     gene198
## 199     gene199
## 200     gene200
\end{verbatim}

To extract the main table or tables that contain the values of interest such
as read counts, we must use the \texttt{assays()} function. This returns a list of
\texttt{DataFrame} objects associated with the object.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{assays}\NormalTok{(se) }\CommentTok{# extract list of assays}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of length 1
## names(1): counts
\end{verbatim}

You can use names with \texttt{\$} or \texttt{{[}{]}} notation to extract specific tables from the list.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{assays}\NormalTok{(se)}\OperatorTok{$}\NormalTok{counts }\CommentTok{# get the table named "counts"}

\KeywordTok{assays}\NormalTok{(se)[[}\DecValTok{1}\NormalTok{]] }\CommentTok{# get the first table}
\end{Highlighting}
\end{Shaded}

\hypertarget{subsetting}{%
\subsubsection{Subsetting}\label{subsetting}}

Subsetting is easy using \texttt{{[}\ {]}} notation. This is similar to the way we
subset data frames or matrices.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# subset the first five transcripts and first three samples}
\NormalTok{se[}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## class: RangedSummarizedExperiment 
## dim: 5 3 
## metadata(0):
## assays(1): counts
## rownames: NULL
## rowData names(1): feature_id
## colnames(3): A B C
## colData names(1): timepoint
\end{verbatim}

One can also use the \texttt{\$} operator to subset based on \texttt{colData()} columns. You can
extract certain samples or in our case, time points.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{se[, se}\OperatorTok{$}\NormalTok{timepoint }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

In addition, as \texttt{SummarizedExperiment} objects are \texttt{GRanges} objects on steroids,
they support all of the \texttt{findOverlaps()} methods and associated functions that
work on \texttt{GRanges} objects.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Subset for only rows which are in chr1:100,000-1,100,000 }
\NormalTok{roi <-}\StringTok{ }\KeywordTok{GRanges}\NormalTok{(}\DataTypeTok{seqnames=}\StringTok{"chr1"}\NormalTok{, }\DataTypeTok{ranges=}\DecValTok{100000}\OperatorTok{:}\DecValTok{1100000}\NormalTok{)}
\KeywordTok{subsetByOverlaps}\NormalTok{(se, roi)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## class: RangedSummarizedExperiment 
## dim: 50 6 
## metadata(0):
## assays(1): counts
## rownames: NULL
## rowData names(1): feature_id
## colnames(6): A B ... E F
## colData names(1): timepoint
\end{verbatim}

\hypertarget{visualizing-and-summarizing-genomic-intervals}{%
\section{Visualizing and summarizing genomic intervals}\label{visualizing-and-summarizing-genomic-intervals}}

Data integration and visualization is cornerstone of genomic data analysis. Below, we will
show different ways of integrating and visualizing genomic intervals. These methods
can be used to visualize large amounts of data in a locus-specific or multi-loci
manner.

\hypertarget{visualizing-intervals-on-a-locus-of-interest}{%
\subsection{Visualizing intervals on a locus of interest}\label{visualizing-intervals-on-a-locus-of-interest}}

Oftentimes, we will be interested in a particular genomic locus and try to visualize
different genomic datasets over that locus. This is similar to looking at the data
over one of the genome browsers. Below we will display genes, GpG islands and read \index{R Packages!\texttt{Gviz}}
coverage from a ChIP-seq experiment using the \texttt{Gviz} package\index{ChIP-seq}. For the \texttt{Gviz} package, we first need to
set the tracks to display. The tracks can be in various formats. They can be R
objects such as \texttt{IRanges},\texttt{GRanges} and \texttt{data.frame}, or they can be in flat file formats
such as bigWig, BED, and BAM. After the tracks are set, we can display them with the
\texttt{plotTracks} function, the resulting plot is shown in Figure \ref{fig:GvizExchp6}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(Gviz)}
\CommentTok{# set tracks to display}

\CommentTok{# set CpG island track}
\NormalTok{cpgi.track=}\KeywordTok{AnnotationTrack}\NormalTok{(cpgi.gr,}
                           \DataTypeTok{name =} \StringTok{"CpG"}\NormalTok{)}

\CommentTok{# set gene track}
\CommentTok{# we will get this from EBI Biomart webservice}
\NormalTok{gene.track <-}\StringTok{ }\KeywordTok{BiomartGeneRegionTrack}\NormalTok{(}\DataTypeTok{genome =} \StringTok{"hg19"}\NormalTok{,}
                                    \DataTypeTok{chromosome =} \StringTok{"chr21"}\NormalTok{, }
                                    \DataTypeTok{start =} \DecValTok{27698681}\NormalTok{, }\DataTypeTok{end =} \DecValTok{28083310}\NormalTok{,}
                                    \DataTypeTok{name =} \StringTok{"ENSEMBL"}\NormalTok{)}


\CommentTok{# set track for ChIP-seq coverage}
\NormalTok{chipseqFile=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                      \StringTok{"wgEncodeHaibTfbsA549.chr21.bw"}\NormalTok{,}
                      \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{cov.track=}\KeywordTok{DataTrack}\NormalTok{(chipseqFile,}\DataTypeTok{type =} \StringTok{"l"}\NormalTok{,}
                    \DataTypeTok{name=}\StringTok{"coverage"}\NormalTok{)}
  
\CommentTok{# call the display function plotTracks}
\NormalTok{track.list=}\KeywordTok{list}\NormalTok{(cpgi.track,gene.track,cov.track)}
\KeywordTok{plotTracks}\NormalTok{(track.list,}\DataTypeTok{from=}\DecValTok{27698681}\NormalTok{,}\DataTypeTok{to=}\DecValTok{28083310}\NormalTok{,}\DataTypeTok{chromsome=}\StringTok{"chr21"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{06-genomicIntervals_files/figure-latex/GvizExchp6-1} 

}

\caption{Genomic data tracks visualized using the Gviz functions.}\label{fig:GvizExchp6}
\end{figure}

\hypertarget{summaries-of-genomic-intervals-on-multiple-loci}{%
\subsection{Summaries of genomic intervals on multiple loci}\label{summaries-of-genomic-intervals-on-multiple-loci}}

Looking at data one region at a time could be inefficient. One can summarize
different data sets over thousands of regions of interest and identify patterns.
These summaries can include different data types such as motifs, read coverage
and other scores associated with genomic intervals. The \texttt{genomation} package can
summarize and help identify patterns in the datasets. The datasets can have
different kinds of information and multiple file types can be used such as BED, GFF, BAM and bigWig. We will look at H3K4me3 ChIP-seq \index{ChIP-seq} \index{histone modification}and DNAse-seq signals from the H1 embryonic stem cell line. H3K4me3 is usually associated with promoters and regions with high DNAse-seq signal are associated with accessible regions, which means mostly regulatory regions. We will summarize those datasets around the transcription start sites (TSS)\index{transcription start site (TSS)} of genes on chromosome 20 of the human hg19 assembly. We will first read the genes and extract the region around the TSS, 500bp upstream and downstream. We will then create a matrix of ChIP-seq scores for those regions. Each row will represent a region around a specific TSS and columns will be the scores per base. We will then plot average enrichment values around the TSS of genes on chromosome 20.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get transcription start sites on chr20}
\KeywordTok{library}\NormalTok{(genomation)}
\NormalTok{transcriptFile=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                      \StringTok{"refseq.hg19.chr20.bed"}\NormalTok{,}
                      \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{feat=}\KeywordTok{readTranscriptFeatures}\NormalTok{(transcriptFile,}
                            \DataTypeTok{remove.unusual =} \OtherTok{TRUE}\NormalTok{,}
                            \DataTypeTok{up.flank =} \DecValTok{500}\NormalTok{, }\DataTypeTok{down.flank =} \DecValTok{500}\NormalTok{)}
\NormalTok{prom=feat}\OperatorTok{$}\NormalTok{promoters }\CommentTok{# get promoters from the features}


\CommentTok{# get for H3K4me3 values around TSSes}
\CommentTok{# we use strand.aware=TRUE so - strands will}
\CommentTok{# be reversed}
\NormalTok{H3K4me3File=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                      \StringTok{"H1.ESC.H3K4me3.chr20.bw"}\NormalTok{,}
                      \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{sm=}\KeywordTok{ScoreMatrix}\NormalTok{(H3K4me3File,prom,}
               \DataTypeTok{type=}\StringTok{"bigWig"}\NormalTok{,}\DataTypeTok{strand.aware =} \OtherTok{TRUE}\NormalTok{)}


\CommentTok{# look for the average enrichment}
\KeywordTok{plotMeta}\NormalTok{(sm, }\DataTypeTok{profile.names =} \StringTok{"H3K4me3"}\NormalTok{, }\DataTypeTok{xcoords =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{500}\NormalTok{,}\DecValTok{500}\NormalTok{),}
         \DataTypeTok{ylab=}\StringTok{"H3K4me3 enrichment"}\NormalTok{,}\DataTypeTok{dispersion =} \StringTok{"se"}\NormalTok{,}
         \DataTypeTok{xlab=}\StringTok{"bases around TSS"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{06-genomicIntervals_files/figure-latex/metaRegionchp6-1} 

}

\caption{Meta-region plot using genomation.}\label{fig:metaRegionchp6}
\end{figure}

The resulting plot is shown in Figure \ref{fig:metaRegionchp6}. The pattern we see is expected, there is a dip just around TSS \index{transcription start site (TSS)}and the signal is more
intense downstream of the TSS.

We can also plot a heatmap where each row is a
region around the TSS and color coded by enrichment. This can show us not only the
general pattern, as in the meta-region
plot, but also how many of the regions produce such a pattern. The \texttt{heatMatrix()} function shown below achieves that. The resulting heatmap plot is shown in Figure \ref{fig:heatmatrix1Chp6}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{heatMatrix}\NormalTok{(sm,}\DataTypeTok{order=}\OtherTok{TRUE}\NormalTok{,}\DataTypeTok{xcoords =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{500}\NormalTok{,}\DecValTok{500}\NormalTok{),}
           \DataTypeTok{xlab=}\StringTok{"bases around TSS"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{06-genomicIntervals_files/figure-latex/heatmatrix1Chp6-1} 

}

\caption{Heatmap of enrichment of H3K4me2 around the TSS.}\label{fig:heatmatrix1Chp6}
\end{figure}

Here we saw that about half of the regions do not have any signal. In addition it seems the multi-modal profile we have observed earlier is more complicated. Certain regions seem to have signal on both sides of the TSS, \index{transcription start site (TSS)}whereas others have signal mostly on the downstream side.

Normally, there would be more than one experiment or we can integrate datasets from
public repositories. In this case, we can see how different signals look in the regions we are interested in. Now, we will also use DNAse-seq data and create a list of matrices with our datasets and plot the average profile of the signals from both datasets. The resulting meta-region plot is shown in Figure \ref{fig:heatmatrixlistchp6}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DNAseFile=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                      \StringTok{"H1.ESC.dnase.chr20.bw"}\NormalTok{,}
                      \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}

\NormalTok{sml=}\KeywordTok{ScoreMatrixList}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DataTypeTok{H3K4me3=}\NormalTok{H3K4me3File,}
                      \DataTypeTok{DNAse=}\NormalTok{DNAseFile),prom,}
                      \DataTypeTok{type=}\StringTok{"bigWig"}\NormalTok{,}\DataTypeTok{strand.aware =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{plotMeta}\NormalTok{(sml)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{06-genomicIntervals_files/figure-latex/heatmatrixlistchp6-1} 

}

\caption{Average profiles of DNAse and H3K4me3 ChIP-seq.}\label{fig:heatmatrixlistchp6}
\end{figure}

We should now look at the heatmaps side by side and we should also cluster the rows
based on their similarity. We will be using \texttt{multiHeatMatrix} since we have multiple \texttt{ScoreMatrix} objects in the list. In this case, we will also use the \texttt{winsorize} argument to limit extreme values,
every score above 95th percentile will be equalized the value of the 95th percentile. In addition, \texttt{heatMatrix} and \texttt{multiHeatMatrix} can cluster the rows.
Below, we will be using k-means clustering with 3 clusters.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1029}\NormalTok{)}
\KeywordTok{multiHeatMatrix}\NormalTok{(sml,}\DataTypeTok{order=}\OtherTok{TRUE}\NormalTok{,}\DataTypeTok{xcoords =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{500}\NormalTok{,}\DecValTok{500}\NormalTok{),}
                \DataTypeTok{xlab=}\StringTok{"bases around TSS"}\NormalTok{,}\DataTypeTok{winsorize =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{95}\NormalTok{),}
                \DataTypeTok{matrix.main =} \KeywordTok{c}\NormalTok{(}\StringTok{"H3K4me3"}\NormalTok{,}\StringTok{"DNAse"}\NormalTok{),}
                \DataTypeTok{column.scale=}\OtherTok{TRUE}\NormalTok{,}
                \DataTypeTok{clustfun=}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{kmeans}\NormalTok{(x, }\DataTypeTok{centers=}\DecValTok{3}\NormalTok{)}\OperatorTok{$}\NormalTok{cluster)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{06-genomicIntervals_files/figure-latex/multiHeatMatrix-1} 

}

\caption{Heatmaps of H3K4me3 and DNAse data.}\label{fig:multiHeatMatrix}
\end{figure}

The resulting heatmaps are shown in Figure \ref{fig:multiHeatMatrix}. These plots revealed a different picture than we have observed before. Almost half of the promoters have no signal for DNAse or H3K4me3; these\index{histone modification} regions are probably not active and associated genes are not expressed. For regions with the H3K4me3 signal, there are two major patterns: one pattern where both downstream and upstream of the TSS are enriched, and on the other pattern, mostly downstream of the TSS is enriched.\index{transcription start site (TSS)}

\hypertarget{making-karyograms-and-circos-plots}{%
\subsection{Making karyograms and circos plots}\label{making-karyograms-and-circos-plots}}

Chromosomal karyograms and circos plots are beneficial for displaying data over the
whole genome of chromosomes of interest, although the information that can be
displayed over these large regions are usually not very clear and only large trends
can be discerned by eye, such as loss of methylation in large regions or genome-wide.
Below, we show how to use the \texttt{ggbio} package for plotting.
This package has a slightly different syntax than base graphics. The syntax follows
``grammar of graphics'' logic, and depends on the \texttt{ggplot2} package we introduced in Chapter \ref{Rintro}. It is
a deconstructed way of thinking about the plot. You add your data and apply mappings
and transformations in order to achieve the final output. In \texttt{ggbio}, things are
relatively easy since a high-level function, the \texttt{autoplot} function, will recognize \index{R Packages!\texttt{ggbio}}
most of the datatypes and guess the most appropriate plot type. You can change
its behavior by applying low-level functions. We first get the sizes of chromosomes
and make a karyogram template. The empty karyogram is shown in Figure \ref{fig:karyo1}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggbio)}
\KeywordTok{data}\NormalTok{(ideoCyto, }\DataTypeTok{package =} \StringTok{"biovizBase"}\NormalTok{)}
\NormalTok{p <-}\StringTok{ }\KeywordTok{autoplot}\NormalTok{(}\KeywordTok{seqinfo}\NormalTok{(ideoCyto}\OperatorTok{$}\NormalTok{hg19), }\DataTypeTok{layout =} \StringTok{"karyogram"}\NormalTok{)}
\NormalTok{p}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{06-genomicIntervals_files/figure-latex/karyo1-1} 

}

\caption{Karyogram example.}\label{fig:karyo1}
\end{figure}

Next, we would like to plot CpG islands on this karyogram. We simply do this
by adding a layer with the \texttt{layout\_karyogram()} function. The resulting karyogram is shown in Figure \ref{fig:karyo2}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read CpG islands from a generic text file}

\NormalTok{CpGiFile=filePath=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                      \StringTok{"CpGi.hg19.table.txt"}\NormalTok{,}
                      \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{cpgi.gr=genomation}\OperatorTok{::}\KeywordTok{readGeneric}\NormalTok{(CpGiFile, }
            \DataTypeTok{chr =} \DecValTok{1}\NormalTok{, }\DataTypeTok{start =} \DecValTok{2}\NormalTok{, }\DataTypeTok{end =} \DecValTok{3}\NormalTok{,}\DataTypeTok{header=}\OtherTok{TRUE}\NormalTok{, }
          \DataTypeTok{keep.all.metadata =}\OtherTok{TRUE}\NormalTok{,}\DataTypeTok{remove.unusual=}\OtherTok{TRUE}\NormalTok{ )}

\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{layout_karyogram}\NormalTok{(cpgi.gr)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{06-genomicIntervals_files/figure-latex/karyo2-1} 

}

\caption{Karyogram of CpG islands over the human genome.}\label{fig:karyo2}
\end{figure}

Next, we would like to plot some data over the chromosomes. This could be the ChIP-seq \index{ChIP-seq}
signal
or any other signal over the genome; we will use CpG island scores from the data set
we read earlier. We will plot a point proportional to ``obsExp'' column in the data set. We use the \texttt{ylim} argument to squish the chromosomal rectangles and plot on top of those. The \texttt{aes} argument defines how the data is mapped to geometry. In this case,
the argument indicates that the points will have an x coordinate from CpG island start positions and a y coordinate from the obsExp score of CpG islands. The resulting karyogram is shown in Figure \ref{fig:karyoCpG}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{layout_karyogram}\NormalTok{(cpgi.gr, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{ start, }\DataTypeTok{y =}\NormalTok{ obsExp),}
                     \DataTypeTok{geom=}\StringTok{"point"}\NormalTok{,}
                     \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{50}\NormalTok{), }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{,}
                     \DataTypeTok{size=}\FloatTok{0.1}\NormalTok{,}\DataTypeTok{rect.height=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{06-genomicIntervals_files/figure-latex/karyoCpG-1} 

}

\caption{Karyogram of CpG islands and their observed/expected scores over the human genome.}\label{fig:karyoCpG}
\end{figure}

Another way to depict regions or quantitative signals on the chromosomes is circos plots. These are circular plots usually used for showing chromosomal rearrangements, but can also be used for depicting signals. The \texttt{ggbio} package can produce all kinds of circos plots. Below, we will show how to use that for our CpG island score example, and the resulting plot is shown in Figure \ref{fig:circosCpG}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set the chromsome in a circle}
\CommentTok{# color set to white to look transparent }
\NormalTok{p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{layout_circle}\NormalTok{(ideoCyto}\OperatorTok{$}\NormalTok{hg19, }\DataTypeTok{geom =} \StringTok{"ideo"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"white"}\NormalTok{,}
                              \DataTypeTok{colour=}\StringTok{"white"}\NormalTok{,}\DataTypeTok{cytoband =} \OtherTok{TRUE}\NormalTok{,}
                              \DataTypeTok{radius =} \DecValTok{39}\NormalTok{, }\DataTypeTok{trackWidth =} \DecValTok{2}\NormalTok{)}
\CommentTok{# plot the scores as points   }
\NormalTok{p <-}\StringTok{ }\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{layout_circle}\NormalTok{(cpgi.gr, }\DataTypeTok{geom =} \StringTok{"point"}\NormalTok{, }\DataTypeTok{grid=}\OtherTok{TRUE}\NormalTok{,}
                           \DataTypeTok{size =} \FloatTok{0.01}\NormalTok{, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ obsExp),}\DataTypeTok{color=}\StringTok{"red"}\NormalTok{,}
                       \DataTypeTok{radius =} \DecValTok{42}\NormalTok{, }\DataTypeTok{trackWidth =} \DecValTok{10}\NormalTok{)}
\CommentTok{# set the chromosome names}
\NormalTok{p <-}\StringTok{ }\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{layout_circle}\NormalTok{(}\KeywordTok{as}\NormalTok{(}\KeywordTok{seqinfo}\NormalTok{(ideoCyto}\OperatorTok{$}\NormalTok{hg19),}\StringTok{"GRanges"}\NormalTok{), }
                       \DataTypeTok{geom =} \StringTok{"text"}\NormalTok{, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{label =}\NormalTok{ seqnames), }
                      \DataTypeTok{vjust =} \DecValTok{0}\NormalTok{, }\DataTypeTok{radius =} \DecValTok{55}\NormalTok{, }\DataTypeTok{trackWidth =} \DecValTok{7}\NormalTok{,}
                      \DataTypeTok{size=}\DecValTok{3}\NormalTok{) }

\CommentTok{# display the plot}
\NormalTok{p}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{06-genomicIntervals_files/figure-latex/circosCpG-1} 

}

\caption{Circos plot for CpG island scores.}\label{fig:circosCpG}
\end{figure}

\hypertarget{exercises-4}{%
\section{Exercises}\label{exercises-4}}

The data for the exercises is within the \texttt{compGenomRData} package.

Run the following to see the data files.

\begin{verbatim}
dir(system.file("extdata",
             package="compGenomRData"))
\end{verbatim}

You will need some of those files to complete the exercises.

\hypertarget{operations-on-genomic-intervals-with-the-genomicranges-package}{%
\subsection{\texorpdfstring{Operations on genomic intervals with the \texttt{GenomicRanges} package}{Operations on genomic intervals with the GenomicRanges package}}\label{operations-on-genomic-intervals-with-the-genomicranges-package}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a \texttt{GRanges} object using the information in the table below:{[}Difficulty: \textbf{Beginner}{]}
\end{enumerate}

\begin{longtable}[]{@{}lllll@{}}
\toprule
chr & start & end & strand & score\tabularnewline
\midrule
\endhead
chr1 & 10000 & 10300 & + & 10\tabularnewline
chr1 & 11100 & 11500 & - & 20\tabularnewline
chr2 & 20000 & 20030 & + & 15\tabularnewline
\bottomrule
\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Use the \texttt{start()}, \texttt{end()}, \texttt{strand()},\texttt{seqnames()} and \texttt{width()} functions on the \texttt{GRanges}
  object you created. Figure out what they are doing. Can you get a subset of the \texttt{GRanges} object for intervals that are only on the + strand? If you can do that, try getting intervals that are on chr1. \emph{HINT:} \texttt{GRanges} objects can be subset using the \texttt{{[}\ {]}} operator, similar to data frames, but you may need
  to use \texttt{start()}, \texttt{end()} and \texttt{strand()},\texttt{seqnames()} within the \texttt{{[}{]}}. {[}Difficulty: \textbf{Beginner/Intermediate}{]}
\item
  Import mouse (mm9 assembly) CpG islands and RefSeq transcripts for chr12 from the UCSC browser as \texttt{GRanges} objects using \texttt{rtracklayer} functions. HINT: Check chapter content and modify the code there as necessary. If that somehow does not work, go to the UCSC browser and download it as a BED file. The track name for Refseq genes is ``RefSeq Genes'' and the table name is ``refGene''. {[}Difficulty: \textbf{Beginner/Intermediate}{]}
\item
  Following from the exercise above, get the promoters of Refseq transcripts (-1000bp and +1000 bp of the TSS) and calculate what percentage of them overlap with CpG islands. HINT: You have to get the promoter coordinates and use the \texttt{findOverlaps()} or \texttt{subsetByOverlaps()} from the \texttt{GenomicRanges} package. To get promoters, type \texttt{?promoters} on the R console and see how to use that function to get promoters or calculate their coordinates as shown in the chapter. {[}Difficulty: \textbf{Beginner/Intermediate}{]}
\item
  Plot the distribution of CpG island lengths for CpG islands that overlap with the
  promoters. {[}Difficulty: \textbf{Beginner/Intermediate}{]}
\item
  Get canonical peaks for SP1 (peaks that are in both replicates) on chr21. Peaks for each replicate are located in the \texttt{wgEncodeHaibTfbsGm12878Sp1Pcr1xPkRep1.broadPeak.gz} and \texttt{wgEncodeHaibTfbsGm12878Sp1Pcr1xPkRep2.broadPeak.gz} files. \textbf{HINT}: You need to use \texttt{findOverlaps()} or \texttt{subsetByOverlaps()} to get the subset of peaks that occur in both replicates (canonical peaks). You can try to read ``\ldots broadPeak.gz'' files using the \texttt{genomation::readBroadPeak()} function; broadPeak is just an extended BED format. In addition, you can try to use \texttt{the\ coverage()} and \texttt{slice()} functions to get more precise canonical peak locations. {[}Difficulty: \textbf{Intermediate/Advanced}{]}
\end{enumerate}

\hypertarget{dealing-with-mapped-high-throughput-sequencing-reads-1}{%
\subsection{Dealing with mapped high-throughput sequencing reads}\label{dealing-with-mapped-high-throughput-sequencing-reads-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Count the reads overlapping with canonical SP1 peaks using the BAM file for one of the replicates. The following file in the \texttt{compGenomRData} package contains the alignments for SP1 ChIP-seq reads: \texttt{wgEncodeHaibTfbsGm12878Sp1Pcr1xAlnRep1.chr21.bam}. \textbf{HINT}: Use functions from the \texttt{GenomicAlignments} package. {[}Difficulty: \textbf{Beginner/Intermediate}{]}
\end{enumerate}

\hypertarget{dealing-with-contiguous-scores-over-the-genome}{%
\subsection{Dealing with contiguous scores over the genome}\label{dealing-with-contiguous-scores-over-the-genome}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Extract the \texttt{Views} object for the promoters on chr20 from the \texttt{H1.ESC.H3K4me1.chr20.bw} file available at \texttt{CompGenomRData} package. Plot the first ``View'' as a line plot. \textbf{HINT}: See the code in the relevant section in the chapter and adapt the code from there. {[}Difficulty: \textbf{Beginner/Intermediate}{]}
\item
  Make a histogram of the maximum signal for the Views in the object you extracted above. You can use any of the view summary functions or use \texttt{lapply()} and write your own summary function. {[}Difficulty: \textbf{Beginner/Intermediate}{]}
\item
  Get the genomic positions of maximum signal in each view and make a \texttt{GRanges} object. \textbf{HINT}: See the \texttt{?viewRangeMaxs} help page. Try to make a \texttt{GRanges} object out of the returned object. {[}Difficulty: \textbf{Intermediate}{]}
\end{enumerate}

\hypertarget{visualizing-and-summarizing-genomic-intervals-1}{%
\subsection{Visualizing and summarizing genomic intervals}\label{visualizing-and-summarizing-genomic-intervals-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Extract -500,+500 bp regions around the TSSes on chr21; there are refseq files for the hg19 human genome assembly in the \texttt{compGenomRData} package. Use SP1 ChIP-seq data in the \texttt{compGenomRData} package, access the file path via the \texttt{system.file()} function, the file name is:
  \texttt{wgEncodeHaibTfbsGm12878Sp1Pcr1xAlnRep1.chr21.bam}. Create an average profile of read coverage around the TSSes. Following that, visualize the read coverage with a heatmap. \textbf{HINT}: All of these are possible using the \texttt{genomation} package functions. Check \texttt{help(ScoreMatrix)} to see how you can use bam files. As an example here is how you can get the file path to refseq annotation on chr21. {[}Difficulty: \textbf{Intermediate/Advanced}{]}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{transcriptFilechr21=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                      \StringTok{"refseq.hg19.chr21.bed"}\NormalTok{,}
                      \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Extract -500,+500 bp regions around the TSSes on chr20. Use H3K4me3 (\texttt{H1.ESC.H3K4me3.chr20.bw}) and H3K27ac (\texttt{H1.ESC.H3K27ac.chr20.bw}) ChIP-seq enrichment data in the \texttt{compGenomRData} package and create heatmaps and average signal profiles for regions around the TSSes.{[}Difficulty: \textbf{Intermediate/Advanced}{]}
\item
  Download P300 ChIP-seq peaks data from the UCSC browser. The peaks are locations where P300 binds. The P300 binding marks enhancer regions in the genome. (\textbf{HINT}: group: ``regulation'', track: ``Txn Factor ChIP'', table:``wgEncodeRegTfbsClusteredV3'', you need to filter the rows for ``EP300'' name.) Check enrichment of H3K4me3, H3K27ac and DNase-seq (\texttt{H1.ESC.dnase.chr20.bw}) experiments on chr20 on and arounf the P300 binding-sites, use data from \texttt{compGenomRData} package. Make multi-heatmaps and metaplots. What is different from the TSS profiles ? {[}Difficulty: \textbf{Advanced}{]}
\item
  Cluster the rows of multi-heatmaps for the task above. Are there obvious clusters? \textbf{HINT}: Check arguments of the \texttt{multiHeatMatrix()} function. {[}Difficulty: \textbf{Advanced}{]}
\item
  Visualize one of the -500,+500 bp regions around the TSS using \texttt{Gviz} functions. You should visualize both H3K4me3 and H3K27ac and the gene models. {[}Difficulty: \textbf{Advanced}{]}
\end{enumerate}

\hypertarget{processingReads}{%
\chapter{Quality Check, Processing and Alignment of High-throughput Sequencing Reads}\label{processingReads}}

Advances in sequencing technology are helping researchers sequence the genome deeper than ever. These sequencing experiments typically yield millions of reads. These reads have to be further processed, quality checked and aligned before we can quantify the genomic signal of interest and apply statistics and/or machine learning methods. For example, you may want to count how many reads overlap with your promoter set of interest or you may want to quantify RNA-seq reads that overlap with exons. Post-alignment operations are usually, but not always, similar to operations on genomic intervals. Dealing with mapped reads is described previously in Chapter \ref{genomicIntervals}. In addition, we have introduced high-throughput sequencing and its applications in general in Chapter \ref{intro}. In this chapter we will introduce the fundamentals of read processing and quality check, and we will show how to do those tasks in R. The read quality check and processing is a fundamental step in all high-throughput sequencing analyses. For example, RNA-seq, ChIP-seq and BS-seq analyses shown in Chapters \ref{rnaseqanalysis}, \ref{chipseq} and \ref{bsseq} require these quality check and processing steps prior to further analysis. For a long time, quality check and mapping tasks were outside the R domain. However, nowadays certain packages in R/Bioconductor can accomplish those tasks.

\hypertarget{fasta-and-fastq-formats}{%
\section{FASTA and FASTQ formats}\label{fasta-and-fastq-formats}}

High-throughput sequencing reads are usually output from sequencing facilities as text files in a format called ``FASTQ'' or ``fastq''. This format depends on an earlier format called FASTA. The FASTA format was developed as a text-based format to represent nucleotide or protein sequences (see Figure \ref{fig:fasta} for an example).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/fastaPic} 

}

\caption{An example fasta file showing the first part of the PAX6 gene.}\label{fig:fasta}
\end{figure}

The first line in a FASTA file usually starts with a ``\textgreater{}'' (greater-than) symbol. This first line is called the ``description line'', and can contain descriptive information about the sequence in the subsequent lines. The description can be the ID or name of the sequence such as gene names. However, very infrequently you may see lines starting with a ``;'' (semicolon). These lines will be taken as a comment, and can hold additional descriptive information about the sequence in subsequent lines.

An extension of the FASTA format is FASTQ format. This format is designed to handle base quality metrics output from sequencing machines. In this format, both the sequence and quality scores are represented as single ASCII characters. The format uses four lines for each sequence, and these four lines are stacked on top of each other in text files output by sequencing workflows. Each of the 4 lines will represent a read. Figure \ref{fig:fastq} shows those four lines with brief explanations for each line.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/fastqPic} 

}

\caption{FASTQ format and a brief explanation of each line in the format.}\label{fig:fastq}
\end{figure}

\textbf{Line 1} begins with the `@' character and is followed by a sequence identifier and an optional description. This line is utilized by the sequencing technology, and usually contains specific information for the technology. It can contain flow cell IDs, lane numbers, and information on read pairs. \textbf{Line 2} is the sequence letters. \textbf{Line 3} begins with a `+' character; it marks the end of the sequence and is optionally followed by the same sequence identifier again in line 1. \textbf{Line 4} encodes the quality values for the sequence in Line 2, and must contain the same number of symbols as letters in the sequence. Each letter corresponds to a quality score. Although there might be different definitions of the quality scores, a \emph{de facto} standard in the field is to use ``Phred quality scores''. These scores represent the likelihood of the base being called wrong. Formally, \({\displaystyle Q_{\text{phred}}=-10\log _{\text{10}}e}\), where \(e\) is the probability that the base is called wrong. Since the score is in minus log scale, the higher the score, the more unlikely that the base is called wrong.

\hypertarget{quality-check-on-sequencing-reads}{%
\section{Quality check on sequencing reads}\label{quality-check-on-sequencing-reads}}

The sequencing technologies usually produce basecalls with varying quality. In addition, there could be sample-specific issues in your sequencing run, such as adapter contamination. It is standard procedure to check the quality of the reads and identify problems before doing further analysis. Checking the quality and making some decisions for the downstream analysis can influence the outcome of your project.

Below, we will walk you through the quality check steps using the \href{https://bioconductor.org/packages/release/bioc/html/Rqc.html}{\texttt{Rqc}} package\index{R Packages!\texttt{Rqc}}. First, we need to feed fastq files to the \texttt{rqc()} function and obtain an object with sequence quality-related results. We are using example fastq files from the \texttt{ShortRead} package\index{R Packages!\texttt{ShortRead}}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(Rqc)}
\NormalTok{folder =}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\DataTypeTok{package=}\StringTok{"ShortRead"}\NormalTok{, }\StringTok{"extdata/E-MTAB-1147"}\NormalTok{)}

\CommentTok{# feeds fastq.qz files in "folder" to quality check function}
\NormalTok{qcRes=}\KeywordTok{rqc}\NormalTok{(}\DataTypeTok{path =}\NormalTok{ folder, }\DataTypeTok{pattern =} \StringTok{".fastq.gz"}\NormalTok{, }\DataTypeTok{openBrowser=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sequence-quality-per-basecycle}{%
\subsection{Sequence quality per base/cycle}\label{sequence-quality-per-basecycle}}

Now that we have the \texttt{qcRes} object, we can plot various sequence quality metrics for our fastq files. We will first plot ``sequence quality per base/cycle''. This plot, shown in Figure \ref{fig:CycleQualityBoxPlot}, depicts the quality scores across all bases at each position in the reads.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rqcCycleQualityBoxPlot}\NormalTok{(qcRes)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{07-Read_Processing_files/figure-latex/CycleQualityBoxPlot-1} 

}

\caption{Per base sequence quality boxplot.}\label{fig:CycleQualityBoxPlot}
\end{figure}

In our case, the x-axis in the plot is labeled as ``cycle''. This is because in each sequencing ``cycle'' a fluorescently labeled nucleotide is added to complement the template sequence, and the sequencing machine identifies which nucleotide is added. Therefore, cycles correspond to bases/nucleotides along the read, and the number of cycles is equivalent to the read length.

Long sequences can have degraded quality towards the ends of the reads. Looking at quality distribution over base positions can help us decide to do trimming towards the end of the reads or not. A good sample will have median quality scores per base above 28. If scores are below 20 towards the ends, you can think about trimming the reads.

\hypertarget{sequence-content-per-basecycle}{%
\subsection{Sequence content per base/cycle}\label{sequence-content-per-basecycle}}

Per-base sequence content shows nucleotide proportions for each position. In a random sequencing library there should be no nucleotide bias and the lines should be almost parallel with each other. The code below shows how to get this plot. The resulting plot is shown in Figure \ref{fig:baseCallFreq}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rqcCycleBaseCallsLinePlot}\NormalTok{(qcRes)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{07-Read_Processing_files/figure-latex/baseCallFreq-1} 

}

\caption{Percentage of nucleotide bases per position across different FASTQ files.}\label{fig:baseCallFreq}
\end{figure}

However, some types of sequencing libraries can produce a biased sequence composition. For example, in RNA-Seq, it is common to have bias at the beginning of the reads. This happens because of random primers annealing to the start of reads during RNA-Seq library preparation. These primers are not truly random, which leads to a variation at the beginning of the reads. Although RNA-seq experiments will usually have these biases, this will not affect the ability of measuring gene expression.

In addition, some libraries are inherently biased in their sequence composition. For example, in bisulfite sequencing experiments, most of the cytosines will be converted to thymines. This will create a difference in C and T base compositions over the read, however this type of difference is normal for bisulfite sequencing experiments.

\hypertarget{read-frequency-plot}{%
\subsection{Read frequency plot}\label{read-frequency-plot}}

This plot shows the degree of duplication for every read in the library. We show how to get this plot in the code snippet below and the resulting plot is in Figure \ref{fig:ReadFrequencyPlot}. A high level of duplication, non-unique reads, is likely to indicate an enrichment bias. Technical duplicates arising from PCR artifacts could cause this. PCR is a common step in library preparation which creates many copies of the sequence fragment. In RNA-seq \index{RNA-seq}data, the non-unique read proportion can reach more than 20\%. However, these duplications may stem from genes simply being expressed at high levels. This means that there will be many copies of transcripts and many copies of the same fragment. Since we cannot be sure these duplicated reads are due to PCR bias or an effect of high transcription, we should not remove duplicated reads in RNA-seq analysis. However, in ChIP-seq experiments duplicated reads are more likely to be due to PCR bias.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rqcReadFrequencyPlot}\NormalTok{(qcRes)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{07-Read_Processing_files/figure-latex/ReadFrequencyPlot-1} 

}

\caption{The percent of different duplication levels in FASTQ files. Most of the reads in all libraries have only one copy in this case. }\label{fig:ReadFrequencyPlot}
\end{figure}

\hypertarget{other-quality-metrics-and-qc-tools}{%
\subsection{Other quality metrics and QC tools}\label{other-quality-metrics-and-qc-tools}}

Over-represented k-mers along the reads can be an additional check. If there are such sequences it may point to adapter contamination and should be trimmed. Adapters are known sequences that are added to the ends of the reads. This kind of contamination could also be visible at ``sequence content per base'' plots. In addition, if you know the adapter sequences, you can match it to the end of the reads and trim them. The most popular tool for sequencing quality control is the fastQC tool \citep{noauthor_babraham_nodate}, which is written in Java. It produces the plots that we described above in addition to k-mer overrepresentation and adapter overrepresentation plots. The R package \href{https://cran.r-project.org/web/packages/fastqcr/index.html}{fastqcr} can run this Java tool\index{R Packages!\texttt{fastqcr}} and produce R-based plots and reports. This package simply calls the Java tool and parses its results. Below, we show how to do that.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(fastqcr)}

\CommentTok{# install the FASTQC java tool}
\KeywordTok{fastqc_install}\NormalTok{()}

\CommentTok{# call FASTQC and record the resulting statistics}
\CommentTok{# in fastqc_results folder}
\KeywordTok{fastqc}\NormalTok{(}\DataTypeTok{fq.dir =}\NormalTok{ folder,}\DataTypeTok{qc.dir =} \StringTok{"fastqc_results"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now that we have run FastQC on our fastq files, we can read the results to R and construct plots or reports. The \texttt{gc\_report()} function can create an Rmarkdown-based report from FastQC output.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# view the report rendered by R functions}
\KeywordTok{qc_report}\NormalTok{(}\DataTypeTok{qc.path=}\StringTok{"fastqc_results"}\NormalTok{, }
          \DataTypeTok{result.file=}\StringTok{"reportFile"}\NormalTok{, }\DataTypeTok{preview =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Alternatively, we can read the results with \texttt{qc\_read()} and make specific plots we are interested in with \texttt{qc\_plot()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read QC results to R for one fastq file}
\NormalTok{qc <-}\StringTok{ }\KeywordTok{qc_read}\NormalTok{(}\StringTok{"fastqc_results/ERR127302_1_subset_fastqc.zip"}\NormalTok{)}

\CommentTok{# make plots, example "Per base sequence quality plot"}
\KeywordTok{qc_plot}\NormalTok{(qc, }\StringTok{"Per base sequence quality"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Apart from this, the bioconductor packages Rqc \citep{Rqc} (see \texttt{Rqc::rqcReport} function), QuasR \citep{gaidatzis_quasr:_2015} (see \texttt{QuasR::qQCReport} function), systemPipeR \citep{backman_systempiper:_2016} (see \texttt{systemPipeR::seeFastq} function), and ShortRead \citep{morgan_shortread:_2009} (see \texttt{ShortRead::report} function) can all generate quality reports in a similar fashion to FastQC with some differences in plot content and number.

\hypertarget{filtering-and-trimming-reads}{%
\section{Filtering and trimming reads}\label{filtering-and-trimming-reads}}

Based on the results of the quality check, you may want to trim or filter the reads. The quality check might have shown the number of reads that have low quality scores. These reads will probably not align very well because of the potential mistakes in base calling, or they may align to wrong places in the genome. Therefore, you may want to remove these reads from your fastq file. Another potential scenario is that parts of your reads needs to be trimmed in order to align the reads. In some cases, adapters will be present in either side of the read; in other cases technical errors will lead to decreasing base quality towards the ends of the reads. Both in these cases, the portion of the read should be trimmed so the read can align or better align the genome. We will show how to use the \texttt{QuasR} package to trim the reads. Other packages such as \texttt{ShortRead} also have capabilities to trim and filter reads. However, the \texttt{QuasR::preprocessReads()} function provides a single interface to multiple preprocessing possibilities. With this function, we match adapter sequences and remove them. We can remove low-complexity reads (reads containing repetitive sequences). We can trim the start or ends of the reads by a pre-defined length.

Below we will first set up the file paths to fastq files and filter them based on their length and whether or not they contain the ``N'' character, which stands for unidentified base. With the same function we will also trim 3 bases from the end of the reads and also trim segments from the start of the reads if they match the ``ACCCGGGA'' sequence.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(QuasR)}

\CommentTok{# obtain a list of fastq file paths}
\NormalTok{fastqFiles <-}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\DataTypeTok{package=}\StringTok{"ShortRead"}\NormalTok{,}
                          \StringTok{"extdata/E-MTAB-1147"}\NormalTok{,}
                          \KeywordTok{c}\NormalTok{(}\StringTok{"ERR127302_1_subset.fastq.gz"}\NormalTok{,}
                            \StringTok{"ERR127302_2_subset.fastq.gz"}\NormalTok{)}
\NormalTok{)}

\CommentTok{# defined processed fastq file names}
\NormalTok{outfiles <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\KeywordTok{tempfile}\NormalTok{(}\DataTypeTok{pattern=}\KeywordTok{c}\NormalTok{(}\StringTok{"processed_1_"}\NormalTok{,}
                              \StringTok{"processed_2_"}\NormalTok{)),}\StringTok{".fastq"}\NormalTok{,}\DataTypeTok{sep=}\StringTok{""}\NormalTok{)}

\CommentTok{# process fastq files}
\CommentTok{# remove reads that have more than 1 N, (nBases)}
\CommentTok{# trim 3 bases from the end of the reads (truncateEndBases)}
\CommentTok{# Remove ACCCGGGA patern if it occurs at the start (Lpattern)}
\CommentTok{# remove reads shorter than 40 base-pairs (minLength)}
\KeywordTok{preprocessReads}\NormalTok{(fastqFiles, outfiles, }
                \DataTypeTok{nBases=}\DecValTok{1}\NormalTok{,}
                \DataTypeTok{truncateEndBases=}\DecValTok{3}\NormalTok{,}
                \DataTypeTok{Lpattern=}\StringTok{"ACCCGGGA"}\NormalTok{,}
                \DataTypeTok{minLength=}\DecValTok{40}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As we have mentioned, the \texttt{ShortRead} package has low-level functions, which \texttt{QuasR::preprocessReads()} also depends on. We can use these low-level functions to filter reads in ways that are not possible using the \texttt{QuasR::preprocessReads()} function. Below we are going to read in a fastq file and filter the reads where every quality score is below 20.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ShortRead)}

\CommentTok{# obtain a list of fastq file paths}
\NormalTok{fastqFile <-}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\DataTypeTok{package=}\StringTok{"ShortRead"}\NormalTok{,}
                          \StringTok{"extdata/E-MTAB-1147"}\NormalTok{,}
                          \StringTok{"ERR127302_1_subset.fastq.gz"}\NormalTok{)}

\CommentTok{# read fastq file}
\NormalTok{fq =}\StringTok{ }\KeywordTok{readFastq}\NormalTok{(fastqFile)}

\CommentTok{# get quality scores per base as a matrix}
\NormalTok{qPerBase =}\StringTok{ }\KeywordTok{as}\NormalTok{(}\KeywordTok{quality}\NormalTok{(fq), }\StringTok{"matrix"}\NormalTok{)}

\CommentTok{# get number of bases per read that have quality score below 20}
\CommentTok{# we use this}
\NormalTok{qcount =}\StringTok{ }\KeywordTok{rowSums}\NormalTok{( qPerBase }\OperatorTok{<=}\StringTok{ }\DecValTok{20}\NormalTok{) }

\CommentTok{# Number of reads where all Phred scores >= 20}
\NormalTok{fq[qcount }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## class: ShortReadQ
## length: 10699 reads; width: 72 cycles
\end{verbatim}

We can finally write out the filtered fastq file with the \texttt{ShortRead::writeFastq()} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# write out fastq file with only reads where all }
\CommentTok{# quality scores per base are above 20}
\KeywordTok{writeFastq}\NormalTok{(fq[qcount }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{], }
           \KeywordTok{paste}\NormalTok{(fastqFile, }\StringTok{"Qfiltered"}\NormalTok{, }\DataTypeTok{sep=}\StringTok{"_"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

As fastq files can be quite large, it may not be feasible to read a 30-Gigabyte file into memory. A more memory-efficient way would be to read the file piece by piece. We can do our filtering operations for each piece, write the filtered part out, and read a new piece. Fortunately, this is possible using the \texttt{ShortRead::FastqStreamer()} function. This function enables ``streaming'' the fastq file in pieces, which are blocks of the fastq file with a pre-defined number of reads. We can access the successive blocks with the \texttt{yield()} function. Each time we call the \texttt{yield()} function after opening the fastq file with \texttt{FastqStreamer()}, a new part of the file will be read to the memory.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set up streaming with block size 1000}
\CommentTok{# every time we call the yield() function 1000 read portion}
\CommentTok{# of the file will be read successively. }
\NormalTok{f <-}\StringTok{ }\KeywordTok{FastqStreamer}\NormalTok{(fastqFile,}\DataTypeTok{readerBlockSize=}\DecValTok{1000}\NormalTok{) }

\CommentTok{# we set up a while loop to call yield() function to}
\CommentTok{# go through the file}
\ControlFlowTok{while}\NormalTok{(}\KeywordTok{length}\NormalTok{(fq <-}\StringTok{ }\KeywordTok{yield}\NormalTok{(f))) \{}
  
    \CommentTok{# remove reads where all quality scores are < 20 }
    \CommentTok{# get quality scores per base as a matrix}
\NormalTok{    qPerBase =}\StringTok{ }\KeywordTok{as}\NormalTok{(}\KeywordTok{quality}\NormalTok{(fq), }\StringTok{"matrix"}\NormalTok{)}

    \CommentTok{# get number of bases per read that have Q score < 20}
\NormalTok{    qcount =}\StringTok{ }\KeywordTok{rowSums}\NormalTok{( qPerBase }\OperatorTok{<=}\StringTok{ }\DecValTok{20}\NormalTok{) }
 
    \CommentTok{# write fastq file with mode="a", so every new block}
    \CommentTok{# is written out to the same file}
    \KeywordTok{writeFastq}\NormalTok{(fq[qcount }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{], }
               \KeywordTok{paste}\NormalTok{(fastqFile, }\StringTok{"Qfiltered"}\NormalTok{, }\DataTypeTok{sep=}\StringTok{"_"}\NormalTok{), }
               \DataTypeTok{mode=}\StringTok{"a"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{mappingaligning-reads-to-the-genome}{%
\section{Mapping/aligning reads to the genome}\label{mappingaligning-reads-to-the-genome}}

After the quality check and potential pre-processing, the reads are ready to be mapped or aligned to the reference genome. This process simply finds the most probable origin of each read in the genome. Since there might be errors in sequencing and mutations in the genomes, we may not find exact matches of reads in the genomes. An important feature of the alignment algorithms is to tolerate potential mismatches between reads and the reference genome. In addition, efficient algorithms and data structures are needed for the alignment to be completed in a reasonable amount of time. Alignment methods usually create data structures to store and efficiently search the genome for matching reads. These data structures are called genome indices and creating these indices is the first step for the read alignment. Based on how indices are created, there are two major types of methods. One class of methods relies on ``hash tables'', to store and search the genomes. Hash tables are simple lookup tables in which all possible k-mers point to locations in the genome. The general idea is that overlapping k-mers constructed from a read go through this lookup table. Each k-mer points to potential locations in the genome. Then, the final location for the read is obtained by optimizing the k-mer chain by their distances in the genome and in the read. This optimization process removes k-mer locations that are distant from other k-mers that map nearby each other.

Another class of algorithms builds genome indices by creating a Burrows-Wheeler transformation of the genome. This in essence creates a compact and searchable data structure for all reads. Although details are out of the scope of this section, these alignment tools provide faster alignment and use less memory. BWA\citep{li2009fast}, Bowtie1/2\citep{langmead2012fast} and SOAP\citep{li2009soap2} are examples of such algorithms.

The read mapping in R can be done with the \texttt{gmapR} \citep{gmapR}, \texttt{QuasR} \citep{gaidatzis_quasr:_2015}, \texttt{Rsubread} \citep{liao_subread_2013}, and \texttt{systemPipeR} \citep{backman_systempiper:_2016} packages. We will demonstrate read mapping with QuasR which uses the \texttt{Rbowtie} package, which wraps the Bowtie aligner. Below, we show how to map reads from a ChIP-seq experiment using QuasR/bowtie.

We will use the \texttt{qAlign()} function which requires two mandatory arguments: 1) a genome file in either fasta format or as a \texttt{BSgenome} package and 2) a sample file which is a text file and contains file paths to fastq files and sample names. In the case below, sample file looks like this:

\begin{verbatim}
FileName    SampleName
chip_1_1.fq.bz2 Sample1
chip_2_1.fq.bz2 Sample2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(QuasR)}

\CommentTok{# copy example data to current working directory}
\KeywordTok{file.copy}\NormalTok{(}\KeywordTok{system.file}\NormalTok{(}\DataTypeTok{package=}\StringTok{"QuasR"}\NormalTok{, }\StringTok{"extdata"}\NormalTok{), }\StringTok{"."}\NormalTok{, }\DataTypeTok{recursive=}\OtherTok{TRUE}\NormalTok{)}

\CommentTok{# genome file in fasta format}
\NormalTok{genomeFile <-}\StringTok{ "extdata/hg19sub.fa"}

\CommentTok{# text file containing sample names and fastq file paths}
\NormalTok{sampleFile <-}\StringTok{ "extdata/samples_chip_single.txt"}

\CommentTok{# create alignments }
\NormalTok{proj <-}\StringTok{ }\KeywordTok{qAlign}\NormalTok{(sampleFile, genomeFile)}
\end{Highlighting}
\end{Shaded}

It is good to explain what is going on here as the \texttt{qAlign()} function makes things look simple. This function is designed to be easy. For example, it creates a genome index automatically if it does not exist, and will look for existing indices before it creates one. We provided only two arguments, a text file containing sample names and fastq file paths and a reference genome file. In fact, this function also has many knobs and you can change its behavior by supplying different arguments in order to affect the behavior of Bowtie. For example, you can supply parameters to Bowtie using the \texttt{alignmentParameter} argument. However the \texttt{qAlign()} function is optimized for different types of alignment problems and selects alignment parameters automatically. It is designed to work with alignment and quantification tasks for RNA-seq\index{RNA-seq}, ChIP-seq\index{ChIP-seq}, small-RNA sequencing, Bisulfite sequencing (DNA methylation)\index{Bisulfite sequencing} and allele-specific analysis. If you want to change the default bowtie parameters, only do it for simple alignment problems such as ChIP-seq and RNA-seq.

\begin{rmdtip}
\begin{rmdtip}

\textbf{Want to know more ?}

\begin{itemize}
\tightlist
\item
  More on hash tables and Burrows-Wheeler-based aligners

  \begin{itemize}
  \tightlist
  \item
    A survey of sequence alignment algorithms for next-generation sequencin: (\url{https://academic.oup.com/bib/article/11/5/473/264166}) H Li, N Homer - Briefings in bioinformatics, 2010
  \end{itemize}
\item
  More on QuasR and all the alignment and post-processing capabilities: (\url{https://bioconductor.org/packages/release/bioc/vignettes/QuasR/inst/doc/QuasR.html})
\end{itemize}

\end{rmdtip}
\end{rmdtip}

\hypertarget{further-processing-of-aligned-reads}{%
\section{Further processing of aligned reads}\label{further-processing-of-aligned-reads}}

After alignment, some further processing might be necessary. However, these steps are usually sequencing protocol specific. For example, for methylation obtained via bisulfite sequencing, C-\textgreater T mismatches should be counted\index{Bisulfite sequencing}. For gene expression measurements, reads that overlap with transcripts should be counted. These further processing tasks are either done by a specialized alignment-related software or can be done in R in some cases. We will explain these further processing steps when they become relevant in the context of the following chapters.

\hypertarget{exercises-5}{%
\section{Exercises}\label{exercises-5}}

For this set of exercises, we will use the \texttt{chip\_1\_1.fq.bz2} and \texttt{chip\_2\_1.fq.bz2} files from the \texttt{QuasR} package. You can reach the folder that contains the files as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{folder=(}\KeywordTok{system.file}\NormalTok{(}\DataTypeTok{package=}\StringTok{"QuasR"}\NormalTok{, }\StringTok{"extdata"}\NormalTok{))}
\KeywordTok{dir}\NormalTok{(folder) }\CommentTok{# will show the contents of the folder}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Plot the base quality distributions of the ChIP-seq samples \texttt{Rqc} package.
  \textbf{HINT}: You need to provide a regular expression pattern for extracting the right files from the folder. \texttt{"\^{}chip"} matches the files beginning with ``chip''. {[}Difficulty: \textbf{Beginner/Intermediate}{]}
\item
  Now we will trim the reads based on the quality scores. Let's trim 2-4 bases on the 3' end depending on the quality scores. You can use the \texttt{QuasR::preprocessReads()} function for this purpose.{[}Difficulty: \textbf{Beginner/Intermediate}{]}
\item
  Align the trimmed and untrimmed reads using \texttt{QuasR} and plot alignment statistics, did the trimming improve alignments? {[}Difficulty: \textbf{Intermediate/Advanced}{]}
\end{enumerate}

\hypertarget{rnaseqanalysis}{%
\chapter{RNA-seq Analysis}\label{rnaseqanalysis}}

\emph{Chapter Author}: \textbf{Bora Uyar}

RNA sequencing (RNA-seq) \index{RNA-seq}has proven to be a revolutionary tool since the time it was introduced. The throughput, accuracy, and resolution of data produced with RNA-seq has been instrumental in the study of transcriptomics in the last decade \citep{wang_rna-seq:_2009}. There is a variety of applications of transcriptome sequencing and each application may consist of different chains of tools each with many alternatives \citep{conesa_survey_2016}. In this chapter, we are going to demonstrate a common workflow for doing differential expression analysis with downstream applications such as GO term and gene set enrichment analysis. We assume that the sequencing data was generated using one of the NGS sequencing platforms. Where applicable, we will try to provide alternatives to the reader in terms of both the tools to carry out a demonstrated analysis and also the other applications of the same sequencing data depending on the different biological questions.

\hypertarget{what-is-gene-expression}{%
\section{What is gene expression?}\label{what-is-gene-expression}}

Gene expression is a term used to describe the contribution of a gene to the overall functions and phenotype of a cell through the activity of the molecular products, which are encoded in the specific nucleotide sequence of the gene. RNA is the primary product encoded in a gene, which is transcribed in the nucleus of a cell. A class of RNA molecules, messenger RNAs, are transported from the nucleus to the cytoplasm, where the translation machinery of the cell translates the nucleotide sequence of the mRNA into proteins. The functional protein repertoire in a given cell is the primary factor that dictates the shape, function, and phenotype of a cell. Due to the prime roles of proteins for a cell's fate, most molecular biology literature is focused on protein-coding genes. However, a bigger proportion of a eukaryotic gene repertoire is reserved for non-coding genes, which code for RNA molecules that are not translated into proteins, yet carry out many important cellular functions. All in all, the term gene expression \index{gene expression}refers to the combined activity of protein-coding or non-coding products of a gene.

In a cell, there are many layers of quality controls and modifications that act upon a gene's product until the end-product attains a particular function. These layers of regulation include epigenetic, transcriptional, post-transcriptional, translational, and post-translational control mechanisms, the latter two applying only to protein-coding genes. A protein or RNA molecule, is only functional if it is produced at the right time, at the right cellular compartment, with the necessary base or amino-acid modifications, with the correct secondary/tertiary structure (or unstructure wherever applicable), among the availability of other metabolites or molecules, which are needed to form complexes to altogether accomplish a certain cellular function. However, traditionally, the number of copies of a gene's products is considered a quantitative measure of a gene's activity. Although this approach does not reflect all of the complexity that defines a functional molecule, quantification of the abundance of transcripts from a gene has proven to be a cost-effective method of understanding genes' functions.

\hypertarget{methods-to-detect-gene-expression}{%
\section{Methods to detect gene expression}\label{methods-to-detect-gene-expression}}

Quantification of how much gene expression levels deviate from a baseline gives clues about which genes are actually important for, for instance, disease outcome or cell/tissue identity. The methods of detecting and quantifying gene expression have evolved from low-throughput methods such as the usage of a reporter gene with a fluorescent protein product to find out if a single gene is expressed at all, to high-throughput methods such as massively parallel RNA-sequencing that can profile -at single-nucleotide resolution- the abundance of tens of thousands of distinct transcripts encoded in the largest eukaryotic genomes.

\hypertarget{gene-expression-analysis-using-high-throughput-sequencing-technologies}{%
\section{Gene expression analysis using high-throughput sequencing technologies}\label{gene-expression-analysis-using-high-throughput-sequencing-technologies}}

With the advent of the second-generation (a.k.a next-generation or high-throughput) sequencing technologies,
the number of genes that can be profiled for expression levels with a single experiment has increased to the order of tens of thousands of genes. Therefore, the bottleneck in this process has become the data analysis rather than the data generation. Many statistical methods and computational tools are required for getting meaningful results from the data, which comes with a lot of valuable information along with a lot of sources of noise. Fortunately, most of the steps of RNA-seq analysis have become quite mature over the years. Below we will first describe how to reach a read count table from raw fastq reads obtained from an Illumina sequencing run. We will then demonstrate in R how to process the count table, make a case-control differential expression analysis, and do some downstream functional enrichment analysis.

\hypertarget{processing-raw-data}{%
\subsection{Processing raw data}\label{processing-raw-data}}

\hypertarget{quality-check-and-read-processing}{%
\subsubsection{Quality check and read processing}\label{quality-check-and-read-processing}}

The first step in any experiment that involves high-throughput short-read sequencing should be to check the sequencing quality of the reads before starting to do any downstream analysis. The quality of the input sequences holds fundamental importance in the confidence for the biological conclusions drawn from the experiment. We have introduced quality check and processing in Chapter \ref{processingReads}, and those tools and workflows also apply in RNA-seq analysis.

\hypertarget{improving-the-quality}{%
\subsubsection{Improving the quality}\label{improving-the-quality}}

The second step in the RNA-seq analysis workflow is to improve the quality of the input reads. This step could be regarded as an optional step when the sequencing quality is very good. However, even with the highest-quality sequencing datasets, this step may still improve the quality of the input sequences. The most common technical artifacts that can be filtered out are the adapter sequences that contaminate the sequenced reads, and the low-quality bases that are usually found at the ends of the sequences. Commonly used tools in the field (trimmomatic \citep{bolger_trimmomatic:_2014}, trimGalore \citep{noauthor_babraham_nodate}) are again not written in R, however there are alternative R libraries for carrying out the same functionality, for instance, QuasR \citep{gaidatzis_quasr:_2015} (see \texttt{QuasR::preprocessReads} function) and ShortRead \citep{morgan_shortread:_2009} (see \texttt{ShortRead::filterFastq} function). Some of these approaches are introduced in Chapter \ref{processingReads}.

The sequencing quality control and read pre-processing steps can be visited multiple times until achieving a satisfactory level of quality in the sequence data before moving on to the downstream analysis steps.

\hypertarget{alignment}{%
\subsection{Alignment}\label{alignment}}

Once a decent level of quality in the sequences is reached, the expression level of the genes can be quantified by first mapping the sequences to a reference genome, and secondly matching the aligned reads to the gene annotations, in order to count the number of reads mapping to each gene. If the species under study has a well-annotated transcriptome, the reads can be aligned to the transcript sequences instead of the reference genome. In cases where there is no good quality reference genome or transcriptome, it is possible to de novo assemble the transcriptome from the sequences and then quantify the expression levels of genes/transcripts.

For RNA-seq read alignments, apart from the availability of reference genomes and annotations, probably the most important factor to consider when choosing an alignment tool is whether the alignment method considers the absence of intronic regions in the sequenced reads, while the target genome may contain introns. Therefore, it is important to choose alignment tools that take into account alternative splicing. In the basic setting where a read, which originates from a cDNA sequence corresponding to an exon-exon junction, needs to be split into two parts when aligned against the genome. There are various tools that consider this factor such as STAR \citep{dobin_star:_2013}, Tophat2 \citep{kim_tophat2:_2013}, Hisat2 \citep{kim_hisat:_2015}, and GSNAP \citep{wu_gmap_2016}. Most alignment tools are written in C/C++ languages because of performance concerns. There are also R libraries that can do short read alignments; these are discussed in Chapter \ref{processingReads}.

\hypertarget{quantification}{%
\subsection{Quantification}\label{quantification}}

After the reads are aligned to the target, a SAM/BAM file sorted by coordinates should have been obtained. The BAM file \index{BAM file}contains all alignment-related information of all the reads that have been attempted to be aligned to the target sequence. This information consists of - most basically - the genomic coordinates (chromosome, start, end, strand) of where a sequence was matched (if at all) in the target, specific insertions/deletions/mismatches that describe the differences between the input and target sequences. These pieces of information are used along with the genomic coordinates of genome annotations such as gene/transcript models in order to count how many reads have been sequenced from a gene/transcript. As simple as it may sound, it is not a trivial task to assign reads to a gene/transcript just by comparing the genomic coordinates of the annotations and the sequences, because of confounding factors such as overlapping gene annotations, overlapping exon annotations from different transcript isoforms of a gene, and overlapping annotations from opposite DNA strands in the absence of a strand-specific sequencing protocol. Therefore, for read counting, it is important to consider:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Strand specificity of the sequencing protocol: Are the reads expected to originate from the forward strand, reverse strand, or unspecific?
\item
  Counting mode:
\end{enumerate}

\begin{verbatim}
- when counting at the gene-level: When there are overlapping annotations, which features should the read be assigned to? Tools usually have a parameter that lets the user select a counting mode.
- when counting at the transcript-level: When there are multiple isoforms of a gene, which isoform should the read be assigned to? This consideration is usually an algorithmic consideration that is not modifiable by the end-user. 
\end{verbatim}

Some tools can couple alignment to quantification (e.g.~STAR), while some assume the alignments are already calculated and require BAM files as input. On the other hand, in the presence of good transcriptome annotations, alignment-free methods (Salmon \citep{patro_salmon:_2017}, Kallisto \citep{bray_near-optimal_2016}, Sailfish \citep{patro_sailfish_2014}) can also be used to estimate the expression levels of transcripts/genes. There are also reference-free quantification methods that can first de novo assemble the transcriptome and estimate the expression levels based on this assembly. Such a strategy can be useful in discovering novel transcripts or may be required in cases when a good reference does not exist. If a reference transcriptome exists but of low quality, a reference-based transcriptome assembler such as Cufflinks \citep{trapnell_transcript_2010} can be used to improve the transcriptome. In case there is no available transcriptome annotation, a de novo assembler such as Trinity \citep{haas_novo_2013} or Trans-ABySS \citep{robertson_novo_2010} can be used to assemble the transcriptome from scratch.

Within R, quantification can be done using:
- \texttt{Rsubread::featureCounts}
- \texttt{QuasR::qCount}
- \texttt{GenomicAlignments::summarizeOverlaps}

\hypertarget{within-sample-normalization-of-the-read-counts}{%
\subsection{Within sample normalization of the read counts}\label{within-sample-normalization-of-the-read-counts}}

The most common application after a gene's expression is quantified (as the number of reads aligned to the gene), is to compare the gene's expression in different conditions, for instance, in a case-control setting (e.g.~disease versus normal) or in a time-series (e.g.~along different developmental stages). Making such comparisons helps identify the genes that might be responsible for a disease or an impaired developmental trajectory. However, there are multiple caveats that needs to be addressed before making a comparison between the read counts of a gene in different conditions \citep{maza_comparison_2013}.

\begin{itemize}
\tightlist
\item
  Library size (i.e.~sequencing depth) varies between samples coming from different lanes of the flow cell of the sequencing machine.
\item
  Longer genes will have a higher number of reads.\\
\item
  Library composition (i.e.~relative size of the studied transcriptome) can be different in two different biological conditions.
\item
  GC content biases across different samples may lead to a biased sampling of genes \citep{risso_gc-content_2011}.
\item
  Read coverage of a transcript can be biased and non-uniformly distributed along the transcript \citep{mortazavi_mapping_2008}.
\end{itemize}

Therefore these factors need to be taken into account before making comparisons.

The most basic normalization\index{normalization} approaches address the sequencing depth bias. Such procedures normalize the read counts per gene by dividing each gene's read count by a certain value and multiplying it by 10\^{}6. These normalized values are usually referred to as CPM (counts per million reads):

\begin{itemize}
\tightlist
\item
  Total Counts Normalization (divide counts by the \textbf{sum} of all counts)
\item
  Upper Quartile Normalization (divide counts by the \textbf{upper quartile} value of the counts)
\item
  Median Normalization (divide counts by the \textbf{median} of all counts)
\end{itemize}

Popular metrics that improve upon CPM are RPKM/FPKM (reads/fragments per kilobase of million reads) and TPM (transcripts per million). RPKM is obtained by dividing the CPM value by another factor, which is the length of the gene per kilobase. FPKM is the same as RPKM, but is used for paired-end reads. Thus, RPKM/FPKM methods account for, firstly, the \textbf{library size}, and secondly, the \textbf{gene lengths}.

TPM also controls for both the library size and the gene lengths, however, with the TPM method, the read counts are first normalized by the gene length (per kilobase), and then gene-length normalized values are divided by the sum of the gene-length normalized values and multiplied by 10\^{}6. Thus, the sum of normalized values for TPM will always be equal to 10\^{}6 for each library, while the sum of RPKM/FPKM values do not sum to 10\^{}6. Therefore, it is easier to interpret TPM values than RPKM/FPKM values.

\hypertarget{computing-different-normalization-schemes-in-r}{%
\subsection{Computing different normalization schemes in R}\label{computing-different-normalization-schemes-in-r}}

Here we will assume that there is an RNA-seq count table comprising raw counts, meaning the number of reads counted for each gene has not been exposed to any kind of normalization and consists of integers. The rows of the count table correspond to the genes and the columns represent different samples. Here we will use a subset of the RNA-seq count table from a colorectal cancer study. We have filtered the original count table for only protein-coding genes (to improve the speed of calculation) and also selected only five metastasized colorectal cancer samples along with five normal colon samples. There is an additional column \texttt{width} that contains the length of the corresponding gene in the unit of base pairs. The length of the genes are important to compute RPKM and TPM values. The original count tables can be found from the recount2 database (\url{https://jhubiostatistics.shinyapps.io/recount/}) using the SRA project code \emph{SRP029880}, and the experimental setup along with other accessory information can be found from the NCBI Trace archive using the SRA project code \href{https://trace.ncbi.nlm.nih.gov/Traces/sra/?study=SRP029880}{SRP029880`}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#colorectal cancer}
\NormalTok{counts_file <-}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata/rna-seq/SRP029880.raw_counts.tsv"}\NormalTok{,}
                           \DataTypeTok{package =} \StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{coldata_file <-}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata/rna-seq/SRP029880.colData.tsv"}\NormalTok{,}
                            \DataTypeTok{package =} \StringTok{"compGenomRData"}\NormalTok{)}

\NormalTok{counts <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{read.table}\NormalTok{(counts_file, }\DataTypeTok{header =}\NormalTok{ T, }\DataTypeTok{sep =} \StringTok{'}\CharTok{\textbackslash{}t}\StringTok{'}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{computing-cpm}{%
\subsubsection{Computing CPM}\label{computing-cpm}}

Let's do a summary of the counts table. Due to space limitations, the summary for only the first three columns is displayed.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(counts[,}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      CASE_1              CASE_2              CASE_3         
##  Min.   :        0   Min.   :        0   Min.   :        0  
##  1st Qu.:     5155   1st Qu.:     6464   1st Qu.:     3972  
##  Median :    80023   Median :    85064   Median :    64145  
##  Mean   :   295932   Mean   :   273099   Mean   :   263045  
##  3rd Qu.:   252164   3rd Qu.:   245484   3rd Qu.:   210788  
##  Max.   :205067466   Max.   :105248041   Max.   :222511278
\end{verbatim}

To compute the CPM values for each sample (excluding the \texttt{width} column):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cpm <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(}\KeywordTok{subset}\NormalTok{(counts, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\NormalTok{width)), }\DecValTok{2}\NormalTok{, }
             \ControlFlowTok{function}\NormalTok{(x) x}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(x)) }\OperatorTok{*}\StringTok{ }\DecValTok{10}\OperatorTok{^}\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Check that the sum of each column after normalization equals to 10\^{}6 (except the width column).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{colSums}\NormalTok{(cpm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## CASE_1 CASE_2 CASE_3 CASE_4 CASE_5 CTRL_1 CTRL_2 CTRL_3 CTRL_4 CTRL_5 
##  1e+06  1e+06  1e+06  1e+06  1e+06  1e+06  1e+06  1e+06  1e+06  1e+06
\end{verbatim}

\hypertarget{computing-rpkm}{%
\subsubsection{Computing RPKM}\label{computing-rpkm}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a vector of gene lengths }
\NormalTok{geneLengths <-}\StringTok{ }\KeywordTok{as.vector}\NormalTok{(}\KeywordTok{subset}\NormalTok{(counts, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(width)))}

\CommentTok{# compute rpkm }
\NormalTok{rpkm <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(}\DataTypeTok{X =} \KeywordTok{subset}\NormalTok{(counts, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\NormalTok{width)),}
              \DataTypeTok{MARGIN =} \DecValTok{2}\NormalTok{, }
              \DataTypeTok{FUN =} \ControlFlowTok{function}\NormalTok{(x) \{}
                \DecValTok{10}\OperatorTok{^}\DecValTok{9} \OperatorTok{*}\StringTok{ }\NormalTok{x }\OperatorTok{/}\StringTok{ }\NormalTok{geneLengths }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(x))}
\NormalTok{               \})}
\end{Highlighting}
\end{Shaded}

Check the sample sizes of RPKM. Notice that the sums of samples are all different.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{colSums}\NormalTok{(rpkm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   CASE_1   CASE_2   CASE_3   CASE_4   CASE_5   CTRL_1   CTRL_2   CTRL_3 
## 158291.0 153324.2 161775.4 173047.4 172761.4 210032.6 301764.2 241418.3 
##   CTRL_4   CTRL_5 
## 291674.5 252005.7
\end{verbatim}

\hypertarget{computing-tpm}{%
\subsubsection{Computing TPM}\label{computing-tpm}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#find gene length normalized values }
\NormalTok{rpk <-}\StringTok{ }\KeywordTok{apply}\NormalTok{( }\KeywordTok{subset}\NormalTok{(counts, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\NormalTok{width)), }\DecValTok{2}\NormalTok{, }
              \ControlFlowTok{function}\NormalTok{(x) x}\OperatorTok{/}\NormalTok{(geneLengths}\OperatorTok{/}\DecValTok{1000}\NormalTok{))}
\CommentTok{#normalize by the sample size using rpk values}
\NormalTok{tpm <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(rpk, }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) x }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(x)) }\OperatorTok{*}\StringTok{ }\DecValTok{10}\OperatorTok{^}\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Check the sample sizes of \texttt{tpm}. Notice that the sums of samples are all equal to 10\^{}6.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{colSums}\NormalTok{(tpm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## CASE_1 CASE_2 CASE_3 CASE_4 CASE_5 CTRL_1 CTRL_2 CTRL_3 CTRL_4 CTRL_5 
##  1e+06  1e+06  1e+06  1e+06  1e+06  1e+06  1e+06  1e+06  1e+06  1e+06
\end{verbatim}

None of these metrics (CPM, RPKM/FPKM, TPM) account for the other important confounding factor when comparing expression levels of genes across samples: the \textbf{library composition}, which may also be referred to as the \textbf{relative size of the compared transcriptomes}. This factor is not dependent on the sequencing technology, it is rather biological. For instance, when comparing transcriptomes of different tissues, there can be sets of genes in one tissue that consume a big chunk of the reads, while in the other tissues they are not expressed at all. This kind of imbalance in the composition of compared transcriptomes can lead to wrong conclusions about which genes are actually differentially expressed. This consideration is addressed in two popular R packages: \texttt{DESeq2}\index{R Packages!\texttt{DESeq2}} \citep{love_moderated_2014} and edgeR \citep{robinson_edger:_2010} each with a different algorithm. \texttt{edgeR}\index{R Packages!\texttt{edgeR}} uses a normalization procedure called Trimmed Mean of M-values (TMM). \texttt{DESeq2} implements a normalization procedure using median of Ratios, which is obtained by finding the ratio of the log-transformed count of a gene divided by the average of log-transformed values of the gene in all samples (geometric mean), and then taking the median of these values for all genes. The raw read count of the gene is finally divided by this value (median of ratios) to obtain the normalized counts.

\hypertarget{exploratory-analysis-of-the-read-count-table}{%
\subsection{Exploratory analysis of the read count table}\label{exploratory-analysis-of-the-read-count-table}}

A typical quality control, in this case interrogating the RNA-seq experiment design, is to measure the similarity of the samples with each other in terms of the quantified expression level profiles across a set of genes. One important observation to make is to see whether the most similar samples to any given sample are the biological replicates of that sample. This can be computed using unsupervised clustering techniques such as hierarchical clustering and visualized as a heatmap with dendrograms. Another most commonly applied technique is a dimensionality reduction technique called Principal Component Analysis (PCA) and visualized as a two-dimensional (or in some cases three-dimensional) scatter plot. In order to find out more about the clustering methods and PCA, please refer to Chapter \ref{unsupervisedLearning}.

\hypertarget{clustering-1}{%
\subsubsection{Clustering}\label{clustering-1}}

We can combine clustering and visualization of the clustering results by using heatmap functions that are available in a variety of R libraries. The basic R installation comes with the \texttt{stats::heatmap} function. However, there are other libraries available in CRAN (e.g.~\texttt{pheatmap} \citep{pheatmap})\index{R Packages!\texttt{pheatmap}} or Bioconductor (e.g.~\texttt{ComplexHeatmap} \citep{gu_complex_2016}) \index{R Packages!\texttt{ComplexHeatmap}}that come with more flexibility and more appealing visualizations.

Here we demonstrate a heatmap using the \texttt{pheatmap} package and the previously calculated \texttt{tpm} matrix.
As these matrices can be quite large, both computing the clustering and rendering the heatmaps can take a lot of resources and time. Therefore, a quick and informative way to compare samples is to select a subset of genes that are, for instance, most variable across samples, and use that subset to do the clustering and visualization.

Let's select the top 100 most variable genes among the samples.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#compute the variance of each gene across samples}
\NormalTok{V <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(tpm, }\DecValTok{1}\NormalTok{, var)}
\CommentTok{#sort the results by variance in decreasing order }
\CommentTok{#and select the top 100 genes }
\NormalTok{selectedGenes <-}\StringTok{ }\KeywordTok{names}\NormalTok{(V[}\KeywordTok{order}\NormalTok{(V, }\DataTypeTok{decreasing =}\NormalTok{ T)][}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

Now we can quickly produce a heatmap where samples and genes are clustered (see Figure \ref{fig:tpmhierClust1} ).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(pheatmap)}
\KeywordTok{pheatmap}\NormalTok{(tpm[selectedGenes,], }\DataTypeTok{scale =} \StringTok{'row'}\NormalTok{, }\DataTypeTok{show_rownames =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{08-rna-seq-analysis_files/figure-latex/tpmhierClust1-1} 

}

\caption{Clustering and visualization of the topmost variable genes as a heatmap.}\label{fig:tpmhierClust1}
\end{figure}

We can also overlay some annotation tracks to observe the clusters.
Here it is important to observe whether the replicates of the same sample cluster most closely with each other, or not. Overlaying the heatmap with such annotation and displaying sample groups with distinct colors helps quickly see if there are samples that don't cluster as expected (see Figure \ref{fig:tpmhierclust2} ).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{colData <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(coldata_file, }\DataTypeTok{header =}\NormalTok{ T, }\DataTypeTok{sep =} \StringTok{'}\CharTok{\textbackslash{}t}\StringTok{'}\NormalTok{, }
                      \DataTypeTok{stringsAsFactors =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{pheatmap}\NormalTok{(tpm[selectedGenes,], }\DataTypeTok{scale =} \StringTok{'row'}\NormalTok{, }
         \DataTypeTok{show_rownames =} \OtherTok{FALSE}\NormalTok{, }
         \DataTypeTok{annotation_col =}\NormalTok{ colData)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{08-rna-seq-analysis_files/figure-latex/tpmhierclust2-1} 

}

\caption{Clustering samples as a heatmap with sample annotations.}\label{fig:tpmhierclust2}
\end{figure}

\hypertarget{pca}{%
\subsubsection{PCA}\label{pca}}

Let's make a PCA plot \index{principal component analysis (PCA)} to see the clustering of replicates as a scatter plot in two dimensions (Figure \ref{fig:pca1}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(stats)}
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{#transpose the matrix }
\NormalTok{M <-}\StringTok{ }\KeywordTok{t}\NormalTok{(tpm[selectedGenes,])}
\CommentTok{# transform the counts to log2 scale }
\NormalTok{M <-}\StringTok{ }\KeywordTok{log2}\NormalTok{(M }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)}
\CommentTok{#compute PCA }
\NormalTok{pcaResults <-}\StringTok{ }\KeywordTok{prcomp}\NormalTok{(M)}

\CommentTok{#plot PCA results making use of ggplot2's autoplot function}
\CommentTok{#ggfortify is needed to let ggplot2 know about PCA data structure. }
\KeywordTok{autoplot}\NormalTok{(pcaResults, }\DataTypeTok{data =}\NormalTok{ colData, }\DataTypeTok{colour =} \StringTok{'group'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/pca1-1} 

}

\caption{PCA plot of samples using TPM counts.}\label{fig:pca1}
\end{figure}

We should observe here whether the samples from the case group (CASE) and samples from the control group (CTRL) can be split into two distinct clusters on the scatter plot of the first two largest principal components.

We can use the \texttt{summary} function to summarize the PCA results to observe the contribution of the principal components in the explained variation.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(pcaResults)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Importance of components:
##                           PC1     PC2     PC3     PC4     PC5    PC6     PC7
## Standard deviation     24.396 2.50514 2.39327 1.93841 1.79193 1.6357 1.46059
## Proportion of Variance  0.957 0.01009 0.00921 0.00604 0.00516 0.0043 0.00343
## Cumulative Proportion   0.957 0.96706 0.97627 0.98231 0.98747 0.9918 0.99520
##                            PC8     PC9      PC10
## Standard deviation     1.30902 1.12657 4.616e-15
## Proportion of Variance 0.00276 0.00204 0.000e+00
## Cumulative Proportion  0.99796 1.00000 1.000e+00
\end{verbatim}

\hypertarget{correlation-plots}{%
\subsubsection{Correlation plots}\label{correlation-plots}}

Another complementary approach to see the reproducibility of the experiments is to compute the correlation scores between each pair of samples and draw a correlation plot. \index{correlation}

Let's first compute pairwise correlation scores between every pair of samples.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(stats)}
\NormalTok{correlationMatrix <-}\StringTok{ }\KeywordTok{cor}\NormalTok{(tpm)}
\end{Highlighting}
\end{Shaded}

Let's have a look at how the correlation matrix looks (\ref{tab:corrplot2})
(showing only two samples each of case and control samples):

\begin{table}

\caption{\label{tab:corrplot2}Correlation scores between samples}
\centering
\begin{tabular}[t]{lrrrr}
\toprule
  & CASE\_1 & CASE\_2 & CTRL\_1 & CTRL\_2\\
\midrule
CASE\_1 & 1.0000000 & 0.9924606 & 0.9594011 & 0.9635760\\
CASE\_2 & 0.9924606 & 1.0000000 & 0.9725646 & 0.9793835\\
CTRL\_1 & 0.9594011 & 0.9725646 & 1.0000000 & 0.9879862\\
CTRL\_2 & 0.9635760 & 0.9793835 & 0.9879862 & 1.0000000\\
\bottomrule
\end{tabular}
\end{table}

We can also draw more visually appealing correlation plots using the \texttt{corrplot} package (Figure \ref{fig:corrplot3}).
Using the \texttt{addrect} argument, we can split clusters into groups and surround them with rectangles.
By setting the \texttt{addCoef.col} argument to `white', we can display the correlation coefficients as numbers in white color.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(corrplot)}
\KeywordTok{corrplot}\NormalTok{(correlationMatrix, }\DataTypeTok{order =} \StringTok{'hclust'}\NormalTok{, }
         \DataTypeTok{addrect =} \DecValTok{2}\NormalTok{, }\DataTypeTok{addCoef.col =} \StringTok{'white'}\NormalTok{, }
         \DataTypeTok{number.cex =} \FloatTok{0.7}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/corrplot3-1} 

}

\caption{Correlation plot of samples ordered by hierarchical clustering}\label{fig:corrplot3}
\end{figure}

Here pairwise correlation levels are visualized as colored circles. \texttt{Blue} indicates positive correlation, while \texttt{Red} indicates negative correlation.

We could also plot this correlation matrix as a heatmap (Figure \ref{fig:corrplot4}). As all the samples have a high pairwise
correlation score, using a heatmap instead of a corrplot helps to see the differences between samples more easily. The
\texttt{annotation\_col} argument helps to display sample annotations and the \texttt{cutree\_cols} argument is set to 2 to split the clusters into two groups based on the hierarchical clustering results.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(pheatmap)}
\CommentTok{# split the clusters into two based on the clustering similarity }
\KeywordTok{pheatmap}\NormalTok{(correlationMatrix,  }
         \DataTypeTok{annotation_col =}\NormalTok{ colData, }
         \DataTypeTok{cutree_cols =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/corrplot4-1} 

}

\caption{Pairwise correlation of samples displayed as a heatmap.}\label{fig:corrplot4}
\end{figure}

\hypertarget{differential-expression-analysis}{%
\subsection{Differential expression analysis}\label{differential-expression-analysis}}

Differential expression analysis allows us to test tens of thousands of hypotheses (one test for each gene) against the null hypothesis that the activity of the gene stays the same in two different conditions. There are multiple limiting factors that influence the power of detecting genes that have real changes between two biological conditions. Among these are the limited number of biological replicates, non-normality of the distribution of the read counts, and higher uncertainty of measurements for lowly expressed genes than highly expressed genes \citep{love_moderated_2014}. Tools such as \texttt{edgeR} and \texttt{DESeq2} address these limitations using sophisticated statistical models in order to maximize the amount of knowledge that can be extracted from such noisy datasets. In essence, these models assume that for each gene, the read counts are generated by a negative binomial distribution\index{negative binomial distribution}. This is a popular distribution that is used for modeling count data. This distribution can be specified with a mean parameter, \(m\), and a dispersion parameter, \(\alpha\). The dispersion parameter \(\alpha\) is directly related to the variance as the variance of this distribution is formulated as: \(m+\alpha m^{2}\). Therefore, estimating these parameters are crucial for differential expression tests. The methods used in \texttt{edgeR} and \texttt{DESeq2} use dispersion estimates from other genes with similar counts to precisely estimate the per-gene dispersion values. With accurate dispersion parameter estimates, one can estimate the variance more precisely, which in turn
improves the result of the differential expression test. Although statistical models are different, the process here is similar to the moderated t-test \index{moderated t-test}and qualifies as an empirical Bayes method \index{empirical Bayes methods} which,we introduced in Chapter \ref{stats}. There, we calculated gene-wise variability and shrunk each gene-wise variability towards the median variability of all genes. In the case of RNA-seq the dispersion coefficient \(\alpha\) is shrunk towards the value of dispersion from other genes with similar read counts.

Now let us take a closer look at the \texttt{DESeq2} \index{R Packages!\texttt{DESeq2}}workflow and how it calculates differential expression:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The read counts are normalized by computing size factors, which addresses the differences not only in the library sizes, but also the library compositions.
\item
  For each gene, a dispersion estimate is calculated. The dispersion value computed by \texttt{DESeq2} is equal to the squared coefficient of variation (variation divided by the mean).\\
\item
  A line is fit across the dispersion estimates of all genes computed in 2) versus the mean normalized counts of the genes.
\item
  Dispersion values of each gene is shrink towards the fitted line in 3).
\item
  A Generalized Linear Model\index{generalized linear model} is fitted which considers additional confounding variables related to the experimental design such as sequencing batches, treatment, temperature, patient's age, sequencing technology, etc., and uses negative binomial distribution for fitting count data.
\item
  For a given contrast (e.g.~treatment type: drug-A versus untreated), a test for differential expression is carried out against the null hypothesis that the log fold change of the normalized counts of the gene in the given pair of groups is exactly zero.
\item
  It adjusts p-values for multiple-testing.
\end{enumerate}

In order to carry out a differential expression analysis using \texttt{DESeq2}, three kinds of inputs are necessary:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The \textbf{read count table}: This table must be raw read counts as integers that are not processed in any form by a normalization technique. The rows represent features (e.g.~genes, transcripts, genomic intervals) and columns represent samples.
\item
  A \textbf{colData} table: This table describes the experimental design.
\item
  A \textbf{design formula}: This formula is needed to describe the variable of interest in the analysis (e.g.~treatment status) along with (optionally) other covariates (e.g.~batch, temperature, sequencing technology).
\end{enumerate}

Let's define these inputs:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#remove the 'width' column}
\NormalTok{countData <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{subset}\NormalTok{(counts, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\NormalTok{width)))}
\CommentTok{#define the experimental setup }
\NormalTok{colData <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(coldata_file, }\DataTypeTok{header =}\NormalTok{ T, }\DataTypeTok{sep =} \StringTok{'}\CharTok{\textbackslash{}t}\StringTok{'}\NormalTok{, }\DataTypeTok{stringsAsFactors =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#define the design formula}
\NormalTok{designFormula <-}\StringTok{ "~ group"}
\end{Highlighting}
\end{Shaded}

Now, we are ready to run \texttt{DESeq2}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(DESeq2)}
\KeywordTok{library}\NormalTok{(stats)}
\CommentTok{#create a DESeq dataset object from the count matrix and the colData }
\NormalTok{dds <-}\StringTok{ }\KeywordTok{DESeqDataSetFromMatrix}\NormalTok{(}\DataTypeTok{countData =}\NormalTok{ countData, }
                              \DataTypeTok{colData =}\NormalTok{ colData, }
                              \DataTypeTok{design =} \KeywordTok{as.formula}\NormalTok{(designFormula))}
\CommentTok{#print dds object to see the contents}
\KeywordTok{print}\NormalTok{(dds)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## class: DESeqDataSet 
## dim: 19719 10 
## metadata(1): version
## assays(1): counts
## rownames(19719): TSPAN6 TNMD ... MYOCOS HSFX3
## rowData names(0):
## colnames(10): CASE_1 CASE_2 ... CTRL_4 CTRL_5
## colData names(2): source_name group
\end{verbatim}

The \texttt{DESeqDataSet} object contains all the information about the experimental setup, the read counts, and the design formulas. Certain functions can be used to access this information separately: \texttt{rownames(dds)} shows which features are used in the study (e.g.~genes), \texttt{colnames(dds)} displays the studied samples, \texttt{counts(dds)} displays the count table, and \texttt{colData(dds)} displays the experimental setup.

Remove genes that have almost no information in any of the given samples.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#For each gene, we count the total number of reads for that gene in all samples }
\CommentTok{#and remove those that don't have at least 1 read. }
\NormalTok{dds <-}\StringTok{ }\NormalTok{dds[ }\KeywordTok{rowSums}\NormalTok{(DESeq2}\OperatorTok{::}\KeywordTok{counts}\NormalTok{(dds)) }\OperatorTok{>}\StringTok{ }\DecValTok{1}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

Now, we can use the \texttt{DESeq()} function of \texttt{DESeq2}, which is a wrapper function that implements estimation of size factors to normalize the counts, estimation of dispersion values, and computing a GLM model based on the experimental design formula. This function returns a \texttt{DESeqDataSet} object, which is an updated version of the \texttt{dds} variable that we pass to the function as input.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dds <-}\StringTok{ }\KeywordTok{DESeq}\NormalTok{(dds)}
\end{Highlighting}
\end{Shaded}

Now, we can compare and contrast the samples based on different variables of interest. In this case, we currently have only one variable, which is the \texttt{group} variable that determines if a sample belongs to the CASE group or the CTRL group.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#compute the contrast for the 'group' variable where 'CTRL' }
\CommentTok{#samples are used as the control group. }
\NormalTok{DEresults =}\StringTok{ }\KeywordTok{results}\NormalTok{(dds, }\DataTypeTok{contrast =} \KeywordTok{c}\NormalTok{(}\StringTok{"group"}\NormalTok{, }\StringTok{'CASE'}\NormalTok{, }\StringTok{'CTRL'}\NormalTok{))}
\CommentTok{#sort results by increasing p-value}
\NormalTok{DEresults <-}\StringTok{ }\NormalTok{DEresults[}\KeywordTok{order}\NormalTok{(DEresults}\OperatorTok{$}\NormalTok{pvalue),]}
\end{Highlighting}
\end{Shaded}

Thus we have obtained a table containing the differential expression status of case samples compared to the control samples.

It is important to note that the sequence of the elements provided in the \texttt{contrast} argument determines which group of samples are to be used as the control. This impacts the way the results are interpreted, for instance, if a gene is found up-regulated (has a positive log2 fold change), the up-regulation status is only relative to the factor that is provided as control. In this case, we used samples from the ``CTRL'' group as control and contrasted the samples from the ``CASE'' group with respect to the ``CTRL'' samples. Thus genes with a positive log2 fold change are called up-regulated in the case samples with respect to the control, while genes with a negative log2 fold change are down-regulated in the case samples. Whether the deregulation is significant or not, warrants assessment of the adjusted p-values.

Let's have a look at the contents of the \texttt{DEresults} table.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#shows a summary of the results}
\KeywordTok{print}\NormalTok{(DEresults)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## log2 fold change (MLE): group CASE vs CTRL 
## Wald test p-value: group CASE vs CTRL 
## DataFrame with 19097 rows and 6 columns
##             baseMean log2FoldChange     lfcSE       stat       pvalue
##            <numeric>      <numeric> <numeric>  <numeric>    <numeric>
## CYP2E1       4829889        9.36024  0.215223    43.4909  0.00000e+00
## FCGBP       10349993       -7.57579  0.186433   -40.6355  0.00000e+00
## ASGR2         426422        8.01830  0.216207    37.0863 4.67898e-301
## GCKR          100183        7.82841  0.233376    33.5442 1.09479e-246
## APOA5         438054       10.20248  0.312503    32.6477 8.64906e-234
## ...              ...            ...       ...        ...          ...
## CCDC195      20.4981      -0.215607   2.89255 -0.0745386           NA
## SPEM3        23.6370     -22.154765   3.02785 -7.3170030           NA
## AC022167.5   21.8451      -2.056240   2.89545 -0.7101618           NA
## BX276092.9   29.9636       0.407326   2.89048  0.1409199           NA
## ETDC         22.5675      -1.795274   2.89421 -0.6202983           NA
##                    padj
##               <numeric>
## CYP2E1      0.00000e+00
## FCGBP       0.00000e+00
## ASGR2      2.87741e-297
## GCKR       5.04945e-243
## APOA5      3.19133e-230
## ...                 ...
## CCDC195              NA
## SPEM3                NA
## AC022167.5           NA
## BX276092.9           NA
## ETDC                 NA
\end{verbatim}

The first three lines in this output show the contrast and the statistical test that were used to compute these results, along with the dimensions of the resulting table (number of columns and rows). Below these lines is the actual table with 6 columns: \texttt{baseMean} represents the average normalized expression of the gene across all considered samples. \texttt{log2FoldChange} represents the base-2 logarithm of the fold change of the normalized expression of the gene in the given contrast. \texttt{lfcSE} represents the standard error of log2 fold change estimate, and \texttt{stat} is the statistic calculated in the contrast which is translated into a \texttt{pvalue} and adjusted for multiple testing in the \texttt{padj} column. To find out about the importance of adjusting for multiple testing, see Chapter \ref{stats}.

\hypertarget{diagnostic-plots}{%
\subsubsection{Diagnostic plots}\label{diagnostic-plots}}

At this point, before proceeding to do any downstream analysis and jumping to conclusions about the biological insights that are reachable with the experimental data at hand, it is important to do some more diagnostic tests to improve our confidence about the quality of the data and the experimental setup.

\hypertarget{ma-plot}{%
\paragraph{MA plot}\label{ma-plot}}

An MA plot is useful to observe if the data normalization worked well (Figure \ref{fig:DEmaplot}). The MA plot is a scatter plot where the x-axis denotes the average of normalized counts across samples and the y-axis denotes the log fold change in the given contrast. Most points are expected to be on the horizontal 0 line (most genes are not expected to be differentially expressed).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(DESeq2)}
\NormalTok{DESeq2}\OperatorTok{::}\KeywordTok{plotMA}\NormalTok{(}\DataTypeTok{object =}\NormalTok{ dds, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/DEmaplot-1} 

}

\caption{MA plot of differential expression results.}\label{fig:DEmaplot}
\end{figure}

\hypertarget{p-value-distribution}{%
\paragraph{P-value distribution}\label{p-value-distribution}}

It is also important to observe the distribution of raw p-values (Figure \ref{fig:DEpvaldist}). We expect to see a peak around low p-values and a uniform distribution at P-values above 0.1. Otherwise, adjustment for multiple testing does not work and the results are not meaningful.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{as.data.frame}\NormalTok{(DEresults), }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ pvalue)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/DEpvaldist-1} 

}

\caption{P-value distribution genes before adjusting for multiple testing.}\label{fig:DEpvaldist}
\end{figure}

\hypertarget{pca-plot}{%
\paragraph{PCA plot}\label{pca-plot}}

A final diagnosis is to check the biological reproducibility of the sample replicates in a PCA plot or a heatmap. To plot the PCA results, we need to extract the normalized counts from the DESeqDataSet object. It is possible to color the points in the scatter plot by the variable of interest, which helps to see if the replicates cluster well (Figure \ref{fig:DEpca}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(DESeq2)}
\CommentTok{# extract normalized counts from the DESeqDataSet object}
\NormalTok{countsNormalized <-}\StringTok{ }\NormalTok{DESeq2}\OperatorTok{::}\KeywordTok{counts}\NormalTok{(dds, }\DataTypeTok{normalized =} \OtherTok{TRUE}\NormalTok{)}

\CommentTok{# select top 500 most variable genes}
\NormalTok{selectedGenes <-}\StringTok{ }\KeywordTok{names}\NormalTok{(}\KeywordTok{sort}\NormalTok{(}\KeywordTok{apply}\NormalTok{(countsNormalized, }\DecValTok{1}\NormalTok{, var), }
                            \DataTypeTok{decreasing =} \OtherTok{TRUE}\NormalTok{)[}\DecValTok{1}\OperatorTok{:}\DecValTok{500}\NormalTok{])}

\KeywordTok{plotPCA}\NormalTok{(countsNormalized[selectedGenes,], }
        \DataTypeTok{col =} \KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group), }\DataTypeTok{adj =} \FloatTok{0.5}\NormalTok{, }
        \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{), }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.6}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/DEpca-1} 

}

\caption{Principle component analysis plot based on top 500 most variable genes.}\label{fig:DEpca}
\end{figure}

Alternatively, the normalized counts can be transformed using the \texttt{DESeq2::rlog} function and \texttt{DESeq2::plotPCA()} can be readily used to plot the PCA results (Figure \ref{fig:DErldnorm}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rld <-}\StringTok{ }\KeywordTok{rlog}\NormalTok{(dds)}
\NormalTok{DESeq2}\OperatorTok{::}\KeywordTok{plotPCA}\NormalTok{(rld, }\DataTypeTok{ntop =} \DecValTok{500}\NormalTok{, }\DataTypeTok{intgroup =} \StringTok{'group'}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{ylim}\NormalTok{(}\OperatorTok{-}\DecValTok{50}\NormalTok{, }\DecValTok{50}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/DErldnorm-1} 

}

\caption{PCA plot of top 500 most variable genes }\label{fig:DErldnorm}
\end{figure}

\hypertarget{relative-log-expression-rle-plot}{%
\paragraph{Relative Log Expression (RLE) plot}\label{relative-log-expression-rle-plot}}

A similar plot to the MA plot is the RLE (Relative Log Expression) plot that is useful in finding out if the data at hand needs normalization \citep{gandolfo_rle_2018}. Sometimes, even the datasets normalized using the explained methods above may need further normalization due to unforeseen sources of variation that might stem from the library preparation, the person who carries out the experiment, the date of sequencing, the temperature changes in the laboratory at the time of library preparation, and so on and so forth. The RLE plot is a quick diagnostic that can be applied on the raw or normalized count matrices to see if further processing is required.

Let's do RLE plots on the raw counts and normalized counts using the \texttt{EDASeq} package \citep{risso_gc-content_2011}\index{R Packages!\texttt{EDASeq}} (see Figure \ref{fig:DErleplot}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(EDASeq)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{plotRLE}\NormalTok{(countData, }\DataTypeTok{outline=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{), }
        \DataTypeTok{col=}\KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group), }
        \DataTypeTok{main =} \StringTok{'Raw Counts'}\NormalTok{)}
\KeywordTok{plotRLE}\NormalTok{(DESeq2}\OperatorTok{::}\KeywordTok{counts}\NormalTok{(dds, }\DataTypeTok{normalized =} \OtherTok{TRUE}\NormalTok{), }
        \DataTypeTok{outline=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{), }
        \DataTypeTok{col =} \KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group), }
        \DataTypeTok{main =} \StringTok{'Normalized Counts'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/DErleplot-1} 

}

\caption{Relative log expression plots based on raw and normalized count matrices}\label{fig:DErleplot}
\end{figure}

Here the RLE plot is comprised of boxplots, where each box-plot represents the distribution of the relative log expression of the genes expressed in the corresponding sample. Each gene's expression is divided by the median expression value of that gene across all samples. Then this is transformed to log scale, which gives the relative log expression value for a single gene. The RLE values for all the genes from a sample are visualized as a boxplot.

Ideally the boxplots are centered around the horizontal zero line and are as tightly distributed as possible \citep{risso_normalization_2014}. From the plots that we have made for the raw and normalized count data, we can observe how the normalized dataset has improved upon the raw count data for all the samples. However, in some cases, it is important to visualize RLE plots in combination with other diagnostic plots such as PCA plots, heatmaps, and correlation plots to see if there is more unwanted variation in the data, which can be further accounted for using packages such as \texttt{RUVSeq} \citep{risso_normalization_2014}\index{R Packages!\texttt{RUVSeq}} and \texttt{sva} \citep{leek_sva_2012}\index{R Packages!\texttt{sva}}. We will cover details about the \texttt{RUVSeq} package to account for unwanted sources of noise in RNA-seq datasets in later sections.

\hypertarget{functional-enrichment-analysis}{%
\subsection{Functional enrichment analysis}\label{functional-enrichment-analysis}}

\hypertarget{go-term-analysis}{%
\subsubsection{GO term analysis}\label{go-term-analysis}}

In a typical differential expression analysis, thousands of genes are found differentially expressed between two groups of samples. While prior knowledge of the functions of individual genes can give some clues about what kind of cellular processes have been affected, e.g.~by a drug treatment, manually going through the whole list of thousands of genes would be very cumbersome and not be very informative in the end. Therefore a commonly used tool to address this problem is to do enrichment analyses of functional terms that appear associated to the given set of differentially expressed genes more often than expected by chance. The functional terms are usually associated to multiple genes. Thus, genes can be grouped into sets by shared functional terms. However, it is important to have an agreed upon controlled vocabulary on the list of terms used to describe the functions of genes. Otherwise, it would be impossible to exchange scientific results globally. That's why initiatives such as the Gene Ontology Consortium have collated a list of Gene Ontology (GO) \index{Gene Ontology (GO)}terms for each gene. GO term analysis is probably the most common analysis applied after a differential expression analysis. GO term analysis helps quickly find out systematic changes that can describe differences between groups of samples.

In R, one of the simplest ways to do functional enrichment analysis for a set of genes is via the \texttt{gProfileR} package. \index{R Packages!\texttt{gProfileR}}.

Let's select the genes that are significantly differentially expressed between the case and control samples.
Let's extract genes that have an adjusted p-value below 0.1 and that show a 2-fold change (either negative or positive) in the case compared to control. We will then feed this gene set into the \texttt{gProfileR} function. The top 10 detected GO terms are displayed in Table \ref{tab:GOanalysistable}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(DESeq2)}
\KeywordTok{library}\NormalTok{(gProfileR)}
\KeywordTok{library}\NormalTok{(knitr)}
\CommentTok{# extract differential expression results}
\NormalTok{DEresults <-}\StringTok{ }\KeywordTok{results}\NormalTok{(dds, }\DataTypeTok{contrast =} \KeywordTok{c}\NormalTok{(}\StringTok{'group'}\NormalTok{, }\StringTok{'CASE'}\NormalTok{, }\StringTok{'CTRL'}\NormalTok{))}

\CommentTok{#remove genes with NA values }
\NormalTok{DE <-}\StringTok{ }\NormalTok{DEresults[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(DEresults}\OperatorTok{$}\NormalTok{padj),]}
\CommentTok{#select genes with adjusted p-values below 0.1}
\NormalTok{DE <-}\StringTok{ }\NormalTok{DE[DE}\OperatorTok{$}\NormalTok{padj }\OperatorTok{<}\StringTok{ }\FloatTok{0.1}\NormalTok{,]}
\CommentTok{#select genes with absolute log2 fold change above 1 (two-fold change)}
\NormalTok{DE <-}\StringTok{ }\NormalTok{DE[}\KeywordTok{abs}\NormalTok{(DE}\OperatorTok{$}\NormalTok{log2FoldChange) }\OperatorTok{>}\StringTok{ }\DecValTok{1}\NormalTok{,]}

\CommentTok{#get the list of genes of interest}
\NormalTok{genesOfInterest <-}\StringTok{ }\KeywordTok{rownames}\NormalTok{(DE)}

\CommentTok{#calculate enriched GO terms}
\NormalTok{goResults <-}\StringTok{ }\KeywordTok{gprofiler}\NormalTok{(}\DataTypeTok{query =}\NormalTok{ genesOfInterest, }
                     \DataTypeTok{organism =} \StringTok{'hsapiens'}\NormalTok{, }
                     \DataTypeTok{src_filter =} \StringTok{'GO'}\NormalTok{, }
                     \DataTypeTok{hier_filtering =} \StringTok{'moderate'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:GOanalysistable}Top GO terms sorted by p-value.}
\centering
\begin{tabular}[t]{lrrrll}
\toprule
  & p.value & term.size & precision & domain & term.name\\
\midrule
52 & 0 & 2740 & 0.223 & CC & plasma membrane part\\
3 & 0 & 1609 & 0.136 & BP & ion transport\\
32 & 0 & 3656 & 0.258 & BP & regulation of biological quality\\
26 & 0 & 385 & 0.042 & BP & extracellular structure organization\\
17 & 0 & 7414 & 0.452 & BP & multicellular organismal process\\
\addlinespace
68 & 0 & 1069 & 0.090 & MF & transmembrane transporter activity\\
43 & 0 & 1073 & 0.090 & BP & organic acid metabolic process\\
6 & 0 & 975 & 0.083 & BP & response to drug\\
9 & 0 & 1351 & 0.107 & BP & biological adhesion\\
49 & 0 & 4760 & 0.302 & BP & system development\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{gene-set-enrichment-analysis}{%
\subsubsection{Gene set enrichment analysis}\label{gene-set-enrichment-analysis}}

A gene set is a collection of genes with some common property. This shared property among a set of genes could be a GO term, a common biological pathway, a shared interaction partner, or any biologically relevant commonality that is meaningful in the context of the pursued experiment. Gene set enrichment analysis (GSEA) is a valuable exploratory analysis tool that can associate systematic changes to a high-level function rather than individual genes. Analysis of coordinated changes of expression levels of gene sets can provide complementary benefits on top of per-gene-based differential expression analyses. For instance, consider a gene set belonging to a biological pathway where each member of the pathway displays a slight deregulation in a disease sample compared to a normal sample. In such a case, individual genes might not be picked up by the per-gene-based differential expression analysis. Thus, the GO/Pathway enrichment on the differentially expressed list of genes would not show an enrichment of this pathway. However, the additive effect of slight changes of the genes could amount to a large effect at the level of the gene set, thus the pathway could be detected as a significant pathway that could explain the mechanistic problems in the disease sample.

We use the bioconductor package \texttt{gage} \citep{luo_gage:_2009} \index{R Packages!\texttt{gage}}to demonstrate how to do GSEA using normalized expression data of the samples as input. Here we are using only two gene sets: one from the top GO term discovered from the previous GO analysis, one that we compile by randomly selecting a list of genes. However, annotated gene sets can be used from databases such as MSIGDB \citep{subramanian_gene_2005}, which compile gene sets from a variety of resources such as KEGG \citep{kanehisa_kegg_2016} and REACTOME \citep{fabregat_reactome_2018}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Let's define the first gene set as the list of genes from one of the}
\CommentTok{#significant GO terms found in the GO analysis. order go results by pvalue}
\NormalTok{goResults <-}\StringTok{ }\NormalTok{goResults[}\KeywordTok{order}\NormalTok{(goResults}\OperatorTok{$}\NormalTok{p.value),]}
\CommentTok{#restrict the terms that have at most 100 genes overlapping with the query}
\NormalTok{go <-}\StringTok{ }\NormalTok{goResults[goResults}\OperatorTok{$}\NormalTok{overlap.size }\OperatorTok{<}\StringTok{ }\DecValTok{100}\NormalTok{,]}
\CommentTok{# use the top term from this table to create a gene set }
\NormalTok{geneSet1 <-}\StringTok{ }\KeywordTok{unlist}\NormalTok{(}\KeywordTok{strsplit}\NormalTok{(go[}\DecValTok{1}\NormalTok{,]}\OperatorTok{$}\NormalTok{intersection, }\StringTok{','}\NormalTok{))}

\CommentTok{#Define another gene set by just randomly selecting 25 genes from the counts}
\CommentTok{#table get normalized counts from DESeq2 results}
\NormalTok{normalizedCounts <-}\StringTok{ }\NormalTok{DESeq2}\OperatorTok{::}\KeywordTok{counts}\NormalTok{(dds, }\DataTypeTok{normalized =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{geneSet2 <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\KeywordTok{rownames}\NormalTok{(normalizedCounts), }\DecValTok{25}\NormalTok{)}

\NormalTok{geneSets <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\StringTok{'top_GO_term'}\NormalTok{ =}\StringTok{ }\NormalTok{geneSet1,}
                 \StringTok{'random_set'}\NormalTok{ =}\StringTok{ }\NormalTok{geneSet2)}
\end{Highlighting}
\end{Shaded}

Using the defined gene sets, we'd like to do a group comparison between the case samples with respect to the control samples.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gage)}
\CommentTok{#use the normalized counts to carry out a GSEA. }
\NormalTok{gseaResults <-}\StringTok{ }\KeywordTok{gage}\NormalTok{(}\DataTypeTok{exprs =} \KeywordTok{log2}\NormalTok{(normalizedCounts}\OperatorTok{+}\DecValTok{1}\NormalTok{), }
           \DataTypeTok{ref =} \KeywordTok{match}\NormalTok{(}\KeywordTok{rownames}\NormalTok{(colData[colData}\OperatorTok{$}\NormalTok{group }\OperatorTok{==}\StringTok{ 'CTRL'}\NormalTok{,]), }
                       \KeywordTok{colnames}\NormalTok{(normalizedCounts)), }
           \DataTypeTok{samp =} \KeywordTok{match}\NormalTok{(}\KeywordTok{rownames}\NormalTok{(colData[colData}\OperatorTok{$}\NormalTok{group }\OperatorTok{==}\StringTok{ 'CASE'}\NormalTok{,]), }
                        \KeywordTok{colnames}\NormalTok{(normalizedCounts)),}
           \DataTypeTok{gsets =}\NormalTok{ geneSets, }\DataTypeTok{compare =} \StringTok{'as.group'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can observe if there is a significant up-regulation or down-regulation of the gene set in the case group compared to the controls by accessing \texttt{gseaResults\$greater} as in Table \ref{tab:gseaPost1} or \texttt{gseaResults\$less} as in Table \ref{tab:gseaPost2}.

\begin{table}

\caption{\label{tab:gseaPost1}Up-regulation statistics}
\centering
\begin{tabular}[t]{lrrrrrr}
\toprule
  & p.geomean & stat.mean & p.val & q.val & set.size & exp1\\
\midrule
top\_GO\_term & 0.0000 & 7.1994 & 0.0000 & 0.0000 & 32 & 0.0000\\
random\_set & 0.4761 & 0.0604 & 0.4761 & 0.4761 & 25 & 0.4761\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:gseaPost2}Down-regulation statistics}
\centering
\begin{tabular}[t]{lrrrrrr}
\toprule
  & p.geomean & stat.mean & p.val & q.val & set.size & exp1\\
\midrule
random\_set & 0.5239 & 0.0604 & 0.5239 & 1 & 25 & 0.5239\\
top\_GO\_term & 1.0000 & 7.1994 & 1.0000 & 1 & 32 & 1.0000\\
\bottomrule
\end{tabular}
\end{table}

We can see that the random gene set shows no significant up- or down-regulation (Tables \ref{tab:gseaPost1} and (\ref{tab:gseaPost2}), while the gene set we defined using the top GO term shows a significant up-regulation (adjusted p-value \textless{} 0.0007) (\ref{tab:gseaPost1}). It is worthwhile to visualize these systematic changes in a heatmap as in Figure \ref{fig:gseaPost3}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(pheatmap)}
\CommentTok{# get the expression data for the gene set of interest}
\NormalTok{M <-}\StringTok{ }\NormalTok{normalizedCounts[}\KeywordTok{rownames}\NormalTok{(normalizedCounts) }\OperatorTok{%in%}\StringTok{ }\NormalTok{geneSet1, ]}
\CommentTok{# log transform the counts for visualization scaling by row helps visualizing}
\CommentTok{# relative change of expression of a gene in multiple conditions}
\KeywordTok{pheatmap}\NormalTok{(}\KeywordTok{log2}\NormalTok{(M}\OperatorTok{+}\DecValTok{1}\NormalTok{), }
         \DataTypeTok{annotation_col =}\NormalTok{ colData, }
         \DataTypeTok{show_rownames =} \OtherTok{TRUE}\NormalTok{, }
         \DataTypeTok{fontsize_row =} \DecValTok{8}\NormalTok{,}
         \DataTypeTok{scale =} \StringTok{'row'}\NormalTok{, }
         \DataTypeTok{cutree_cols =} \DecValTok{2}\NormalTok{, }
         \DataTypeTok{cutree_rows =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/gseaPost3-1} 

}

\caption{Heatmap of expression value from the genes with the top GO term.}\label{fig:gseaPost3}
\end{figure}

We can see that almost all genes from this gene set display an increased level of expression in the case samples
compared to the controls.

\hypertarget{accounting-for-additional-sources-of-variation}{%
\subsection{Accounting for additional sources of variation}\label{accounting-for-additional-sources-of-variation}}

When doing a differential expression analysis in a case-control setting, the variable of interest, i.e.~the variable that explains the separation of the case samples from the control, is usually the treatment, genotypic differences, a certain phenotype, etc. However, in reality, depending on how the experiment and the sequencing were designed, there may be additional factors that might contribute to the variation between the compared samples. Sometimes, such variables are known, for instance, the date of the sequencing for each sample (batch information), or the temperature under which samples were kept. Such variables are not necessarily biological but rather technical, however, they still impact the measurements obtained from an RNA-seq experiment. Such variables can introduce systematic shifts in the obtained measurements. Here, we will demonstrate: firstly how to account for such variables using DESeq2, when the possible sources of variation are actually known; secondly, how to account for such variables when all we have is just a count table but we observe that the variable of interest only explains a small proportion of the differences between case and control samples.

\hypertarget{accounting-for-covariates-using-deseq2}{%
\subsubsection{Accounting for covariates using DESeq2}\label{accounting-for-covariates-using-deseq2}}

For demonstration purposes, we will use a subset of the count table obtained for a heart disease study, where there are RNA-seq samples from subjects with normal and failing hearts. We again use a subset of the samples, focusing on 6 case and 6 control samples and we only consider protein-coding genes (for speed concerns).

Let's import count and colData for this experiment.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{counts_file <-}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\StringTok{'extdata/rna-seq/SRP021193.raw_counts.tsv'}\NormalTok{, }
                           \DataTypeTok{package =} \StringTok{'compGenomRData'}\NormalTok{)}
\NormalTok{colData_file <-}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\StringTok{'extdata/rna-seq/SRP021193.colData.tsv'}\NormalTok{, }
                            \DataTypeTok{package =} \StringTok{'compGenomRData'}\NormalTok{)}

\NormalTok{counts <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(counts_file)}
\NormalTok{colData <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(colData_file, }\DataTypeTok{header =}\NormalTok{ T, }\DataTypeTok{sep =} \StringTok{'}\CharTok{\textbackslash{}t}\StringTok{'}\NormalTok{, }
                      \DataTypeTok{stringsAsFactors =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's take a look at how the samples cluster by calculating the TPM counts as displayed as a heatmap in Figure \ref{fig:batcheffects2}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(pheatmap)}
\CommentTok{#find gene length normalized values }
\NormalTok{geneLengths <-}\StringTok{ }\NormalTok{counts}\OperatorTok{$}\NormalTok{width}
\NormalTok{rpk <-}\StringTok{ }\KeywordTok{apply}\NormalTok{( }\KeywordTok{subset}\NormalTok{(counts, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\NormalTok{width)), }\DecValTok{2}\NormalTok{, }
              \ControlFlowTok{function}\NormalTok{(x) x}\OperatorTok{/}\NormalTok{(geneLengths}\OperatorTok{/}\DecValTok{1000}\NormalTok{))}
\CommentTok{#normalize by the sample size using rpk values}
\NormalTok{tpm <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(rpk, }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) x }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(x)) }\OperatorTok{*}\StringTok{ }\DecValTok{10}\OperatorTok{^}\DecValTok{6}\NormalTok{)}

\NormalTok{selectedGenes <-}\StringTok{ }\KeywordTok{names}\NormalTok{(}\KeywordTok{sort}\NormalTok{(}\KeywordTok{apply}\NormalTok{(tpm, }\DecValTok{1}\NormalTok{, var), }
                            \DataTypeTok{decreasing =}\NormalTok{ T)[}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{])}
\KeywordTok{pheatmap}\NormalTok{(tpm[selectedGenes,], }
             \DataTypeTok{scale =} \StringTok{'row'}\NormalTok{,}
             \DataTypeTok{annotation_col =}\NormalTok{ colData, }
             \DataTypeTok{show_rownames =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/batcheffects2-1} 

}

\caption{Visualizing batch effects in an experiment.}\label{fig:batcheffects2}
\end{figure}

Here we can see from the clusters that the dominating variable is the `Library Selection' variable rather than the `diagnosis' variable, which determines the state of the organ from which the sample was taken. Case and control samples are all mixed in both two major clusters. However, ideally, we'd like to see a separation of the case and control samples regardless of the additional covariates. When testing for differential gene expression between conditions, such confounding variables can be accounted for using \texttt{DESeq2}. Below is a demonstration of how we instruct \texttt{DESeq2} to account for the `library selection' variable:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(DESeq2)}
\CommentTok{# remove the 'width' column from the counts matrix}
\NormalTok{countData <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{subset}\NormalTok{(counts, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\NormalTok{width)))}
\CommentTok{# set up a DESeqDataSet object}
\NormalTok{dds <-}\StringTok{ }\KeywordTok{DESeqDataSetFromMatrix}\NormalTok{(}\DataTypeTok{countData =}\NormalTok{ countData, }
                              \DataTypeTok{colData =}\NormalTok{ colData, }
                              \DataTypeTok{design =} \OperatorTok{~}\StringTok{ }\NormalTok{LibrarySelection }\OperatorTok{+}\StringTok{ }\NormalTok{group)}
\end{Highlighting}
\end{Shaded}

When constructing the design formula, it is very important to pay attention to the sequence of variables. We leave the variable of interest to the last and we can add as many covariates as we want to the beginning of the design formula. Please refer to the \texttt{DESeq2} vignette if you'd like to learn more about how to construct design formulas.

Now, we can run the differential expression analysis as has been demonstrated previously.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# run DESeq}
\NormalTok{dds <-}\StringTok{ }\KeywordTok{DESeq}\NormalTok{(dds)}
\CommentTok{# extract results}
\NormalTok{DEresults <-}\StringTok{ }\KeywordTok{results}\NormalTok{(dds, }\DataTypeTok{contrast =} \KeywordTok{c}\NormalTok{(}\StringTok{'group'}\NormalTok{, }\StringTok{'CASE'}\NormalTok{, }\StringTok{'CTRL'}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{accounting-for-estimated-covariates-using-ruvseq}{%
\subsubsection{Accounting for estimated covariates using RUVSeq}\label{accounting-for-estimated-covariates-using-ruvseq}}

In cases when the sources of potential variation are not known, it is worthwhile to use tools such as \texttt{RUVSeq} or \texttt{sva} that can estimate potential sources of variation and clean up the counts table from those sources of variation. Later on, the estimated covariates can be integrated into DESeq2's design formula.

Let's see how to utilize the \texttt{RUVseq} package to first diagnose the problem and then solve it. Here, for demonstration purposes, we'll use a count table from a lung carcinoma study in which a transcription factor (Ets homologous factor - EHF) is overexpressed and compared to the control samples with baseline EHF expression. Again, we only consider protein coding genes and use only five case and five control samples. The original data can be found on the \texttt{recount2} database with the accession `SRP049988'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{counts_file <-}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\StringTok{'extdata/rna-seq/SRP049988.raw_counts.tsv'}\NormalTok{, }
                           \DataTypeTok{package =} \StringTok{'compGenomRData'}\NormalTok{)}
\NormalTok{colData_file <-}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\StringTok{'extdata/rna-seq/SRP049988.colData.tsv'}\NormalTok{, }
                            \DataTypeTok{package =} \StringTok{'compGenomRData'}\NormalTok{)}

\NormalTok{counts <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(counts_file)}
\NormalTok{colData <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(colData_file, }\DataTypeTok{header =}\NormalTok{ T, }
                      \DataTypeTok{sep =} \StringTok{'}\CharTok{\textbackslash{}t}\StringTok{'}\NormalTok{, }\DataTypeTok{stringsAsFactors =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{# simplify condition descriptions}
\NormalTok{colData}\OperatorTok{$}\NormalTok{source_name <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group }\OperatorTok{==}\StringTok{ 'CASE'}\NormalTok{, }
                              \StringTok{'EHF_overexpression'}\NormalTok{, }\StringTok{'Empty_Vector'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's start by making heatmaps of the samples using TPM counts (see Figure \ref{fig:ruvdiagnose1}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#find gene length normalized values }
\NormalTok{geneLengths <-}\StringTok{ }\NormalTok{counts}\OperatorTok{$}\NormalTok{width}
\NormalTok{rpk <-}\StringTok{ }\KeywordTok{apply}\NormalTok{( }\KeywordTok{subset}\NormalTok{(counts, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\NormalTok{width)), }\DecValTok{2}\NormalTok{, }
              \ControlFlowTok{function}\NormalTok{(x) x}\OperatorTok{/}\NormalTok{(geneLengths}\OperatorTok{/}\DecValTok{1000}\NormalTok{))}
\CommentTok{#normalize by the sample size using rpk values}
\NormalTok{tpm <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(rpk, }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) x }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(x)) }\OperatorTok{*}\StringTok{ }\DecValTok{10}\OperatorTok{^}\DecValTok{6}\NormalTok{)}
\NormalTok{selectedGenes <-}\StringTok{ }\KeywordTok{names}\NormalTok{(}\KeywordTok{sort}\NormalTok{(}\KeywordTok{apply}\NormalTok{(tpm, }\DecValTok{1}\NormalTok{, var), }
                            \DataTypeTok{decreasing =}\NormalTok{ T)[}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{])}
\KeywordTok{pheatmap}\NormalTok{(tpm[selectedGenes,], }
             \DataTypeTok{scale =} \StringTok{'row'}\NormalTok{,}
             \DataTypeTok{annotation_col =}\NormalTok{ colData, }
             \DataTypeTok{cutree_cols =} \DecValTok{2}\NormalTok{, }
             \DataTypeTok{show_rownames =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/ruvdiagnose1-1} 

}

\caption{Diagnostic plot to observe.}\label{fig:ruvdiagnose1}
\end{figure}

We can see that the overall clusters look fine, except that one of the case samples (CASE\_5) clusters more closely with the control samples than the other case samples. This mis-clustering could be a result of some batch effect, or any other technical preparation steps. However, the \texttt{colData} object doesn't contain any variables that we can use to pinpoint the exact cause of this. So, let's use \texttt{RUVSeq} to estimate potential covariates to see if the clustering results can be improved.

First, we set up the experiment:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(EDASeq)}
\CommentTok{# remove 'width' column from counts}
\NormalTok{countData <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{subset}\NormalTok{(counts, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\NormalTok{width)))}
\CommentTok{# create a seqExpressionSet object using EDASeq package }
\NormalTok{set <-}\StringTok{ }\KeywordTok{newSeqExpressionSet}\NormalTok{(}\DataTypeTok{counts =}\NormalTok{ countData,}
                           \DataTypeTok{phenoData =}\NormalTok{ colData)}
\end{Highlighting}
\end{Shaded}

Next, let's make a diagnostic RLE plot on the raw count table.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# make an RLE plot and a PCA plot on raw count data and color samples by group}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plotRLE}\NormalTok{(set, }\DataTypeTok{outline=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{), }\DataTypeTok{col=}\KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group))}
\KeywordTok{plotPCA}\NormalTok{(set, }\DataTypeTok{col =} \KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group), }\DataTypeTok{adj =} \FloatTok{0.5}\NormalTok{, }
        \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.7}\NormalTok{, }\FloatTok{0.5}\NormalTok{), }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/ruvdiagnose2p1-1} 

}

\caption{Diagnostic RLE and PCA plots based on raw count table.}\label{fig:ruvdiagnose2p1}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## make RLE and PCA plots on TPM matrix }
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plotRLE}\NormalTok{(tpm, }\DataTypeTok{outline=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{), }\DataTypeTok{col=}\KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group))}
\KeywordTok{plotPCA}\NormalTok{(tpm, }\DataTypeTok{col=}\KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group), }\DataTypeTok{adj =} \FloatTok{0.5}\NormalTok{, }
        \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.3}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/ruvdiagnose2p2-1} 

}

\caption{Diagnostic RLE and PCA plots based on TPM normalized count table.}\label{fig:ruvdiagnose2p2}
\end{figure}

Both RLE and PCA plots look better on normalized data (Figure \ref{fig:ruvdiagnose2p2}) compared to raw data (Figure \ref{fig:ruvdiagnose2p1}), but still suggest the necessity of further improvement, because the CASE\_5 sample still clusters with the control samples. We haven't yet accounted for the source of unwanted variation.

\hypertarget{removing-unwanted-variation-from-the-data}{%
\subsubsection{Removing unwanted variation from the data}\label{removing-unwanted-variation-from-the-data}}

\texttt{RUVSeq} has three main functions for removing unwanted variation: \texttt{RUVg()}, \texttt{RUVs()}, and \texttt{RUVr()}. Here, we will demonstrate how to use \texttt{RUVg} and \texttt{RUVs}. \texttt{RUVr} will be left as an exercise for the reader.

\hypertarget{using-ruvg}{%
\paragraph{Using RUVg}\label{using-ruvg}}

One way of removing unwanted variation depends on using a set of reference genes that are not expected to change by the sources of technical variation. One strategy along this line is to use spike-in genes, which are artificially introduced into the sequencing run \citep{jiang_synthetic_2011}. However, there are many sequencing datasets that don't have this spike-in data available. In such cases, an empirical set of genes can be collected from the expression data by doing a differential expression analysis and discovering genes that are unchanged in the given conditions. These unchanged genes are used to clean up the data from systematic shifts in expression due to the unwanted sources of variation. Another strategy could be to use a set of house-keeping genes as negative controls, and use them as a reference to correct the systematic biases in the data. Let's use a list of \textasciitilde500 house-keeping genes compiled here: \url{https://www.tau.ac.il/~elieis/HKG/HK_genes.txt}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(RUVSeq)}
 
\CommentTok{#source for house-keeping genes collection:}
\CommentTok{#https://m.tau.ac.il/~elieis/HKG/HK_genes.txt}
\NormalTok{HK_genes <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\DataTypeTok{file =} \KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata/rna-seq/HK_genes.txt"}\NormalTok{, }
                                          \DataTypeTok{package =} \StringTok{'compGenomRData'}\NormalTok{), }
                       \DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{)}
\CommentTok{# let's take an intersection of the house-keeping genes with the genes available}
\CommentTok{# in the count table}
\NormalTok{house_keeping_genes <-}\StringTok{ }\KeywordTok{intersect}\NormalTok{(}\KeywordTok{rownames}\NormalTok{(set), HK_genes}\OperatorTok{$}\NormalTok{V1)}
\end{Highlighting}
\end{Shaded}

We will now run \texttt{RUVg()} with the different number of factors of unwanted variation. We will plot the PCA after removing the unwanted variation. We should be able to see which \texttt{k} values, number of factors, produce better separation between sample groups.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# now, we use these genes as the empirical set of genes as input to RUVg.}
\CommentTok{# we try different values of k and see how the PCA plots look }
 
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{(k }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{) \{}
\NormalTok{  set_g <-}\StringTok{ }\KeywordTok{RUVg}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ set, }\DataTypeTok{cIdx =}\NormalTok{ house_keeping_genes, }\DataTypeTok{k =}\NormalTok{ k)}
  \KeywordTok{plotPCA}\NormalTok{(set_g, }\DataTypeTok{col=}\KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group), }\DataTypeTok{cex =} \FloatTok{0.9}\NormalTok{, }\DataTypeTok{adj =} \FloatTok{0.5}\NormalTok{, }
          \DataTypeTok{main =} \KeywordTok{paste0}\NormalTok{(}\StringTok{'with RUVg, k = '}\NormalTok{,k), }
          \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/ruvgf1-1} 

}

\caption{PCA plots on RUVg normalized data with varying number of covariates (k).}\label{fig:ruvgf1}
\end{figure}

Based on the separation of case and control samples in the PCA plots in Figure \ref{fig:ruvgf1},
we choose k = 1 and re-run the \texttt{RUVg()} function with the house-keeping genes to do more diagnostic plots.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# choose k = 1}
 
\NormalTok{set_g <-}\StringTok{ }\KeywordTok{RUVg}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ set, }\DataTypeTok{cIdx =}\NormalTok{ house_keeping_genes, }\DataTypeTok{k =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now let's do diagnostics: compare the count matrices with or without RUVg processing, comparing RLE plots (Figure \ref{fig:ruvgf2}) and PCA plots (Figure \ref{fig:ruvgf3}) to see the effect of RUVg on the normalization and separation of case and control samples.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# RLE plots}
 
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plotRLE}\NormalTok{(set, }\DataTypeTok{outline=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{), }
        \DataTypeTok{col=}\KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group), }\DataTypeTok{main =} \StringTok{'without RUVg'}\NormalTok{)}
\KeywordTok{plotRLE}\NormalTok{(set_g, }\DataTypeTok{outline=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{), }
        \DataTypeTok{col=}\KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group), }\DataTypeTok{main =} \StringTok{'with RUVg'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/ruvgf2-1} 

}

\caption{RLE plots to observe the effect of RUVg.}\label{fig:ruvgf2}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# PCA plots}
 
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plotPCA}\NormalTok{(set, }\DataTypeTok{col=}\KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group), }\DataTypeTok{adj =} \FloatTok{0.5}\NormalTok{,}
        \DataTypeTok{main =} \StringTok{'without RUVg'}\NormalTok{, }
        \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{), }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{))}
\KeywordTok{plotPCA}\NormalTok{(set_g, }\DataTypeTok{col=}\KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group), }\DataTypeTok{adj =} \FloatTok{0.5}\NormalTok{, }
        \DataTypeTok{main =} \StringTok{'with RUVg'}\NormalTok{,}
        \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{), }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/ruvgf3-1} 

}

\caption{PCA plots to observe the effect of RUVg.}\label{fig:ruvgf3}
\end{figure}

We can observe that using \texttt{RUVg()} with house-keeping genes as reference has improved the clusters, however not yielded ideal separation. Probably the effect that is causing the `CASE\_5' to cluster with the control samples still hasn't been completely eliminated.

\hypertarget{using-ruvs}{%
\paragraph{Using RUVs}\label{using-ruvs}}

There is another strategy of \texttt{RUVSeq} that works better in the presence of replicates in the absence of a confounded experimental design, which is the \texttt{RUVs()} function. Let's see how that performs with this data. This time we don't use the house-keeping genes. We rather use all genes as input to \texttt{RUVs()}. This function estimates the correction factor by assuming that replicates should have constant biological variation, rather, the variation in the replicates are the unwanted variation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# make a table of sample groups from colData }
\NormalTok{differences <-}\StringTok{ }\KeywordTok{makeGroups}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group)}
\CommentTok{## looking for two different sources of unwanted variation (k = 2)}
\CommentTok{## use information from all genes in the expression object}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{(k }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{) \{}
\NormalTok{  set_s <-}\StringTok{ }\KeywordTok{RUVs}\NormalTok{(set, }\KeywordTok{unique}\NormalTok{(}\KeywordTok{rownames}\NormalTok{(set)), }
                \DataTypeTok{k=}\NormalTok{k, differences) }\CommentTok{#all genes}
  \KeywordTok{plotPCA}\NormalTok{(set_s, }\DataTypeTok{col=}\KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group), }
          \DataTypeTok{cex =} \FloatTok{0.9}\NormalTok{, }\DataTypeTok{adj =} \FloatTok{0.5}\NormalTok{, }
        \DataTypeTok{main =} \KeywordTok{paste0}\NormalTok{(}\StringTok{'with RUVs, k = '}\NormalTok{,k), }
        \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.6}\NormalTok{, }\FloatTok{0.6}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/ruvsf1-1} 

}

\caption{PCA plots on RUVs normalized data with varying number of covariates (k).}\label{fig:ruvsf1}
\end{figure}

Based on the separation of case and control samples in the PCA plots in Figure \ref{fig:ruvsf1},
we can see that the samples are better separated event at k = 2 when using \texttt{RUVs()}. Here, we re-run the \texttt{RUVs()} function using k = 2, in order to do more diagnostic plots. We try to pick a value of k that is good enough to distinguish the samples by condition of interest. While setting the value of k to higher values could improve the percentage of explained variation by the first principle component to up to 61\%, we try to avoid setting the value unnecessarily high to avoid removing factors that might also correlate with important biological differences between conditions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# choose k = 2}
\NormalTok{set_s <-}\StringTok{ }\KeywordTok{RUVs}\NormalTok{(set, }\KeywordTok{unique}\NormalTok{(}\KeywordTok{rownames}\NormalTok{(set)), }\DataTypeTok{k=}\DecValTok{2}\NormalTok{, differences) }\CommentTok{#}
\end{Highlighting}
\end{Shaded}

Now let's do diagnostics again: compare the count matrices with or without RUVs processing, comparing RLE plots (Figure \ref{fig:ruvsf2}) and PCA plots (Figure \ref{fig:ruvsf3}) to see the effect of RUVg on the normalization and separation of case and control samples.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## compare the initial and processed objects}
\CommentTok{## RLE plots}
 
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plotRLE}\NormalTok{(set, }\DataTypeTok{outline=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{), }
        \DataTypeTok{col=}\KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group), }
        \DataTypeTok{main =} \StringTok{'without RUVs'}\NormalTok{)}
\KeywordTok{plotRLE}\NormalTok{(set_s, }\DataTypeTok{outline=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{), }
        \DataTypeTok{col=}\KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group),}
        \DataTypeTok{main =} \StringTok{'with RUVs'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/ruvsf2-1} 

}

\caption{RLE plots to observe the effect of RUVs.}\label{fig:ruvsf2}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## PCA plots}
 
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plotPCA}\NormalTok{(set, }\DataTypeTok{col=}\KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group), }
        \DataTypeTok{main =} \StringTok{'without RUVs'}\NormalTok{, }\DataTypeTok{adj =} \FloatTok{0.5}\NormalTok{,}
        \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.75}\NormalTok{, }\FloatTok{0.75}\NormalTok{), }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.75}\NormalTok{, }\FloatTok{0.75}\NormalTok{))}
\KeywordTok{plotPCA}\NormalTok{(set_s, }\DataTypeTok{col=}\KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group), }
        \DataTypeTok{main =} \StringTok{'with RUVs'}\NormalTok{, }\DataTypeTok{adj =} \FloatTok{0.5}\NormalTok{,}
        \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.75}\NormalTok{, }\FloatTok{0.75}\NormalTok{), }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.75}\NormalTok{, }\FloatTok{0.75}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/ruvsf3-1} 

}

\caption{PCA plots to observe the effect of RUVs.}\label{fig:ruvsf3}
\end{figure}

Let's compare PCA results from RUVs and RUVg with the initial raw counts matrix. We will simply run the \texttt{plotPCA()} function on different normalization schemes. The resulting plots are in Figure \ref{fig:ruvcompare}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\KeywordTok{plotPCA}\NormalTok{(countData, }\DataTypeTok{col=}\KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group), }
        \DataTypeTok{main =} \StringTok{'without RUV - raw counts'}\NormalTok{, }\DataTypeTok{adj =} \FloatTok{0.5}\NormalTok{,}
        \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.75}\NormalTok{, }\FloatTok{0.75}\NormalTok{), }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.75}\NormalTok{, }\FloatTok{0.75}\NormalTok{))}
\KeywordTok{plotPCA}\NormalTok{(set_g, }\DataTypeTok{col=}\KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group), }
        \DataTypeTok{main =} \StringTok{'with RUVg'}\NormalTok{, }\DataTypeTok{adj =} \FloatTok{0.5}\NormalTok{,}
        \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.75}\NormalTok{, }\FloatTok{0.75}\NormalTok{), }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.75}\NormalTok{, }\FloatTok{0.75}\NormalTok{))}
\KeywordTok{plotPCA}\NormalTok{(set_s, }\DataTypeTok{col=}\KeywordTok{as.numeric}\NormalTok{(colData}\OperatorTok{$}\NormalTok{group), }
        \DataTypeTok{main =} \StringTok{'with RUVs'}\NormalTok{, }\DataTypeTok{adj =} \FloatTok{0.5}\NormalTok{,}
        \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.75}\NormalTok{, }\FloatTok{0.75}\NormalTok{), }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.75}\NormalTok{, }\FloatTok{0.75}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{08-rna-seq-analysis_files/figure-latex/ruvcompare-1} 

}

\caption{PCA plots to observe the before/after effect of RUV functions.}\label{fig:ruvcompare}
\end{figure}

It looks like \texttt{RUVs()} has performed better than \texttt{RUVg()} in this case. So, let's use count data that is processed by \texttt{RUVs()} to re-do the initial heatmap. The resulting heatmap is in Figure \ref{fig:ruvpost}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(EDASeq)}
\KeywordTok{library}\NormalTok{(pheatmap)}
\CommentTok{# extract normalized counts that are cleared from unwanted variation using RUVs}
\NormalTok{normCountData <-}\StringTok{ }\KeywordTok{normCounts}\NormalTok{(set_s)}
\NormalTok{selectedGenes <-}\StringTok{ }\KeywordTok{names}\NormalTok{(}\KeywordTok{sort}\NormalTok{(}\KeywordTok{apply}\NormalTok{(normCountData, }\DecValTok{1}\NormalTok{, var), }
                            \DataTypeTok{decreasing =} \OtherTok{TRUE}\NormalTok{))[}\DecValTok{1}\OperatorTok{:}\DecValTok{500}\NormalTok{]}
\KeywordTok{pheatmap}\NormalTok{(normCountData[selectedGenes,], }
         \DataTypeTok{annotation_col =}\NormalTok{ colData, }
         \DataTypeTok{show_rownames =} \OtherTok{FALSE}\NormalTok{, }
         \DataTypeTok{cutree_cols =} \DecValTok{2}\NormalTok{,}
         \DataTypeTok{scale =} \StringTok{'row'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{08-rna-seq-analysis_files/figure-latex/ruvpost-1} 

}

\caption{Clustering samples using top 500 most variable genes normalized using RUVs (k = 2).}\label{fig:ruvpost}
\end{figure}

As can be observed the replicates from different groups cluster much better with each other after processing with \texttt{RUVs()}. It is important to note that RUVs uses information from replicates to shift the expression data and it would not work in a confounding design where the replicates of case samples and replicates of the control samples are sequenced in different batches.

\hypertarget{re-run-deseq2-with-the-computed-covariates}{%
\subsubsection{Re-run DESeq2 with the computed covariates}\label{re-run-deseq2-with-the-computed-covariates}}

Having computed the sources of variation using \texttt{RUVs()}, we can actually integrate these variables with \texttt{DESeq2} to re-do the differential expression analysis.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(DESeq2)}
\CommentTok{#set up DESeqDataSet object}
\NormalTok{dds <-}\StringTok{ }\KeywordTok{DESeqDataSetFromMatrix}\NormalTok{(}\DataTypeTok{countData =}\NormalTok{ countData,}
                              \DataTypeTok{colData =}\NormalTok{ colData, }
                              \DataTypeTok{design =} \OperatorTok{~}\StringTok{ }\NormalTok{group)}
\CommentTok{# filter for low count genes}
\NormalTok{dds <-}\StringTok{ }\NormalTok{dds[}\KeywordTok{rowSums}\NormalTok{(DESeq2}\OperatorTok{::}\KeywordTok{counts}\NormalTok{(dds)) }\OperatorTok{>}\StringTok{ }\DecValTok{10}\NormalTok{]}

\CommentTok{# insert the covariates W1 and W2 computed using RUVs into DESeqDataSet object}
\KeywordTok{colData}\NormalTok{(dds) <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{colData}\NormalTok{(dds), }
                      \KeywordTok{pData}\NormalTok{(set_s)[}\KeywordTok{rownames}\NormalTok{(}\KeywordTok{colData}\NormalTok{(dds)), }
                                   \KeywordTok{grep}\NormalTok{(}\StringTok{'W_[0-9]'}\NormalTok{, }
                                        \KeywordTok{colnames}\NormalTok{(}\KeywordTok{pData}\NormalTok{(set_s)))])}

\CommentTok{# update the design formula for the DESeq analysis (save the variable of}
\CommentTok{# interest to the last!)}
\KeywordTok{design}\NormalTok{(dds) <-}\StringTok{ }\ErrorTok{~}\StringTok{ }\NormalTok{W_}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{W_}\DecValTok{2} \OperatorTok{+}\StringTok{ }\NormalTok{group }
\CommentTok{# repeat the analysis }
\NormalTok{dds <-}\StringTok{ }\KeywordTok{DESeq}\NormalTok{(dds)}
\CommentTok{# extract deseq results }
\NormalTok{res <-}\StringTok{ }\KeywordTok{results}\NormalTok{(dds, }\DataTypeTok{contrast =} \KeywordTok{c}\NormalTok{(}\StringTok{'group'}\NormalTok{, }\StringTok{'CASE'}\NormalTok{, }\StringTok{'CTRL'}\NormalTok{))}
\NormalTok{res <-}\StringTok{ }\NormalTok{res[}\KeywordTok{order}\NormalTok{(res}\OperatorTok{$}\NormalTok{padj),]}
\end{Highlighting}
\end{Shaded}

\hypertarget{other-applications-of-rna-seq}{%
\section{Other applications of RNA-seq}\label{other-applications-of-rna-seq}}

RNA-seq generates valuable data that contains information not only at the gene level but also at the level of exons and transcripts. Moreover, the kind of information that we can extract from RNA-seq is not limited to expression quantification. It is possible to detect alternative splicing events such as novel isoforms \citep{trapnell_transcript_2010}, and differential usage of exons \citep{anders_detecting_2012}. It is also possible to observe sequence variants (substitutions, insertions, deletions, RNA-editing) that may change the translated protein product \citep{mckenna_genome_2010}. In the context of cancer genomes, gene-fusion events can be detected with RNA-seq \citep{mcpherson_defuse:_2011}. Finally, for the purposes of gene prediction or improving existing gene predictions, RNA-seq is a valuable method \citep{stanke_augustus:_2005}. In order to learn more about how to implement these, it is recommended that you go through the tutorials of the cited tools.

\hypertarget{exercises-6}{%
\section{Exercises}\label{exercises-6}}

\hypertarget{exploring-the-count-tables}{%
\subsection{Exploring the count tables}\label{exploring-the-count-tables}}

Here, import an example count table and do some exploration of the expression data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{counts_file <-}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata/rna-seq/SRP029880.raw_counts.tsv"}\NormalTok{,}
                           \DataTypeTok{package =} \StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{coldata_file <-}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata/rna-seq/SRP029880.colData.tsv"}\NormalTok{, }
                            \DataTypeTok{package =} \StringTok{"compGenomRData"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Normalize the counts using the TPM approach. {[}Difficulty: \textbf{Beginner}{]}
\item
  Plot a heatmap of the top 500 most variable genes. Compare with the heatmap obtained using the 100 most variable genes. {[}Difficulty: \textbf{Beginner}{]}
\item
  Re-do the heatmaps setting the \texttt{scale} argument to \texttt{none}, and \texttt{column}. Compare the results with \texttt{scale\ =\ \textquotesingle{}row\textquotesingle{}}. {[}Difficulty: \textbf{Beginner}{]}
\item
  Draw a correlation plot for the samples depicting the sample differences as `ellipses', drawing only the upper end of the matrix, and order samples by hierarchical clustering results based on \texttt{average} linkage clustering method. {[}Difficulty: \textbf{Beginner}{]}
\item
  How else could the count matrix be subsetted to obtain quick and accurate clusters? Try selecting the top 100 genes that have the highest total expression in all samples and re-draw the cluster heatmaps and PCA plots. {[}Difficulty: \textbf{Intermediate}{]}
\item
  Add an additional column to the annotation data.frame object to annotate the samples and use the updated annotation data.frame to plot the heatmaps. (Hint: Assign different batch values to CASE and CTRL samples). Make a PCA plot and color samples by the added variable (e.g.~batch). {[}Difficulty: Intermediate{]}
\item
  Try making the heatmaps using all the genes in the count table, rather than sub-selecting. {[}Difficulty: \textbf{Advanced}{]}
\item
  Use the \href{https://cran.r-project.org/web/packages/Rtsne/Rtsne.pdf}{\texttt{Rtsne} package} to draw a t-SNE plot of the expression values. Color the points by sample group. Compare the results with the PCA plots. {[}Difficulty: \textbf{Advanced}{]}
\end{enumerate}

\hypertarget{differential-expression-analysis-1}{%
\subsection{Differential expression analysis}\label{differential-expression-analysis-1}}

Firstly, carry out a differential expression analysis starting from raw counts

Use the following datasets:

\begin{verbatim}
counts_file <- system.file("extdata/rna-seq/SRP029880.raw_counts.tsv", 
                            package = "compGenomRData")
coldata_file <- system.file("extdata/rna-seq/SRP029880.colData.tsv", 
                            package = "compGenomRData")
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Import the read counts and colData tables.
\item
  Set up a DESeqDataSet object.
\item
  Filter out genes with low counts.
\item
  Run DESeq2 contrasting the \texttt{CASE} sample with \texttt{CONTROL} samples.
\end{itemize}

Now, you are ready to do the following exercises:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Make a volcano plot using the differential expression analysis results. (Hint: x-axis denotes the log2FoldChange and the y-axis represents the -log10(pvalue)). {[}Difficulty: \textbf{Beginner}{]}\\
\item
  Use DESeq2::plotDispEsts to make a dispersion plot and find out the meaning of this plot. (Hint: Type ?DESeq2::plotDispEsts) {[}Difficulty: \textbf{Beginner}{]}
\item
  Explore \texttt{lfcThreshold} argument of the \texttt{DESeq2::results} function. What is its default value? What does it mean to change the default value to, for instance, \texttt{1}? {[}Difficulty: \textbf{Intermediate}{]}
\item
  What is independent filtering? What happens if we don't use it? Google \texttt{independent\ filtering\ statquest} and watch the online video about independent filtering. {[}Difficulty: \textbf{Intermediate}{]}
\item
  Re-do the differential expression analysis using the \texttt{edgeR} package. Find out how much DESeq2 and edgeR agree on the list of differentially expressed genes. {[}Difficulty: \textbf{Advanced}{]}
\item
  Use the \texttt{compcodeR} package to run the differential expression analysis using at least three different tools and compare and contrast the results following the \texttt{compcodeR} vignette. {[}Difficulty: \textbf{Advanced}{]}
\end{enumerate}

\hypertarget{functional-enrichment-analysis-1}{%
\subsection{Functional enrichment analysis}\label{functional-enrichment-analysis-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Re-run gProfileR, this time using pathway annotations such as KEGG, REACTOME, and protein complex databases such as CORUM, in addition to the GO terms. Sort the resulting tables by columns \texttt{precision} and/or \texttt{recall}. How do the top GO terms change when sorted for \texttt{precision}, \texttt{recall}, or \texttt{p.value}? {[}Difficulty: \textbf{Beginner}{]}
\item
  Repeat the gene set enrichment analysis by trying different options for the \texttt{compare} argument of the \texttt{GAGE:gage}
  function. How do the results differ? {[}Difficulty: \textbf{Beginner}{]}
\item
  Make a scatter plot of GO term sizes and obtained p-values by setting the \texttt{gProfiler::gprofiler} argument \texttt{significant\ =\ FALSE}. Is there a correlation of term sizes and p-values? (Hint: Take -log10 of p-values). If so, how can this bias be mitigated? {[}Difficulty: \textbf{Intermediate}{]}
\item
  Do a gene-set enrichment analysis using gene sets from top 10 GO terms. {[}Difficulty: \textbf{Intermediate}{]}
\item
  What are the other available R packages that can carry out gene set enrichment analysis for RNA-seq datasets? {[}Difficulty: \textbf{Intermediate}{]}
\item
  Use the topGO package (\url{https://bioconductor.org/packages/release/bioc/html/topGO.html}) to re-do the GO term analysis. Compare and contrast the results with what has been obtained using the \texttt{gProfileR} package. Which tool is faster, \texttt{gProfileR} or topGO? Why? {[}Difficulty: \textbf{Advanced}{]}
\item
  Given a gene set annotated for human, how can it be utilized to work on \emph{C. elegans} data? (Hint: See \texttt{biomaRt::getLDS}). {[}Difficulty: \textbf{Advanced}{]}
\item
  Import curated pathway gene sets with Entrez identifiers from the \href{http://software.broadinstitute.org/gsea/msigdb/collections.jsp}{MSIGDB database} and re-do the GSEA for all curated gene sets. {[}Difficulty: \textbf{Advanced}{]}
\end{enumerate}

\hypertarget{removing-unwanted-variation-from-the-expression-data}{%
\subsection{Removing unwanted variation from the expression data}\label{removing-unwanted-variation-from-the-expression-data}}

For the exercises below, use the datasets at:

\begin{verbatim}
counts_file <- system.file('extdata/rna-seq/SRP049988.raw_counts.tsv', 
                           package = 'compGenomRData')
colData_file <- system.file('extdata/rna-seq/SRP049988.colData.tsv', 
                           package = 'compGenomRData')
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run RUVSeq using multiple values of \texttt{k} from 1 to 10 and compare and contrast the PCA plots obtained from the normalized counts of each RUVSeq run. {[}Difficulty: \textbf{Beginner}{]}
\item
  Re-run RUVSeq using the \texttt{RUVr()} function. Compare PCA plots from \texttt{RUVs}, \texttt{RUVg} and \texttt{RUVr} using the same \texttt{k} values and find out which one performs the best. {[}Difficulty: \textbf{Intermediate}{]}
\item
  Do the necessary diagnostic plots using the differential expression results from the EHF count table. {[}Difficulty: \textbf{Intermediate}{]}
\item
  Use the \texttt{sva} package to discover sources of unwanted variation and re-do the differential expression analysis using variables from the output of \texttt{sva} and compare the results with \texttt{DESeq2} results using \texttt{RUVSeq} corrected normalization counts. {[}Difficulty: \textbf{Advanced}{]}
\end{enumerate}

\hypertarget{chipseq}{%
\chapter{ChIP-seq analysis}\label{chipseq}}

\emph{Chapter Author}: \textbf{Vedran Franke}

Protein-DNA interactions are responsible for a large part of the gene expression regulation. Proteins such as transcription factors as well as histones are directly related to how much and in which contexts the genes are expressed. Some of these concepts are already introduced in Chapter \ref{intro} if readers need a more in-depth introduction. In this chapter, we will introduce how to process and analyze ChIP-seq data in order to identify genome-wide protein binding sites and to discover underlying sequence context via transcription factor binding-site motifs.

\hypertarget{regulatory-protein-dna-interactions}{%
\section{Regulatory protein-DNA interactions}\label{regulatory-protein-dna-interactions}}

One of the most fascinating biological phenomena is the fact that a myriad of different cell types, in a multicellular organism, are encoded by one single genome. How exactly this is achieved is still a
major unanswered question in biology.
Cell types differ based on a multitude of features:
their size, shape, mobility, surface receptors, metabolic content.
However, the main predominant feature, which influences all of the above, is which
genes are expressed in each cell type.
Therefore, if we can understand what controls which
genes will be expressed, and where they will be expressed,
we can start forming a picture of how a single genomic template,
can give rise to a complex organism.

As explained in Chapter \ref{intro}, gene expression is controlled by a special class of genes called
transcription factors - genes which control other genes.
Transcription factor genes encode proteins which
can bind to the DNA, and control whether a certain part of DNA will be
transcribed (expressed), or stay silent (repressed).
They program the expression patterns in each cell.
Transcription factors contain DNA binding domains, which are specifically folded protein sequences
which recognize specific DNA motifs (a short nucleotide sequence).
Such sequence binding imparts transcription factors with specificity,
transcription factors do not bind everywhere on the DNA, rather they are localized to
short stretches which contain the corresponding DNA motif.

DNA in the nucleus is wrapped around a protein complex called the histone complex.
Histones form a chain of beads along the DNA. By changing their position, histones can make
certain parts of the DNA more or less accessible to transcription
factors. Histone complexes can be chemically modified with different post-translational modifications (see Chapter \ref{intro}). Such modifications change histone
mobility, and their interactions with different proteins, thereby creating
an additional regulatory layer on top of the DNA sequence.

In order to understand the target genes of a certain transcription factor,
and how they control the gene expression, we need to know where on the DNA the
transcription factor is located.

\hypertarget{measuring-protein-dna-interactions-with-chip-seq}{%
\section{Measuring protein-DNA interactions with ChIP-seq}\label{measuring-protein-dna-interactions-with-chip-seq}}

ChIP-seq stands for chromatin immunoprecipitation followed by sequencing, and is an experimental method for finding locations on DNA which are bound by proteins. It has been extensively used to study
in-vivo binding preferences of transcription factors, and genomic distribution of modified histones.

In the remainder of this chapter, you will learn how to assess quality control
of ChIP-seq data sets, perform peak calling to find bound regions, and
assess the quality of the peak calling.

Once you have obtained peaks, you will learn how to perform sequence analysis
to construct motif models, and compare signals between experiments.
Biological experiments often contain a multitude of consecutive steps. Each
step can profoundly influence the quality of the data, and the subsequent analysis.
The computational biologist has to have an in-depth knowledge of the experimental
design, and the underlying experimental steps, in order to choose the proper tools
and the type of analysis, which will give proper and correct results \citep{kharchenko_2008, kidder_2011, landt_2012, chen_2012, felsani_2015}.
In this chapter we will go through the main experimental steps in the
ChIP-seq analysis and address the most common experimental pitfalls.

The main principle of the method is to use a specific antibody to enrich
DNA fragments which are bound by the protein of interest.

The DNA fragments are then sequenced, mapped onto the corresponding
reference genome, and computationally analyzed to distinguish regions which
were really bound by the protein, from the background regions.

The experimental methodology is depicted in
Figure \ref{fig:ChIP-seq-Protocol-plot}, and consists of the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Cross linking of cells with formaldehyde to bind the proteins to the DNA.
  This process covalently links the proteins to the DNA.
\item
  Fragmentation of DNA using sonication or enzymatic digestion, shearing
  of DNA into small fragments (ranging from 50 - 500 bp).
\item
  Immunoprecipitation using a specific antibody. An immunoprecipitation step
  which enriches fragments bound by the protein.
\item
  Cross-link reversal. Frees the DNA fragments for further processing.
\item
  Size selection of DNA fragments. Only fragments of certain length are used in the
  library preparation and sequencing.
\item
  Fragment amplification using PCR. The amount of DNA is a limiting step for the
  protocol. Therefore the fragments need to be amplified using PCR.
\item
  DNA fragment sequencing
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{./Figures/Chip-Seq_Protocol_Extended} 

}

\caption{Main experimental steps in the ChIP-seq protocol.}\label{fig:ChIP-seq-Protocol-plot}
\end{figure}

After sequencing, the role of the computational biologist is to assess the
quality of the experiment, find the location of the protein of interest, and
finally, to integrate with existing data sets.

Each step of the experimental protocol can affect the quality
of the data set, and the subsequent analysis steps. It is, therefore, of crucial
importance to perform quality control for every sequenced experiment.

\hypertarget{factors-that-affect-chip-seq-experiment-and-analysis-quality}{%
\section{Factors that affect ChIP-seq experiment and analysis quality}\label{factors-that-affect-chip-seq-experiment-and-analysis-quality}}

\hypertarget{antibody-specificity}{%
\subsection{Antibody specificity}\label{antibody-specificity}}

Antibody specificity is a term which refers to how strongly an antibody
binds to its preferred target, with respect to everything else present in the cell.
It is the paramount measure influencing the successful execution of a ChIP
experiment.
Antibodies can bind multiple proteins with the same affinity.
This is called antibody cross-reactivity. If an antibody cross-reacts with
multiple proteins, the results of a ChIP experiment will be ambiguous.
Instead of finding where our protein binds to the DNA, we will get a
superposition of binding of multiple proteins.
Such data are impossible to analyze correctly, and will produce false conclusions.
There are many experimental procedures for validating antibody specificities, and
an antibody should pass multiple tests in order to be considered valid.
The exact recommendations are listed by the ENCODE consortium \citep{landt_2012}.

Every time we are analyzing a new ChIP-seq experiment, we have to take our time
to convince ourselves that all of the appropriate experimental controls were performed
to validate the antibody specificity \citep{Wardle_2015}.

\hypertarget{sequencing-depth}{%
\subsection{Sequencing depth}\label{sequencing-depth}}

Variation in sequencing depth is the first systematic technical bias we
encounter in ChIP-seq experiments. Namely, different samples will contain different number of
sequenced reads. Different sequencing depth influences our ability to detect
enriched regions, and complicates comparisons between samples \citep{jung_2014}.
The statistical procedure of removing the influence of sequencing depth on the
quantification is called depth scaling; we calculate a scaling factor which
is used to multiply the signal strength before the comparison.
There are multiple methods for normalization, and each method comes with its assumptions.
\textbf{Scale normalization} is done by dividing the read counts (in certain genomic locations)
by the total amount of sequenced reads. This method presumes that the ChIP
efficiency worked equally well in all studied conditions. Because the ChIP efficiency
differs in different antibodies, it is often unsuitable for comparisons of ChIP-seq
experiments done on different proteins.
\textbf{Robust normalization} tries to locate genomic regions which do not change between
different biological conditions (regions where the protein is constantly bound),
and then uses the sum of the reads in those regions as the scaling factor. This
method presumes that we can reliably identify regions which do not change \citep{shao_2012}.
\textbf{Background normalization} presumes that the genome can be split into two categories:
background regions and true signal regions. It then uses the number of reads in the
background regions to define the scaling factor \citep{liang_2012}.
\textbf{External normalization} uses external reference for normalization; we
add known amounts of chromatin from a distant species, or artificial spike-ins which are then
used as a scaling reference. This is used when we think there are global changes
in the biding profiles between two biological conditions -- very large changes in the
signal profile \citep{bonhoure_2014}.

The choice of normalization method depends on the type of analysis \citep{angelini_2015};
if we want to quantitatively compare the abundance of different histone marks in
different cell types, we will need the different normalization procedure than if
we want to compare TF binding in the same setting.

\hypertarget{pcr-duplication}{%
\subsection{PCR duplication}\label{pcr-duplication}}

The amounts of the DNA obtained after the ChIP experiment are quite often lower
than the minimal amount which can be sequenced.
Polymerase chain reaction (PCR) is a procedure used for amplification of DNA fragments.
It is used to increase the amount of DNA in our sample prior to sequencing.
PCR is a stochastic procedure, meaning that the results of each PCR
reaction cannot be predicted. Due to its stochastic nature, PCR can
be a significant source of variability in the ChIP-seq experiments
\citep{aird_2011, benjamini_2012, teng_2016}. A quality control is necessary to
check whether all of our samples have the same sequence properties, i.e.~the
same enrichment of dinucleotides, such as CpG.
If the samples differ in their sequence properties, that means we have
to account for them during the analysis \citep{Teng_2017}.

\hypertarget{biological-replicates}{%
\subsection{Biological replicates}\label{biological-replicates}}

Biological replicates are independently executed ChIP-seq experiments from different
samples, corresponding to the same biological conditions. They are indispensable
for estimating ChIP quality, and give us an estimate of the variability in the
experiment which we can expect due to unknown biological variables.
Without biological replicates, it is statistically impossible to compare
ChIP-seq samples from different biological conditions, because we do not know
whether the observed changes are a result of the inherent biological variability
(the source of which we do not understand), or they result from the change
in the biological condition (different tissue or transcription factor used
in the experiment).
If we encounter an experimental setup which does not include biological
replicates, we should be extremely skeptical about all conclusions derived
from such analysis.

\hypertarget{control-experiments}{%
\subsection{\texorpdfstring{Control experiments\index{Peak calling}}{Control experiments}}\label{control-experiments}}

There are three types of control experiments which can be performed to control
for known and unknown experimental biases:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Input control}: Sequencing of genomic DNA without the immunoprecipitation step.
\item
  \textbf{IgG control}: Using a polyclonal mixture of non-specific IgG antibodies instead of a specific antibody.
\item
  \textbf{Knockout control}: Performing the ChIP experiment in a biological system which
  does not contains our protein of interest (i.e.~in a cell line where the transcription
  factor was knocked out) \citep{krebs_2014}.
\end{enumerate}

Each type of control experiment controls for a certain set of experimental biases.

\textbf{Input control} is the most frequent type of control performed.
It shows the differential susceptibility of genomic regions to the ChIP-procedure.
Due to the hierarchical structure of chromatin, different genomic regions have different propensities for cross-linking,
sonication, and immunoprecipitation. This causes an uneven probability of
observing DNA fragments originating from different genomic regions.
Because different cell types (cell lines, and cancer cell lines),
have different chromatin structure, ChIP samples will show a cell-type-specific
bias in observed enrichment profiles.
An important note to consider is that the input control is basically a
reduced whole genome sequencing experiment, while the ChIP enriches for only a subset
of genomic regions. If both ChIP and Input samples are sequenced to the same
depth (same number of reads), the background distribution in the input sample will
be under sampled. It is recommended to sequence the input sample deeper than the ChIP sample \citep{chen2012systematic}.

\textbf{IgG control} uses a soup of nonspecific antibodies to control for background
binding. In principle, the antibodies should be isolated from the same batch of
serum which was used to create the specific antibody (used for ChIP). It should,
in theory, give a background profile of non-specific binding.
The proper control, is however, seldom available. Additionally, because the antibodies
are unspecific, the amount of precipitated DNA will be low, and the samples
will require additional rounds of PCR amplification.

\textbf{KO control} is a ChIP experiment performed in the biological system where
the native protein is not present. Such an experiment profiles the non-specific
binding of the antibody to other proteins, and directly to the DNA.
The primary, and only, concern is that the perturbation caused by the knock-out (or knock-down),
changes the cell so much, that the ChIP profile is not comparable to the original cell.
This is the most accurate type of control experiment, however, it is frequently technically challenging
to perform if the cells are not viable after the knock-out, or
if the knock-out is impossible to perform.

\hypertarget{using-tagged-proteins}{%
\subsection{Using tagged proteins}\label{using-tagged-proteins}}

If an antibody of sufficient quality is not available, it is possible
to resort to constructs where the protein of interest gets engineered
with a ChIP-able tag. The proper control for such experiments is to perform
the ChIP in the cell line containing the engineered protein, and without the
protein.
It must be noted that the tagging procedure can change the binding preferences
of the protein, and therefore the experimental conclusions.

\hypertarget{pre-processing-chip-data}{%
\section{Pre-processing ChIP data}\label{pre-processing-chip-data}}

The focus of ChIP preprocessing is to check the quality of the sequencing
experiment, remove sequencing artifacts, and find the genomic location of
sequenced fragments using read mapping.
The quality control consists of read quality control and adapter trimming.
These methods are described in depth in Chapter \ref{processingReads}.

\hypertarget{mapping-of-chip-seq-data}{%
\subsection{Mapping of ChIP-seq data}\label{mapping-of-chip-seq-data}}

Mapping is a procedure of trying to locate the exact genomic location which
created each genomic fragment, each sequenced read.
Several tools are available for mapping ChIP-seq data sets:
Bowtie, Bowtie2, BWA \citep{langmead_2009, langmead_2012, li_2009}, and all of them have comparable sensitivity and
specificity \citep{ruffalo_2011}.
Read length is the variable with the biggest effect on the mapping procedure.
The longer the sequenced reads, the more uniquely can the read be assigned
to a position on the genome. Reads which are assigned ambiguously to multiple
locations in the genome are called multi-mapping reads. Such fragments
are most often produced by repetitive genomic regions, such as retrotransposons,
pseudogenes or paralogous genes \citep{li_2014}.
It is important to, a priori, decide whether such duplicated regions are of
interest for the current experimental setup (i.e.~whether we want to study transcription factor binding
in olfactory receptors). If they are, then the multi-mapping
reads should be included in the analysis. If they are not, they should be omitted.
This is done during the mapping step, by limiting the number of locations to which a read can map.
The methodology of working with multi-mapping reads differs according to the
use case, and will not be considered in this chapter. For more information, please
see the references \citep{chung_2011}.

Current Illumina sequencing procedures enable sequencing of DNA
fragments from just one, or both ends.
Sequencing from both ends is called \textbf{paired-end} sequencing and greatly enhances
the sample \textbf{mappability}, the percentage of genome which can be uniquely mapped.
Additionally, it provides an out-of-the-box estimate of the
average DNA fragment length, a parameter which is important for quality control
and peak calling.
Although it would always be preferable to do paired-end sequencing it
substantially increases the sequencing costs, which can be prohibitive.

Different reads, which map to the same genomic location (same chromosome, position, and strand),
are called \textbf{duplicated reads}. Such reads are an indication that the same
DNA fragment was present multiple times during the library preparation. This
can happen due to high enrichment with highly specific antibodies, or such
fragments can be artificially produced during PCR amplification. Because we do not know
the exact origin of the duplicated fragments, they are most often collapsed during
the peak calling procedure, i.e.~when multiple reads map to the same chromosome,
position, and strand, only one read is used. If the transcription factor
binds to a small number of regions in the genome, such data reduction might be
too stringent, and we can increase the sensitivity by allowing up to \textbf{N} different
reads, per position (i.e.~if more than \textbf{N} reads map to the same location, only
\textbf{N} reads are kept for the downstream analysis).

Some peak calling algorithms have automated statistical methods for determining the number
of reads, per position, which will be used in the analysis \citep{zhang_2008}.

An important consideration to take into account is the genome which was used in
the experiment. Cell lines, cancer samples, and personal genomes usually contain
structural genomic alterations which are not present in the reference genome
(duplications, insertions, and deletions). Such regions can cause false
negatives, and false positives in the ChIP-seq experiment.
If a region was present multiple times in the experimental system, and only a single
time in the reference genome, it will be relatively enriched in the final
sequencing library. Such fragments will pile up on a single location during
the mapping step, and create an artificial peak, which can be falsely characterized
as a binding event.
Such regions are called \textbf{blacklisted} regions and should be removed from the
downstream analysis. The \href{http://genome.ucsc.edu}{UCSC browser database} contains tables with
such regions for the most commonly used model organism species.

This chapter presumes that the user is already familiar with
the following technical and conceptual knowledge in computational data processing.
From Chapters \ref{processingReads} and \ref{genomicIntervals}, you should be familiar with \index{read filtering}
\index{read mapping} the concept of multi-mapping reads,
and the following file formats BED\index{BED file}, GTF\index{GTF file},
WIG\index{WIG file}, bigWig\index{bigWig file}, BAM\index{BAM file}.
You should also be familiar with PCR\index{PCR}, what
are PCR duplicates, positive and negative DNA strands, and technical and biological
replicates.

\hypertarget{chip-quality-control}{%
\section{ChIP quality control}\label{chip-quality-control}}

While the goal of the read quality assessment is to check whether the sequencing
produced a high enough number of high-quality reads
the goal of ChIP quality control is to ascertain whether the chromatin immunoprecipitation
enrichment was successful.
This is a crucial step in the ChIP-seq analysis because it can help us
identify low-quality ChIP samples, and give information about which experimental
steps went wrong.

There are four steps in ChIP quality control:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Sample correlation clustering: Clustering of the pair-wise correlations between
  genome-wide signal profiles.
\item
  Data visualization in a genomic browser.
\item
  Average fragment length determination: Determining whether the ChIP was enriched for fragments of a certain length.
\item
  Visualization of GC bias. Here we will plot the ChIP enrichment versus the
  average GC content in the corresponding genomic bin.
\end{enumerate}

\hypertarget{the-data}{%
\subsection{The data}\label{the-data}}

Here we will familiarize ourselves with the datasets that will be used in the
chapter.
Experimental data was downloaded from the public ENCODE \citep{ENCODE_Project_Consortium2012-wf}
database of ChIP-seq experiments.
The experiments were performed on a lymphoblastoid cell line, GM12878, and mapped
to the GRCh38 (hg38) version of the human genome, using the standard ENCODE
ChIP-seq pipeline. In this chapter, due to compute time considerations, we have taken a subset of the data which corresponds to the human chromosome 21 (chr21).

The data sets are located in the \texttt{compGenomRData}\index{R Packages!\texttt{compGenomRData}} package.
The location of the data sets can be accessed using the \texttt{system.file()} command,
in the following way:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data_path =}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\StringTok{'extdata/chip-seq'}\NormalTok{,}\DataTypeTok{package=}\StringTok{'compGenomRData'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The available datasets can be listed using the \texttt{list.files()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chip_files =}\StringTok{ }\KeywordTok{list.files}\NormalTok{(data_path, }\DataTypeTok{full.names=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The dataset consists of the following ChIP experiments:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Transcription factors}: CTCF\index{CTCF protein}, SMC3, ZNF143, PolII
  (RNA polymerase 2)
\item
  \textbf{Histone modifications}\index{histone modification}: H3K4me3, H3K36me3, H3k27ac, H3k27me3
\item
  Various input samples
\end{enumerate}

\hypertarget{sample-clustering}{%
\subsection{Sample clustering}\label{sample-clustering}}

Clustering is an ordering procedure which groups samples by similarity;
the more similar samples are grouped closer to one another.
The details of clustering methodologies are described in Chapter \ref{unsupervisedLearning}.
Clustering of ChIP signal profiles is used for two purposes:
The first one is to ascertain whether there is concordance between
biological replicates; biological replicates should show greater similarity
than ChIP of different proteins. The second function is to see whether our experiments conform to known prior knowledge. For example, we would expect to see greater similarity between proteins
which belong to the same protein complex.

To quantify the ChIP signal we will firstly construct 1-kilobase-wide tilling
windows over the genome, and subsequently count the number of reads
in each window, for each experiment. We will then normalize the counts, to
account for a different total number of reads in each experiment, and finally
calculate the correlation between all pairs of samples.
Although this procedure represents a crude way of data quantification, it provides sufficient
information to ascertain the data quality.

Using the \texttt{GenomeInfoDb} we will first fetch the chromosome lengths corresponding
to the hg38 version of the human genome, and filter the length for human
chromosome 21.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the chromosome info package}
\KeywordTok{library}\NormalTok{(GenomeInfoDb)}

\CommentTok{# fetch the chromosome lengths for the human genome}
\NormalTok{hg_chrs =}\StringTok{ }\KeywordTok{getChromInfoFromUCSC}\NormalTok{(}\StringTok{'hg38'}\NormalTok{)}

\CommentTok{# find the length of chromosome 21}
\NormalTok{hg_chrs =}\StringTok{ }\KeywordTok{subset}\NormalTok{(hg_chrs, }\KeywordTok{grepl}\NormalTok{(}\StringTok{'chr21$'}\NormalTok{,chrom))}
\end{Highlighting}
\end{Shaded}

The \texttt{tileGenome()} function from the \texttt{GenomicRanges} package constructs equally sized
windows over the genome of interest.
The function takes two arguments:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A vector of chromosome lengths
\item
  Window size
\end{enumerate}

Firstly, we convert the chromosome lengths \emph{data.frame} into a \emph{named vector}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# downloaded hg_chrs is a data.frame object,}
\CommentTok{# we need to convert the data.frame into a named vector}
\NormalTok{seqlengths =}\StringTok{ }\KeywordTok{with}\NormalTok{(hg_chrs, }\KeywordTok{setNames}\NormalTok{(size, chrom))}
\end{Highlighting}
\end{Shaded}

Then we construct the windows.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the genomic ranges package}
\KeywordTok{library}\NormalTok{(GenomicRanges)}

\CommentTok{# tileGenome function returns a list of GRanges of a given width, }
\CommentTok{# spanning the whole chromosome}
\NormalTok{tilling_window =}\StringTok{ }\KeywordTok{tileGenome}\NormalTok{(seqlengths, }\DataTypeTok{tilewidth=}\DecValTok{1000}\NormalTok{)}

\CommentTok{# unlist converts the list to one GRanges object}
\NormalTok{tilling_window =}\StringTok{ }\KeywordTok{unlist}\NormalTok{(tilling_window)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## GRanges object with 46710 ranges and 0 metadata columns:
##           seqnames            ranges strand
##              <Rle>         <IRanges>  <Rle>
##       [1]    chr21            1-1000      *
##       [2]    chr21         1001-2000      *
##       [3]    chr21         2001-3000      *
##       [4]    chr21         3001-4000      *
##       [5]    chr21         4001-5000      *
##       ...      ...               ...    ...
##   [46706]    chr21 46704985-46705984      *
##   [46707]    chr21 46705985-46706984      *
##   [46708]    chr21 46706985-46707984      *
##   [46709]    chr21 46707985-46708984      *
##   [46710]    chr21 46708985-46709983      *
##   -------
##   seqinfo: 1 sequence from an unspecified genome
\end{verbatim}

We will use the \texttt{summarizeOverlaps()} function from the \texttt{GenomicAlignments} package
to count the number of reads in each genomic window.
The function will do the counting automatically for all our experiments.
The \texttt{summarizeOverlaps()} function returns a \texttt{SummarizedExperiment} object.
The object contains the counts, genomic ranges which were used for the quantification,
and the sample descriptions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load GenomicAlignments}
\KeywordTok{library}\NormalTok{(GenomicAlignments)}

\CommentTok{# fetch bam files from the data folder}
\NormalTok{bam_files =}\StringTok{ }\KeywordTok{list.files}\NormalTok{(}
    \DataTypeTok{path       =}\NormalTok{ data_path, }
    \DataTypeTok{full.names =} \OtherTok{TRUE}\NormalTok{, }
    \DataTypeTok{pattern    =} \StringTok{'bam$'}
\NormalTok{)}

\CommentTok{# use summarizeOverlaps to count the reads}
\NormalTok{so =}\StringTok{ }\KeywordTok{summarizeOverlaps}\NormalTok{(tilling_window, bam_files)}

\CommentTok{# extract the counts from the SummarizedExperiment}
\NormalTok{counts =}\StringTok{ }\KeywordTok{assays}\NormalTok{(so)[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

Different ChIP experiments were sequenced to different depths; each experiment
contains a different number of reads. To remove the effect of the experimental
depth on the quantification, the samples need to be normalized.
The standard normalization procedure, for ChIP data, is to divide the
counts in each tilling window by the total number of sequenced reads, and
multiply it by a constant factor (to avoid extremely small numbers).
This normalization procedure is called the \textbf{cpm}\index{cpm} - counts per million.

\[
CPM = counts * (10^{6} / total\>number\>of\>reads)
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate the cpm from the counts matrix}
\CommentTok{# the following command works because }
\CommentTok{# R calculates everything by columns}
\NormalTok{cpm =}\StringTok{ }\KeywordTok{t}\NormalTok{(}\KeywordTok{t}\NormalTok{(counts)}\OperatorTok{*}\NormalTok{(}\DecValTok{1000000}\OperatorTok{/}\KeywordTok{colSums}\NormalTok{(counts)))}
\end{Highlighting}
\end{Shaded}

We remove all tiles which do not have overlapping reads.
Tiles with 0 counts do not provide any additional discriminatory power, rather,
they introduce artificial similarity between the samples (i.e.~samples with
only a handful of bound regions will have a lot of tiles with \(0\) counts, while
they do not have to have any overlapping enriched tiles).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# remove all tiles which do not contain reads}
\NormalTok{cpm =}\StringTok{ }\NormalTok{cpm[}\KeywordTok{rowSums}\NormalTok{(cpm) }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

We use the \texttt{sub()} function to shorten the column names of the cpm matrix.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# change the formatting of the column names}
\CommentTok{# remove the .chr21.bam suffix}
\KeywordTok{colnames}\NormalTok{(cpm) =}\StringTok{ }\KeywordTok{sub}\NormalTok{(}\StringTok{'.chr21.bam'}\NormalTok{,}\StringTok{''}\NormalTok{,   }\KeywordTok{colnames}\NormalTok{(cpm))}

\CommentTok{# remove the GM12878_hg38 prefix}
\KeywordTok{colnames}\NormalTok{(cpm) =}\StringTok{ }\KeywordTok{sub}\NormalTok{(}\StringTok{'GM12878_hg38_'}\NormalTok{,}\StringTok{''}\NormalTok{,}\KeywordTok{colnames}\NormalTok{(cpm))}
\end{Highlighting}
\end{Shaded}

Finally, we calculate the pairwise Pearson correlation coefficient using the
\texttt{cor()} function.
The function takes as input a region-by-sample count matrix, and returns
a sample X sample matrix, where each field contains the correlation coefficient
between two samples.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculates the pearson correlation coefficient between the samples}
\NormalTok{correlation_matrix =}\StringTok{ }\KeywordTok{cor}\NormalTok{(cpm, }\DataTypeTok{method=}\StringTok{'pearson'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{Heatmap()} function from the \texttt{ComplexHeatmap} \citep{Gu_2016} package is used to visualize
the correlation coefficient.\index{R Packages!\texttt{ComplexHeatmap}}
The function automatically performs hierarchical clustering - it groups the
samples which have the highest pairwise correlation.
The diagonal represents the correlation of each sample with itself.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load ComplexHeatmap}
\KeywordTok{library}\NormalTok{(ComplexHeatmap)}

\CommentTok{# load the circlize package, and define }
\CommentTok{# the color palette which will be used in the heatmap}
\KeywordTok{library}\NormalTok{(circlize)}
\NormalTok{heatmap_col =}\StringTok{ }\NormalTok{circlize}\OperatorTok{::}\KeywordTok{colorRamp2}\NormalTok{(}
    \DataTypeTok{breaks =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
    \DataTypeTok{colors =} \KeywordTok{c}\NormalTok{(}\StringTok{'blue'}\NormalTok{,}\StringTok{'white'}\NormalTok{,}\StringTok{'red'}\NormalTok{)}
\NormalTok{)}

\CommentTok{# plot the heatmap using the Heatmap function}
\KeywordTok{Heatmap}\NormalTok{(}
    \DataTypeTok{matrix =}\NormalTok{ correlation_matrix, }
    \DataTypeTok{col    =}\NormalTok{ heatmap_col}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{09-chip-seq-analysis_files/figure-latex/sample-clustering-complex-heatmap-1} 

}

\caption{Heatmap showing ChIP-seq sample similarity using the Pearson correlation coefficient.}\label{fig:sample-clustering-complex-heatmap}
\end{figure}

In Figure \ref{fig:sample-clustering-complex-heatmap} we can see a
perfect example of why quality control is important.
\textbf{CTCF} is a zinc finger protein which co-localizes with the Cohesin complex.
\textbf{SMC3} is a sub unit of the Cohesin complex, and we would therefore expect to
see that the \textbf{SMC3} signal profile has high correlation with the \textbf{CTCF} signal profile.
This is true for the second biological replicate of \textbf{SMC3}, while the first
replicate (SMC3\_r1) clusters with the input samples. This indicates that the
sample likely has low enrichment.
We can see that the ChIP and Input samples form separate clusters. This implies
that the ChIP samples have an enrichment of fragments.
Additionally, we see that the biological replicates of other experiments
cluster together.

\hypertarget{visualization-in-the-genome-browser}{%
\subsection{Visualization in the genome browser}\label{visualization-in-the-genome-browser}}

One of the first steps in any ChIP-seq analysis should be looking at the
data. By looking at the data we get an intuition about the quality of the
experiment, and start seeing preliminary correlations between the samples, which
we can use to guide our analysis.
This can be achieved either by plotting signal profiles around
regions of interest, or by loading data into a genome browser
(such as IGV\index{IGV Browser}, or UCSC genome browsers\index{UCSC Genome Browser}).

Genome browsers are standalone applications which represent the genome
as a one-dimensional (1D) coordinate system. The browsers enable
simultaneous visualization and comparison of multiple types of annotations and experimental data.

Genome browsers can visualize most of the commonly used genomic data formats:
BAM\index{BAM file}, BED\index{BED file}, wig\index{wig file}, and bigWig\index{bigWig file}.
The easiest way to access our data would be to load the .bam files into the browser. This will show us the sequence and position of every mapped read. If we want to view multiple samples in parallel, loading every mapped read can be restrictive. It takes up a lot of computational resources, and the amount of information
makes the visual comparison hard to do.
We would like to convert our data so that we get a compressed visualization,
which would show us the main properties of our samples, namely, the quality and
the location of the enrichment.
This is achieved by summarizing the read enrichment into a signal profile -
the whole experiment is converted into a numeric vector - a coverage vector.
The vector contains information on how many reads overlap each position
in the genome.

We will proceed as follows: Firstly, we will import a \textbf{.bam} file into \textbf{R}. Then we will calculate the signal profile (construct the coverage vector), and finally, we export the vector as a \textbf{.bigWig} file.

First we select one of the ChIP samples.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# list the bam files in the directory}
\CommentTok{# the '$' sign tells the pattern recognizer to omit bam.bai files}
\NormalTok{bam_files =}\StringTok{ }\KeywordTok{list.files}\NormalTok{(}
    \DataTypeTok{path       =}\NormalTok{ data_path, }
    \DataTypeTok{full.names =} \OtherTok{TRUE}\NormalTok{, }
    \DataTypeTok{pattern    =} \StringTok{'bam$'}
\NormalTok{)}

\CommentTok{# select the first bam file}
\NormalTok{chip_file =}\StringTok{ }\NormalTok{bam_files[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

We will use the \texttt{readGAlignments()} function from the \texttt{GenomicAlignments}
package to load the reads into \textbf{R}, and then the \texttt{GRanges()} function
to convert them into a \texttt{GRanges} object.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the genomic alignments package}
\KeywordTok{library}\NormalTok{(GenomicAlignments)}

\CommentTok{# read the ChIP reads into R}
\NormalTok{reads =}\StringTok{ }\KeywordTok{readGAlignments}\NormalTok{(chip_file)}

\CommentTok{# the reads need to be converted to a granges object}
\NormalTok{reads =}\StringTok{ }\KeywordTok{granges}\NormalTok{(reads)}
\end{Highlighting}
\end{Shaded}

Because DNA fragments are being sequenced from their ends (both the 3' and 5' end),
the read enrichment does not correspond to the exact location of the bound protein.
Rather, reads end to form clusters of enrichment upstream and downstream of the true binding location.
To correct for this, we use a small hack. Before we create the signal profiles,
we will extend the reads towards their \textbf{3'} end. The reads are extended to
form fragments of 200 base pairs. This is an empiric measure, which
corresponds to the average fragment size of the Illumina sample preparation kit.
The exact average fragment size will differ from 200 base pairs, but if the
deviation is not large (i.e.~more than 200 base pairs),
it will not affect the visual properties of our samples.

Read extension is done using the \texttt{resize()} function. The function
takes two arguments:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{width}: resulting fragment width
\item
  \texttt{fix}: which position of the fragment should not be changed (if \texttt{fix} is set to start,
  the reads will be extended towards the \textbf{3'} end. If \texttt{fix} is set to end, they will
  be extended towards the \textbf{5'} end)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# extends the reads towards the 3' end}
\NormalTok{reads =}\StringTok{ }\KeywordTok{resize}\NormalTok{(reads, }\DataTypeTok{width=}\DecValTok{200}\NormalTok{, }\DataTypeTok{fix=}\StringTok{'start'}\NormalTok{)}

\CommentTok{# keeps only chromosome 21}
\NormalTok{reads =}\StringTok{ }\KeywordTok{keepSeqlevels}\NormalTok{(reads, }\StringTok{'chr21'}\NormalTok{, }\DataTypeTok{pruning.mode=}\StringTok{'coarse'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Conversion of reads into coverage vectors is done with the \texttt{coverage()}
function.
The function takes only one argument (\texttt{width}), which corresponds to chromosome sizes.
For this purpose we can use the, previously created, \texttt{seqlengths} variable.
The \texttt{coverage()} function converts the reads into a compressed \texttt{Rle} object. We have introduced these workflows in Chapter \ref{genomicIntervals}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert the reads into a signal profile}
\NormalTok{cov =}\StringTok{ }\KeywordTok{coverage}\NormalTok{(reads, }\DataTypeTok{width =}\NormalTok{ seqlengths)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## RleList of length 1
## $chr21
## integer-Rle of length 46709983 with 199419 runs
##   Lengths: 5038228     200   63546      20 ...     200    1203     200   27856
##   Values :       0       1       0       1 ...       1       0       1       0
\end{verbatim}

The name of the output file is created by changing the file suffix from \textbf{.bam}
to \textbf{.bigWig}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# change the file extension from .bam to .bigWig}
\NormalTok{output_file =}\StringTok{ }\KeywordTok{sub}\NormalTok{(}\StringTok{'.bam'}\NormalTok{,}\StringTok{'.bigWig'}\NormalTok{, chip_file)}
\end{Highlighting}
\end{Shaded}

Now we can use the \texttt{export.bw()} function from the rtracklayer package to
write the bigWig file.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the rtracklayer package}
\KeywordTok{library}\NormalTok{(rtracklayer)}

\CommentTok{# export the bigWig output file}
\KeywordTok{export.bw}\NormalTok{(cov, }\StringTok{'output_file'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{vizualization-of-track-data-using-gviz}{%
\subsubsection{Vizualization of track data using Gviz}\label{vizualization-of-track-data-using-gviz}}

We can create genome browserlike visualizations using the \texttt{Gviz} package,
which was introduced in Chapter \ref{genomicIntervals}.
The \texttt{Gviz} is a tool which enables exhaustive customized visualization of
genomics experiments. The basic usage principle is to define tracks, where each track can represent
genomic annotation, or a signal profile; subsequently we define the order
of the tracks and plot them.
Here we will define two tracks, a genome axis, which will show the position
along the human chromosome 21; and a signal track from our CTCF experiment.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(Gviz)}
\CommentTok{# define the genome axis track}
\NormalTok{axis   =}\StringTok{ }\KeywordTok{GenomeAxisTrack}\NormalTok{(}
    \DataTypeTok{range =} \KeywordTok{GRanges}\NormalTok{(}\StringTok{'chr21'}\NormalTok{, }\KeywordTok{IRanges}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{width=}\NormalTok{seqlengths))}
\NormalTok{)}

\CommentTok{# convert the signal into genomic ranges and define the signal track}
\NormalTok{gcov   =}\StringTok{ }\KeywordTok{as}\NormalTok{(cov, }\StringTok{'GRanges'}\NormalTok{)}
\NormalTok{dtrack =}\StringTok{ }\KeywordTok{DataTrack}\NormalTok{(gcov, }\DataTypeTok{name =} \StringTok{"CTCF"}\NormalTok{, }\DataTypeTok{type=}\StringTok{'l'}\NormalTok{)}

\CommentTok{# define the track ordering}
\NormalTok{track_list =}\StringTok{ }\KeywordTok{list}\NormalTok{(axis,dtrack)}
\end{Highlighting}
\end{Shaded}

Tracks are plotted with the \texttt{plotTracks()} function. The \texttt{sizes} argument needs to be the same size as the track\_list, and defines the
relative size of each track.
Figure \ref{fig:genome-browser-gviz-show} shows the output of the
\texttt{plotTracks()} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the list of browser tracks}
\CommentTok{# sizes argument defines the relative sizes of tracks}
\CommentTok{# background title defines the color for the track labels}
\KeywordTok{plotTracks}\NormalTok{(}
    \DataTypeTok{trackList        =}\NormalTok{ track_list, }
    \DataTypeTok{sizes            =} \KeywordTok{c}\NormalTok{(.}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }
    \DataTypeTok{background.title =} \StringTok{"black"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{09-chip-seq-analysis_files/figure-latex/genome-browser-gviz-show-1} 

}

\caption{ChIP-seq signal visualized as a browser track using Gviz.}\label{fig:genome-browser-gviz-show}
\end{figure}

\hypertarget{plus-and-minus-strand-cross-correlation}{%
\subsection{Plus and minus strand cross-correlation}\label{plus-and-minus-strand-cross-correlation}}

Cross-correlation between plus and minus strands is a method
which quantifies whether the DNA library was enriched for fragments of
a certain length.

Similarity between the plus and minus strands defined as the correlation of
the signal profiles for the reads that map to the \textbf{+} and the \textbf{-} strands.
The distribution of reads is shown in Figure \ref{fig:Figure-BrowserScreenshot}.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{./Figures/BrowserScreenshot} 

}

\caption{Browser screenshot of aligned reads for one ChIP, and control sample. ChIP samples have an asymetric distribution of reads; reads mapping to the + strand are located on the left side of the peak, while the reads mapping to the - strand are found on the right side of the peak.}\label{fig:Figure-BrowserScreenshot}
\end{figure}

Due to the sequencing properties, reads which correspond to
the \textbf{5'} fragment ends will map to the opposite strand from the reads
coming from the \textbf{3'} ends. Most often (depending on the sequencing protocol)
the reads from the \textbf{5'} fragment ends map to the \textbf{+} strand,
while the reads from the \textbf{3'} ends map to the \textbf{-} strand.

We calculate the cross-correlation by shifting the signal on the \textbf{+} strand,
by a pre-defined amount (i.e.~shift by 1 - 400 nucleotides), and calculating,
for each shift, the correlation between the \textbf{+}, and the \textbf{-} strands.
Subsequently we plot the correlation versus shift, and locate the maximum value.
The maximum value should correspond to the average DNA fragment length which
was present in the library. This value tells us whether the ChIP enriched for
fragments of certain length (i.e.~whether the ChIP was successful).

Due to the size of genomic data, it might be computationally prohibitive to
calculate the Pearson correlation between whole genome (or even whole chromosome)
signal profiles.
To get around this problem, we will resort to a trick; we will disregard the dynamic
range of the signal profiles, and only keep the information of which
genomic bases contained the ends of the fragments.
This is done by calculating the coverage vector of the read starting position (separately
for each strand), and converting the coverage vector into a Boolean vector.
The Boolean vector contains the information of which genomic positions
contained the DNA fragment ends.

Similarity between two boolean vectors can be promptly computed using the Jaccard index.
The Jaccard index is defined as an intersection between two Boolean vectors,
divided by their union as shown in Figure \ref{fig:FigureJaccardSimilarity}.

\begin{figure}

{\centering \includegraphics[width=0.3\linewidth]{./Figures/Jaccard} 

}

\caption{Jaccard similarity is defined as the ratio of the intersection and union of two sets.}\label{fig:FigureJaccardSimilarity}
\end{figure}

Firstly, we load the reads for one of the CTCF ChIP experiments.
Then we create signal profiles, separately for reads on the \textbf{+} and \textbf{-}
strands.
Unlike before, we do not extend the reads to the average expected fragment
length (200 base pairs); we keep only the starting position of each read.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the reads}
\NormalTok{reads =}\StringTok{ }\KeywordTok{readGAlignments}\NormalTok{(chip_file)}
\NormalTok{reads =}\StringTok{ }\KeywordTok{granges}\NormalTok{(reads)}

\CommentTok{# keep only the starting position of each read}
\NormalTok{reads =}\StringTok{ }\KeywordTok{resize}\NormalTok{(reads, }\DataTypeTok{width=}\DecValTok{1}\NormalTok{, }\DataTypeTok{fix=}\StringTok{'start'}\NormalTok{)}

\NormalTok{reads =}\StringTok{ }\KeywordTok{keepSeqlevels}\NormalTok{(reads, }\StringTok{'chr21'}\NormalTok{, }\DataTypeTok{pruning.mode=}\StringTok{'coarse'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we can calculate the coverage vector of the read starting position.
The coverage vector is then automatically converted into a boolean vector by
asking which genomic positions have \(coverage > 0\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate the coverage profile for plus and minus strand}
\NormalTok{reads =}\StringTok{ }\KeywordTok{split}\NormalTok{(reads, }\KeywordTok{strand}\NormalTok{(reads))}

\CommentTok{# coverage(x, width = seqlengths)[[1]] > 0 }
\CommentTok{# calculates the coverage and converts}
\CommentTok{# the coverage vector into a boolean}
\NormalTok{cov   =}\StringTok{ }\KeywordTok{lapply}\NormalTok{(reads, }\ControlFlowTok{function}\NormalTok{(x)\{}
    \KeywordTok{coverage}\NormalTok{(x, }\DataTypeTok{width =}\NormalTok{ seqlengths)[[}\DecValTok{1}\NormalTok{]] }\OperatorTok{>}\StringTok{ }\DecValTok{0}
\NormalTok{\})}
\NormalTok{cov   =}\StringTok{ }\KeywordTok{lapply}\NormalTok{(cov, as.vector)}
\end{Highlighting}
\end{Shaded}

We will now shift the coverage vector from the plus strand by \(1\) to \(400\) base pairs, and for each pair shift we will calculate the Jaccard index between the vectors
on the plus and minus strand.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# defines the shift range}
\NormalTok{wsize =}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{400}

\CommentTok{# defines the jaccard similarity}
\NormalTok{jaccard =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x,y)}\KeywordTok{sum}\NormalTok{((x }\OperatorTok{&}\StringTok{ }\NormalTok{y)) }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{((x }\OperatorTok{|}\StringTok{ }\NormalTok{y))}

\CommentTok{# shifts the + vector by 1 - 400 nucleotides and }
\CommentTok{# calculates the correlation coefficient}
\NormalTok{cc =}\StringTok{ }\KeywordTok{shiftApply}\NormalTok{(}
    \DataTypeTok{SHIFT =}\NormalTok{ wsize, }
    \DataTypeTok{X     =}\NormalTok{ cov[[}\StringTok{'+'}\NormalTok{]], }
    \DataTypeTok{Y     =}\NormalTok{ cov[[}\StringTok{'-'}\NormalTok{]], }
    \DataTypeTok{FUN   =}\NormalTok{ jaccard}
\NormalTok{)}

\CommentTok{# converts the results into a data frame}
\NormalTok{cc =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{fragment_size =}\NormalTok{ wsize, }\DataTypeTok{cross_correlation =}\NormalTok{ cc)}
\end{Highlighting}
\end{Shaded}

We can finally plot the shift in base pairs versus the correlation coefficient:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ cc, }\KeywordTok{aes}\NormalTok{(fragment_size, cross_correlation)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_vline}\NormalTok{(}\DataTypeTok{xintercept =} \KeywordTok{which.max}\NormalTok{(cc}\OperatorTok{$}\NormalTok{cross_correlation), }
               \DataTypeTok{size=}\DecValTok{2}\NormalTok{, }\DataTypeTok{color=}\StringTok{'red'}\NormalTok{, }\DataTypeTok{linetype=}\DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme_bw}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}
        \DataTypeTok{axis.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{10}\NormalTok{, }\DataTypeTok{face=}\StringTok{'bold'}\NormalTok{),}
        \DataTypeTok{axis.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{14}\NormalTok{,}\DataTypeTok{face=}\StringTok{"bold"}\NormalTok{),}
        \DataTypeTok{plot.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{hjust =} \FloatTok{0.5}\NormalTok{)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{xlab}\NormalTok{(}\StringTok{'Shift in base pairs'}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{ylab}\NormalTok{(}\StringTok{'Jaccard similarity'}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{09-chip-seq-analysis_files/figure-latex/correlation-plot-1} 

}

\caption{The figure shows the correlation coefficient between the ChIP-seq signal on + and $-$ strands. The peak of the distribution designates the fragment size}\label{fig:correlation-plot}
\end{figure}

Figure \ref{fig:correlation-plot} shows the shift in base pairs,
which corresponds to the maximum value of the correlation coefficient
gives us an approximation to the expected average DNA fragment length.
Because this value is not 0, or monotonically decreasing, we can conclude
that there was substantial enrichment of certain fragments in the ChIP samples.

\hypertarget{gc-bias-quantification}{%
\subsection{GC bias quantification}\label{gc-bias-quantification}}

The PCR amplification procedure can cause a significant bias in the ChIP
experiments. The bias can be influenced by the DNA fragment size distribution,
sequence composition, hexamer distribution of PCR primers, and the number of cycles used
for the amplification.
One way to determine whether some of the samples have significantly
different sequence composition is to look at whether regions with
differing GC composition were equally enriched in all experiments.

We will do the following: Firstly we will calculate the GC content of each
of the tilling windows, and then we will compare the GC content with the corresponding
cpm\index{cpm} (count per million reads) value, for each tile.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fetches the chromosome lengths and constructs the tiles}
\KeywordTok{library}\NormalTok{(GenomeInfoDb)}
\KeywordTok{library}\NormalTok{(GenomicRanges)}

\NormalTok{hg_chrs        =}\StringTok{ }\KeywordTok{getChromInfoFromUCSC}\NormalTok{(}\StringTok{'hg38'}\NormalTok{)}
\NormalTok{hg_chrs        =}\StringTok{ }\KeywordTok{subset}\NormalTok{(hg_chrs, }\KeywordTok{grepl}\NormalTok{(}\StringTok{'chr21$'}\NormalTok{,chrom))}
\NormalTok{seqlengths     =}\StringTok{ }\KeywordTok{with}\NormalTok{(hg_chrs, }\KeywordTok{setNames}\NormalTok{(size, chrom))}

\CommentTok{# tileGenome produces a list per chromosome}
\CommentTok{# unlist combines the elemenents of the list }
\CommentTok{# into one GRanges object}
\NormalTok{tilling_window =}\StringTok{ }\KeywordTok{unlist}\NormalTok{(}\KeywordTok{tileGenome}\NormalTok{(}
    \DataTypeTok{seqlengths =}\NormalTok{ seqlengths, }
    \DataTypeTok{tilewidth  =} \DecValTok{1000}
\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We will extract the sequence information from the \texttt{BSgenome.Hsapiens.UCSC.hg38}
package. \texttt{BSgenome} are generic Bioconductor containers for genomic sequences.
Sequences are extracted from the \texttt{BSgenome} container using the \texttt{getSeq()} function.
The \texttt{getSeq()} function takes as input the genome object, and the ranges with the
regions of interest; in our case, the tilling windows.
The function returns a \texttt{DNAString} object.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# loads the human genome sequence}
\KeywordTok{library}\NormalTok{(BSgenome.Hsapiens.UCSC.hg38)}

\CommentTok{# extracts the sequence from the human genome}
\NormalTok{seq =}\StringTok{ }\KeywordTok{getSeq}\NormalTok{(BSgenome.Hsapiens.UCSC.hg38, tilling_window)}
\end{Highlighting}
\end{Shaded}

To calculate the GC content, we will use the \texttt{oligonucleotideFrequency()} function on the
\texttt{DNAString} object. By setting the width parameter to 2 we will
calculate the \textbf{dinucleotide} frequency.
Each row in the resulting table will contain the number of all possible
dinucleotides observed in each tilling window.
Because we have tilling windows of the same length, we do not
necessarily need to normalize the counts by the window length.
If all of the windows have different lengths (i.e.~when at the ChIP-seq peaks), then normalization is a prerequisite.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculates the frequency of all possible dimers }
\CommentTok{# in our sequence set}
\NormalTok{nuc =}\StringTok{ }\KeywordTok{oligonucleotideFrequency}\NormalTok{(seq, }\DataTypeTok{width =} \DecValTok{2}\NormalTok{)}

\CommentTok{# converts the matrix into a data.frame}
\NormalTok{nuc =}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(nuc)}

\CommentTok{# calculates the percentages, and rounds the number}
\NormalTok{nuc =}\StringTok{ }\KeywordTok{round}\NormalTok{(nuc}\OperatorTok{/}\DecValTok{1000}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we can combine the GC frequency with the cpm values.
We will convert the cpm values to the log10 scale. To avoid
taking the \(log(0)\), we add a pseudo count of 1 to cpm.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# counts the number of reads per tilling window }
\CommentTok{# for each experiment}
\NormalTok{so =}\StringTok{ }\KeywordTok{summarizeOverlaps}\NormalTok{(tilling_window, bam_files)}

\CommentTok{# converts the raw counts to cpm values}
\NormalTok{counts  =}\StringTok{ }\KeywordTok{assays}\NormalTok{(so)[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{cpm     =}\StringTok{ }\KeywordTok{t}\NormalTok{(}\KeywordTok{t}\NormalTok{(counts)}\OperatorTok{*}\NormalTok{(}\DecValTok{1000000}\OperatorTok{/}\KeywordTok{colSums}\NormalTok{(counts)))}

\CommentTok{# because the cpm scale has a large dynamic range}
\CommentTok{# we transform it using the log function}
\NormalTok{cpm_log =}\StringTok{ }\KeywordTok{log10}\NormalTok{(cpm}\OperatorTok{+}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Combine the cpm values with the GC content,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gc =}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{data.frame}\NormalTok{(cpm_log), }\DataTypeTok{GC =}\NormalTok{ nuc[}\StringTok{'GC'}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

and plot the results.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}
    \DataTypeTok{data =}\NormalTok{ gc, }
    \KeywordTok{aes}\NormalTok{(}
        \DataTypeTok{x =}\NormalTok{ GC, }
        \DataTypeTok{y =}\NormalTok{ GM12878_hg38_CTCF_r1.chr21.bam}
\NormalTok{    )) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size=}\DecValTok{2}\NormalTok{, }\DataTypeTok{alpha=}\NormalTok{.}\DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}
    \DataTypeTok{axis.text  =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{10}\NormalTok{, }\DataTypeTok{face=}\StringTok{'bold'}\NormalTok{),}
    \DataTypeTok{axis.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{14}\NormalTok{,}\DataTypeTok{face=}\StringTok{"bold"}\NormalTok{),}
    \DataTypeTok{plot.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{hjust =} \FloatTok{0.5}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{'GC content in one kilobase windows'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{'log10( cpm + 1 )'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{'CTCF Replicate 1'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{09-chip-seq-analysis_files/figure-latex/gc-plot-1} 

}

\caption{GC content abundance in a ChIP-seq experiment}\label{fig:gc-plot}
\end{figure}

Figure \ref{fig:gc-plot} visualizes the CPM versus GC content, and
gives us two important pieces of information.
Firstly, it shows whether there was a specific amplification of regions
with extremely high or extremely low GC content. This would be a strong indication
that either the PCR or the size selection procedure were not successfully
executed.
The second piece of information comes by comparison of plots
corresponding to multiple experiments. If different ChIP-samples have
highly diverging enrichment of different ChIP regions, then
some of the samples were affected by unknown batch effects. Such effects
need to be taken into account in downstream analysis.

Firstly, we will reorder the columns of the \texttt{data.frame} using the \texttt{pivot\_longer()}
function from the \texttt{tidyr} package\index{R Packages!\texttt{tidyr}}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the tidyr package}
\KeywordTok{library}\NormalTok{(tidyr)}

\CommentTok{# pivot_longer converts a fat data.frame into a tall data.frame, }
\CommentTok{# which is the format used by the ggplot package}
\NormalTok{gcd =}\StringTok{ }\KeywordTok{pivot_longer}\NormalTok{(}
    \DataTypeTok{data      =}\NormalTok{ gc, }
    \DataTypeTok{cols      =} \OperatorTok{-}\NormalTok{GC,}
    \DataTypeTok{names_to  =} \StringTok{'experiment'}\NormalTok{,}
    \DataTypeTok{values_to =} \StringTok{'cpm'}
\NormalTok{)}

\CommentTok{# we select the ChIP files corresponding to the ctcf experiment}
\NormalTok{gcd =}\StringTok{ }\KeywordTok{subset}\NormalTok{(gcd, }\KeywordTok{grepl}\NormalTok{(}\StringTok{'CTCF'}\NormalTok{, experiment))}

\CommentTok{# remove the chr21 suffix}
\NormalTok{gcd}\OperatorTok{$}\NormalTok{experiment =}\StringTok{ }\KeywordTok{sub}\NormalTok{(}\StringTok{'chr21.'}\NormalTok{,}\StringTok{''}\NormalTok{,gcd}\OperatorTok{$}\NormalTok{experiment)     }
\end{Highlighting}
\end{Shaded}

We can now visualize the relationship using a scatter plot.
Figure \ref{fig:gc-tidy-plot} compares the GC content dependency on the CPM between
the first and the second CTCF replicate. In this case, the replicate looks similar.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ gcd, }\KeywordTok{aes}\NormalTok{(GC, }\KeywordTok{log10}\NormalTok{(cpm}\OperatorTok{+}\DecValTok{1}\NormalTok{))) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size=}\DecValTok{2}\NormalTok{, }\DataTypeTok{alpha=}\NormalTok{.}\DecValTok{05}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{experiment, }\DataTypeTok{nrow=}\DecValTok{1}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}
    \DataTypeTok{axis.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{10}\NormalTok{, }\DataTypeTok{face=}\StringTok{'bold'}\NormalTok{),}
    \DataTypeTok{axis.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{14}\NormalTok{,}\DataTypeTok{face=}\StringTok{"bold"}\NormalTok{),}
    \DataTypeTok{plot.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{hjust =} \FloatTok{0.5}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{'GC content in one kilobase windows'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{'log10( cpm + 1 )'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{'CTCF Replicates 1 and 2'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{09-chip-seq-analysis_files/figure-latex/gc-tidy-plot-1} 

}

\caption{Comparison of GC content and signal abundance between two CTCF biological replicates}\label{fig:gc-tidy-plot}
\end{figure}

\hypertarget{sequence-read-genomic-distribution}{%
\subsection{Sequence read genomic distribution}\label{sequence-read-genomic-distribution}}

The fourth way to look at the ChIP quality control is to visualize
the genomic distribution of reads in different functional genomic regions.
If the ChIP samples have the same distribution of reads as the Input samples,
this implies a lack of specific enrichment. Additionally, if we have
prior knowledge of where our proteins should be located, we can use
the visualization to judge how well the genomic distributions conform to our priors.
For example, the trimethylation of histone H3 on lysine 36 - \textbf{H3K36me3} is associated
with elongating polymerase and productive transcription. If we performed a
successful ChIP experiment with an anti-\textbf{H3K36me3} antibody, we would expect most of the reads
to fall within gene bodies (introns and exons).

\hypertarget{hierarchical-annotation-of-genomic-features}{%
\subsubsection{Hierarchical annotation of genomic features}\label{hierarchical-annotation-of-genomic-features}}

Overlapping genomic features (a transcription start site of one
gene might be in an intron of another gene) will cause an ambiguity during
the read annotation. If a read overlaps more than one functional category, we are not
certain which category it should be assigned to.
To solve the problem of multiple assignments, we need to construct a set of annotation rules.
A heuristic solution is to organize the genomic annotation into a
hierarchy which will imply prioritization.
We can then look, for each read, which functional categories it overlaps, and
if it is within multiple categories, we assign the read to the topmost category.
As an example, let's say that we have 4 genomic categories: 1) TSS (transcription start sites)\index{transcription start site (TSS)}, 2) exon, 3) intron, and 4) intergenic with the following hierarchy: \textbf{TSS -\textgreater{} exon -\textgreater{} intron -\textgreater{} intergenic}. This means that if a read overlaps a TSS\index{transcription start site (TSS)} and an intron, it will be annotates as TSS. This approach is shown in Figure
\ref{fig:Figure-Hierarchical-Annotation}.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{./Figures/Hierarchical_Annotation} 

}

\caption{Principle of hierarchical annotation. The region of interest is annotated as the topmost ranked category that it overlaps. In this case, our region overlaps a TSS, an exon, and an intergenic region. Because the TSS has the topmost rank, it is annotated as a TSS.}\label{fig:Figure-Hierarchical-Annotation}
\end{figure}

Now we will construct the set of functional genomic regions, and annotate
the reads.

\hypertarget{finding-annotations}{%
\subsubsection{Finding annotations}\label{finding-annotations}}

There are multiple sources of genomic annotation. \textbf{UCSC}\index{UCSC Genome Browser},
\textbf{Genbank}, and \textbf{Ensembl}\index{Ensembl Genome Browser} databases represent stable resources,
from which the annotation can be easily obtained.

\texttt{AnnotationHub}\index{R Packages!\texttt{AnnotationHub}} is a Bioconductor -based online resource which contains a large number of experiments from various
sources. We will use the \texttt{AnnotationHub} to download the location of
genes corresponding to the \textbf{hg38} genome. The hub is accessed in the following way:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the AnnotationHub package}
\KeywordTok{library}\NormalTok{(AnnotationHub)}

\CommentTok{# connect to the hub object}
\NormalTok{hub =}\StringTok{ }\KeywordTok{AnnotationHub}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

The \texttt{hub} variable contains the programming interface towards the online database. We can use the \texttt{query()} function to find out the ID of the
``ENSEMBL''\index{Ensembl Genome Browser} gene annotation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# query the hub for the human annotation}
\NormalTok{AnnotationHub}\OperatorTok{::}\KeywordTok{query}\NormalTok{(}
    \DataTypeTok{x       =}\NormalTok{ hub, }
    \DataTypeTok{pattern =} \KeywordTok{c}\NormalTok{(}\StringTok{'ENSEMBL'}\NormalTok{,}\StringTok{'Homo'}\NormalTok{,}\StringTok{'GRCh38'}\NormalTok{,}\StringTok{'chr'}\NormalTok{,}\StringTok{'gtf'}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## AnnotationHub with 32 records
## # snapshotDate(): 2020-04-27
## # $dataprovider: Ensembl
## # $species: Homo sapiens
## # $rdataclass: GRanges
## # additional mcols(): taxonomyid, genome, description,
## #   coordinate_1_based, maintainer, rdatadateadded, preparerclass, tags,
## #   rdatapath, sourceurl, sourcetype 
## # retrieve records with, e.g., 'object[["AH50842"]]' 
## 
##             title                                           
##   AH50842 | Homo_sapiens.GRCh38.84.chr.gtf                  
##   AH50843 | Homo_sapiens.GRCh38.84.chr_patch_hapl_scaff.gtf 
##   AH51012 | Homo_sapiens.GRCh38.85.chr.gtf                  
##   AH51013 | Homo_sapiens.GRCh38.85.chr_patch_hapl_scaff.gtf 
##   AH51953 | Homo_sapiens.GRCh38.86.chr.gtf                  
##   ...       ...                                             
##   AH75392 | Homo_sapiens.GRCh38.98.chr_patch_hapl_scaff.gtf 
##   AH79159 | Homo_sapiens.GRCh38.99.chr.gtf                  
##   AH79160 | Homo_sapiens.GRCh38.99.chr_patch_hapl_scaff.gtf 
##   AH80075 | Homo_sapiens.GRCh38.100.chr.gtf                 
##   AH80076 | Homo_sapiens.GRCh38.100.chr_patch_hapl_scaff.gtf
\end{verbatim}

We are interested in the version \textbf{GRCh38.92}, which is available under \textbf{AH61126}.
To download the data from the hub, we use the \texttt{{[}{[}} operator on the
hub API.
We will download the annotation in the \textbf{GTF}\index{GTF file} format, into a \texttt{GRanges} object.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# retrieve the human gene annotation}
\NormalTok{gtf =}\StringTok{ }\NormalTok{hub[[}\StringTok{'AH61126'}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## GRanges object with 6 ranges and 3 metadata columns:
##       seqnames      ranges strand |   source       type     score
##          <Rle>   <IRanges>  <Rle> | <factor>   <factor> <numeric>
##   [1]        1 11869-14409      + |   havana gene              NA
##   [2]        1 11869-14409      + |   havana transcript        NA
##   [3]        1 11869-12227      + |   havana exon              NA
##   [4]        1 12613-12721      + |   havana exon              NA
##   [5]        1 13221-14409      + |   havana exon              NA
##   [6]        1 12010-13670      + |   havana transcript        NA
##   -------
##   seqinfo: 25 sequences (1 circular) from GRCh38 genome
\end{verbatim}

By default the ENSEMBL project labels chromosomes using numeric identifiers (i.e.~1,2,3 \ldots{} X),
without the \textbf{chr} prefix.
We need to therefore append the prefix to the chromosome names (seqlevels).
\texttt{pruning.mode\ =\ \textquotesingle{}coarse\textquotesingle{}} designates that the chromosome names will be replaced
in the gtf object.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# extract ensemel chromosome names}
\NormalTok{ensembl_seqlevels =}\StringTok{ }\KeywordTok{seqlevels}\NormalTok{(gtf)}

\CommentTok{# paste the chr prefix to the chromosome names}
\NormalTok{ucsc_seqlevels    =}\StringTok{ }\KeywordTok{paste0}\NormalTok{(}\StringTok{'chr'}\NormalTok{, ensembl_seqlevels)}

\CommentTok{# replace ensembl with ucsc chromosome names}
\KeywordTok{seqlevels}\NormalTok{(gtf, }\DataTypeTok{pruning.mode=}\StringTok{'coarse'}\NormalTok{) =}\StringTok{ }\NormalTok{ucsc_seqlevels}
\end{Highlighting}
\end{Shaded}

And finally we subset only regions which correspond to chromosome 21.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# keep only chromosome 21}
\NormalTok{gtf =}\StringTok{ }\NormalTok{gtf[}\KeywordTok{seqnames}\NormalTok{(gtf) }\OperatorTok{==}\StringTok{ 'chr21'}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\hypertarget{constructing-genomic-annotation}{%
\subsubsection{Constructing genomic annotation}\label{constructing-genomic-annotation}}

Once we have downloaded the annotation we can define the functional hierarchy\index{annotation hierarchy}.
We will use the previously mentioned ordering: \textbf{TSS -\textgreater{} exon -\textgreater{} intron -\textgreater{} intergenic}, with \textbf{TSS}\index{transcription start site (TSS)} having the highest priority and the intergenic regions having the lowest priority.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# construct a GRangesList with human annotation}
\NormalTok{annotation_list =}\StringTok{ }\KeywordTok{GRangesList}\NormalTok{(}
    
    \CommentTok{# promoters function extends the gtf around the TSS }
    \CommentTok{# by an upstream and downstream amounts}
    \DataTypeTok{tss    =} \KeywordTok{promoters}\NormalTok{(}
        \DataTypeTok{x =} \KeywordTok{subset}\NormalTok{(gtf, type}\OperatorTok{==}\StringTok{'gene'}\NormalTok{), }
        \DataTypeTok{upstream   =} \DecValTok{1000}\NormalTok{, }
        \DataTypeTok{downstream =} \DecValTok{1000}\NormalTok{),}
    \DataTypeTok{exon   =} \KeywordTok{subset}\NormalTok{(gtf, type}\OperatorTok{==}\StringTok{'exon'}\NormalTok{),}
    \DataTypeTok{intron =} \KeywordTok{subset}\NormalTok{(gtf, type}\OperatorTok{==}\StringTok{'gene'}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{annotating-reads}{%
\subsubsection{Annotating reads}\label{annotating-reads}}

To annotate the reads we will define a function that takes as input a
\textbf{.bam}\index{BAM file} file, and an annotation list, and returns the frequency of
reads in each genomic category.
We will then loop over all of the \textbf{.bam}\index{BAM file}
files to annotate each experiment.

The \texttt{annotateReads()} function works in the following way:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Load the \textbf{.bam} file.
\item
  Find overlaps between the reads and the annotation categories.
\item
  Arrange the annotated reads based on the hierarchy\index{annotation hierarchy}, and remove duplicated assignments.
\item
  Count the number of reads in each category.
\end{enumerate}

The crucial step to understand here is using the \texttt{arrange()} and \texttt{filter()} functions to keep only one annotated category per read.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{annotateReads =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(bam_file, annotation_list)\{}
    
    \KeywordTok{library}\NormalTok{(dplyr)}
    \KeywordTok{message}\NormalTok{(}\KeywordTok{basename}\NormalTok{(bam_file))}
    
    \CommentTok{# load the reads into R}
\NormalTok{    bam    =}\StringTok{ }\KeywordTok{readGAlignments}\NormalTok{(bam_file)}
    
    \CommentTok{# find overlaps between reads and annotation}
\NormalTok{    result =}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}
        \KeywordTok{findOverlaps}\NormalTok{(bam, annotation_list)}
\NormalTok{    ) }
    
    \CommentTok{# appends to the annotation index the corresponding}
    \CommentTok{# annotation name}
\NormalTok{    annotation_name   =}\StringTok{ }\KeywordTok{names}\NormalTok{(annotation_list)[result}\OperatorTok{$}\NormalTok{subjectHits]}
\NormalTok{    result}\OperatorTok{$}\NormalTok{annotation =}\StringTok{ }\NormalTok{annotation_name}
    
    \CommentTok{# order the overlaps based on the hierarchy}
\NormalTok{    result =}\StringTok{ }\NormalTok{result[}\KeywordTok{order}\NormalTok{(result}\OperatorTok{$}\NormalTok{subjectHits),]}
        
    \CommentTok{# select only one category per read}
\NormalTok{    result =}\StringTok{ }\KeywordTok{subset}\NormalTok{(result, }\OperatorTok{!}\KeywordTok{duplicated}\NormalTok{(queryHits))}
        
    \CommentTok{# count the number of reads in each category}
    \CommentTok{# group the result data frame by the corresponding category}
\NormalTok{    result =}\StringTok{ }\KeywordTok{group_by}\NormalTok{(}\DataTypeTok{.data=}\NormalTok{result, annotation)}
    
    \CommentTok{# count the number of reads in each category}
\NormalTok{    result =}\StringTok{ }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{.data =}\NormalTok{ result, }\DataTypeTok{counts =} \KeywordTok{length}\NormalTok{(annotation))}
    
    \CommentTok{# classify all reads which are outside of }
    \CommentTok{# the annotation as intergenic}
\NormalTok{    result =}\StringTok{ }\KeywordTok{rbind}\NormalTok{(}
\NormalTok{        result, }
        \KeywordTok{data.frame}\NormalTok{(}
                \DataTypeTok{annotation =} \StringTok{'intergenic'}\NormalTok{,}
                \DataTypeTok{counts     =} \KeywordTok{length}\NormalTok{(bam) }\OperatorTok{-}\StringTok{ }\KeywordTok{sum}\NormalTok{(result}\OperatorTok{$}\NormalTok{counts)}
\NormalTok{            )}
\NormalTok{        )}
    
    \CommentTok{# calculate the frequency}
\NormalTok{    result}\OperatorTok{$}\NormalTok{frequency  =}\StringTok{ }\KeywordTok{with}\NormalTok{(result, }\KeywordTok{round}\NormalTok{(counts}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(counts),}\DecValTok{2}\NormalTok{))}

    \CommentTok{# append the experiment name}
\NormalTok{    result}\OperatorTok{$}\NormalTok{experiment =}\StringTok{ }\KeywordTok{basename}\NormalTok{(bam_file)}
        
    \KeywordTok{return}\NormalTok{(result)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We execute the annotation function on all files.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# list all bam files in the folder}
\NormalTok{bam_files   =}\StringTok{ }\KeywordTok{list.files}\NormalTok{(data_path, }\DataTypeTok{full.names=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{pattern=}\StringTok{'bam$'}\NormalTok{)}

\CommentTok{# calculate the read distribution for every file}
\NormalTok{annot_reads_list =}\StringTok{ }\KeywordTok{lapply}\NormalTok{(bam_files, }\ControlFlowTok{function}\NormalTok{(x)\{}
    \KeywordTok{annotateReads}\NormalTok{(}
        \DataTypeTok{bam_file        =}\NormalTok{ x, }
        \DataTypeTok{annotation_list =}\NormalTok{ annotation_list}
\NormalTok{    )}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

First, we combine the results in one data frame, and
reformat the experiment names.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# collapse the per-file read distributions into one data.frame}
\NormalTok{annot_reads_df =}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{bind_rows}\NormalTok{(annot_reads_list)}

\CommentTok{# format the experiment names}
\NormalTok{experiment_name =}\StringTok{ }\NormalTok{annot_reads_df}\OperatorTok{$}\NormalTok{experiment}
\NormalTok{experiment_name =}\StringTok{ }\KeywordTok{sub}\NormalTok{(}\StringTok{'.chr21.bam'}\NormalTok{,}\StringTok{''}\NormalTok{, experiment_name)}
\NormalTok{experiment_name =}\StringTok{ }\KeywordTok{sub}\NormalTok{(}\StringTok{'GM12878_hg38_'}\NormalTok{,}\StringTok{''}\NormalTok{,experiment_name)}
\NormalTok{annot_reads_df}\OperatorTok{$}\NormalTok{experiment =}\StringTok{ }\NormalTok{experiment_name}
\end{Highlighting}
\end{Shaded}

And plot the results.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ annot_reads_df, }
       \KeywordTok{aes}\NormalTok{(}
           \DataTypeTok{x    =}\NormalTok{ experiment, }
           \DataTypeTok{y    =}\NormalTok{ frequency, }
           \DataTypeTok{fill =}\NormalTok{ annotation}
\NormalTok{        )) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat=}\StringTok{'identity'}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme_bw}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{'Set2'}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}
        \DataTypeTok{axis.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{10}\NormalTok{, }\DataTypeTok{face=}\StringTok{'bold'}\NormalTok{),}
        \DataTypeTok{axis.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{14}\NormalTok{,}\DataTypeTok{face=}\StringTok{"bold"}\NormalTok{),}
        \DataTypeTok{plot.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{hjust =} \FloatTok{0.5}\NormalTok{),}
        \DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{90}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{xlab}\NormalTok{(}\StringTok{'Sample'}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{ylab}\NormalTok{(}\StringTok{'Percentage of reads'}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{'Percentage of reads in annotation'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{09-chip-seq-analysis_files/figure-latex/read-annotation-plot-1} 

}

\caption{Read distribution in genomice functional annotation categories.}\label{fig:read-annotation-plot}
\end{figure}

Figure \ref{fig:read-annotation-plot} shows a slight increase of \textbf{H3K36me3} on the exons
and introns, and \textbf{H3K4me3} on the \textbf{TSS}. Interestingly, both replicates of the \textbf{ZNF143}
transcription factor show increased read abundance around the TSS.

\hypertarget{peak-calling}{%
\section{Peak calling}\label{peak-calling}}

\index{Peak calling}

After we are convinced that the data is of sufficient quality, we can
proceed with the downstream analysis.
One of the first steps in the ChIP-seq analysis is peak calling.
Peak calling is a statistical procedure, which uses coverage properties
of ChIP and Input samples to find regions which are enriched due to
protein binding.
The procedure requires mapped reads, and outputs a set of regions, which
represent the putative binding locations. Each region is usually associated
with a significance score which is an indicator of enrichment.

For peak calling we will use the \texttt{normR}\index{R Packages!\texttt{normR}} Bioconductor package.
\texttt{normR} uses a binomial mixture model, and performs simultaneous
normalization and peak finding. Due to the nature of the model, it is
quite flexible and can be used for different types of ChIP experiments.

One of the caveats of \texttt{normR} is that it does not inherently support
multiple biological replicates, for the same biological sample.
Therefore, the peak calling procedure needs to be done on each replicate
separately, and the peaks need to be combined in post-processing.

\hypertarget{types-of-chip-seq-experiments}{%
\subsection{Types of ChIP-seq experiments}\label{types-of-chip-seq-experiments}}

Based on the binding properties of ChIP-ped proteins, ChIP-seq
signal profiles can be divided into three classes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Sharp} (point signal): A signal profile which is localized to specific
  short genomic regions (up to couple of hundred base pairs)
  It is usually obtained from transcription factors, or highly localized posttranslational histone modifications
  (H3K4me3, which is found on gene promoters).
\item
  \textbf{Broad} (wide signal): The signal covers broad genomic domains spanning up to several kilobases.
  Usually produced by disperse histone modifications\index{histone modification} (H3K36me3, located
  on gene bodies, or H3K23me3, which is deposited by the Polycomb complex in large genomic regions).
\item
  \textbf{Mixed}: The signal consists of a mixture of sharp and broad regions.
  It is produced by proteins which have dynamic behavior. Most often these are ChIP experiments
  of RNA Polymerase 2.
\end{enumerate}

Different types of ChIP experiments usually require specialized analysis tools. Some peak callers are developed to specifically detect narrow peaks \citep{zhang_2008, xu_2010, shao_2012}, while others
detect enrichment in diffuse broad regions \citep{zang_2009, micsinai_2012, beck_2012, song_2011, xing_2012},
or mixed (Polymerase 2) signals \citep{han_2012}.
Recent developments in peak calling methods (such as \texttt{normR}) can however accommodate
multiple types of ChIP experiments \citep{rashid_2011}.
The choice of the algorithm will largely depend on the type of the wanted
results, and the peculiarities of the experimental design and execution \citep{laajala_2009, wilbanks_2010}.

If you are not certain what kind of signal profile to expect from a ChIP-seq
experiment, the best solution is to visualize the data. We will now use the data from \textbf{H3K4me3} (Sharp), \textbf{H3K36me3} (Broad), and \textbf{POL2} (Mixed)
ChIP experiments to show the differences in the signal profiles. We will use the bigWig files to visualize the signal profiles around a
highly expressed human gene from chromosome 21. This will give us an indication
of how the profiles for different types of ChIP experiments differ. First we select the files of interest:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set names for chip-seq bigWig files}
\NormalTok{chip_files =}\StringTok{ }\KeywordTok{list}\NormalTok{(}
    \DataTypeTok{H3K4me3  =} \StringTok{'GM12878_hg38_H3K4me3.chr21.bw'}\NormalTok{,}

    \DataTypeTok{H3K36me3 =} \StringTok{'GM12878_hg38_H3K36me3.chr21.bw'}\NormalTok{,}

    \DataTypeTok{POL2     =} \StringTok{'GM12878_hg38_POLR2A.chr21.bw'}
\NormalTok{)}
\CommentTok{# get full paths to the files}
\NormalTok{chip_files =}\StringTok{ }\KeywordTok{lapply}\NormalTok{(chip_files, }\ControlFlowTok{function}\NormalTok{(x)\{}
    \KeywordTok{file.path}\NormalTok{(data_path, x)}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

Next we import the coverage profiles into \textbf{R}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load rtracklayer}
\KeywordTok{library}\NormalTok{(rtracklayer)}

\CommentTok{# import the ChIP bigWig files}
\NormalTok{chip_profiles =}\StringTok{ }\KeywordTok{lapply}\NormalTok{(chip_files, rtracklayer}\OperatorTok{::}\NormalTok{import.bw)}
\end{Highlighting}
\end{Shaded}

We fetch the reference annotation for human chromosome 21.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(AnnotationHub)}
\NormalTok{hub =}\StringTok{ }\KeywordTok{AnnotationHub}\NormalTok{()}
\NormalTok{gtf =}\StringTok{ }\NormalTok{hub[[}\StringTok{'AH61126'}\NormalTok{]]}

\CommentTok{# select only chromosome 21}
\KeywordTok{seqlevels}\NormalTok{(gtf, }\DataTypeTok{pruning.mode=}\StringTok{'coarse'}\NormalTok{) =}\StringTok{ '21'}

\CommentTok{# extract chromosome names}
\NormalTok{ensembl_seqlevels =}\StringTok{ }\KeywordTok{seqlevels}\NormalTok{(gtf)}

\CommentTok{# paste the chr prefix to the chromosome names}
\NormalTok{ucsc_seqlevels    =}\StringTok{ }\KeywordTok{paste0}\NormalTok{(}\StringTok{'chr'}\NormalTok{, ensembl_seqlevels)}

\CommentTok{# replace ensembl with ucsc chromosome names}
\KeywordTok{seqlevels}\NormalTok{(gtf, }\DataTypeTok{pruning.mode=}\StringTok{'coarse'}\NormalTok{) =}\StringTok{ }\NormalTok{ucsc_seqlevels}
\end{Highlighting}
\end{Shaded}

To enable \texttt{Gviz} to work with genomic annotation we will convert the \texttt{GRanges}
object into a transcript database using the following function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the GenomicFeatures object}
\KeywordTok{library}\NormalTok{(GenomicFeatures)}

\CommentTok{# convert the gtf annotation into a data.base}
\NormalTok{txdb         =}\StringTok{ }\KeywordTok{makeTxDbFromGRanges}\NormalTok{(gtf)}
\end{Highlighting}
\end{Shaded}

And convert the transcript database into a \texttt{Gviz} track.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define the gene track object}
\NormalTok{gene_track  =}\StringTok{ }\KeywordTok{GeneRegionTrack}\NormalTok{(txdb, }\DataTypeTok{chr=}\StringTok{'chr21'}\NormalTok{, }\DataTypeTok{genome=}\StringTok{'hg38'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once we have downloaded the annotation, and imported the signal profiles into \textbf{R} we are ready to visualize the data.
We will again use the \texttt{Gviz} library. We firstly define the coordinate system. The ideogram track which will show
the position of our current viewpoint on the chromosome, and a genome axis track, which will show the exact coordinates.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load Gviz package}
\KeywordTok{library}\NormalTok{(Gviz)}
\CommentTok{# fetches the chromosome length information}
\NormalTok{hg_chrs =}\StringTok{ }\KeywordTok{getChromInfoFromUCSC}\NormalTok{(}\StringTok{'hg38'}\NormalTok{)}
\NormalTok{hg_chrs =}\StringTok{ }\KeywordTok{subset}\NormalTok{(hg_chrs, (}\KeywordTok{grepl}\NormalTok{(}\StringTok{'chr21$'}\NormalTok{,chrom)))}

\CommentTok{# convert data.frame to named vector}
\NormalTok{seqlengths =}\StringTok{ }\KeywordTok{with}\NormalTok{(hg_chrs, }\KeywordTok{setNames}\NormalTok{(size, chrom))}

\CommentTok{# constructs the ideogram track}
\NormalTok{chr_track   =}\StringTok{ }\KeywordTok{IdeogramTrack}\NormalTok{(}
    \DataTypeTok{chromosome =} \StringTok{'chr21'}\NormalTok{, }
    \DataTypeTok{genome     =} \StringTok{'hg38'}
\NormalTok{)}

\CommentTok{# constructs the coordinate system}
\NormalTok{axis =}\StringTok{ }\KeywordTok{GenomeAxisTrack}\NormalTok{(}
    \DataTypeTok{range =} \KeywordTok{GRanges}\NormalTok{(}\StringTok{'chr21'}\NormalTok{, }\KeywordTok{IRanges}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{width=}\NormalTok{seqlengths))}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We use a loop to convert the signal profiles into a \texttt{DataTrack} object.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# use a lapply on the imported bw files to create the track objects}
\CommentTok{# we loop over experiment names, and select the corresponding object}
\CommentTok{# within the function}
\NormalTok{data_tracks =}\StringTok{ }\KeywordTok{lapply}\NormalTok{(}\KeywordTok{names}\NormalTok{(chip_profiles), }\ControlFlowTok{function}\NormalTok{(exp_name)\{}
    
    \CommentTok{# chip_profiles[[exp_name]] - selects the }
    \CommentTok{# proper experiment using the exp_name}
    \KeywordTok{DataTrack}\NormalTok{(}
        \DataTypeTok{range =}\NormalTok{ chip_profiles[[exp_name]],   }
        \DataTypeTok{name  =}\NormalTok{ exp_name,  }
        
        \CommentTok{# type of the track}
        \DataTypeTok{type  =} \StringTok{'h'}\NormalTok{, }
        
        \CommentTok{# line width parameter}
        \DataTypeTok{lwd   =} \DecValTok{5}
\NormalTok{    )}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

We are finally ready to create the genome screenshot.
We will focus on an extended region around the URB1 gene.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select the start coordinate for the URB1 gene}
\NormalTok{start =}\StringTok{ }\KeywordTok{min}\NormalTok{(}\KeywordTok{start}\NormalTok{(}\KeywordTok{subset}\NormalTok{(gtf, gene_name }\OperatorTok{==}\StringTok{ 'URB1'}\NormalTok{)))}

\CommentTok{# select the end coordinate for the URB1 gene}
\NormalTok{end   =}\StringTok{ }\KeywordTok{max}\NormalTok{(}\KeywordTok{end}\NormalTok{(}\KeywordTok{subset}\NormalTok{(gtf, gene_name }\OperatorTok{==}\StringTok{ 'URB1'}\NormalTok{)))}

\CommentTok{# plot the signal profiles around the URB1 gene}
\KeywordTok{plotTracks}\NormalTok{(}
    \DataTypeTok{trackList =} \KeywordTok{c}\NormalTok{(chr_track, axis, gene_track, data_tracks),}
    
    \CommentTok{# relative track sizes}
    \DataTypeTok{sizes     =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }
    
    \CommentTok{# background color}
    \DataTypeTok{background.title     =} \StringTok{"black"}\NormalTok{,}
    
    \CommentTok{# controls visualization of gene sets}
    \DataTypeTok{collapseTranscripts  =} \StringTok{"longest"}\NormalTok{, }
    \DataTypeTok{transcriptAnnotation =} \StringTok{"symbol"}\NormalTok{,}
    
    \CommentTok{# coordinates to visualize }
    \DataTypeTok{from =}\NormalTok{ start }\OperatorTok{-}\StringTok{ }\DecValTok{5000}\NormalTok{,}
    \DataTypeTok{to   =}\NormalTok{ end   }\OperatorTok{+}\StringTok{ }\DecValTok{5000}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{09-chip-seq-analysis_files/figure-latex/chip-type-plot-gviz-1} 

}

\caption{ChIP-seq signal around the URB1 gene.}\label{fig:chip-type-plot-gviz}
\end{figure}

Figure \ref{fig:chip-type-plot-gviz} shows the signal profile around the URB1 gene. H3K4me3 signal profile contains a strong narrow peak on the transcription start site. H3K36me3 shows strong enrichment in the gene body, while the POL2 ChIP shows a mixed profile, with a strong peak at the TSS and an enrichment over the gene body.

\hypertarget{peak-calling-sharp-peaks}{%
\subsection{Peak calling: Sharp peaks}\label{peak-calling-sharp-peaks}}

\index{Peak calling}

We will now use the \texttt{normR} \citep{helmuth_2016} package for peak calling in sharp and broad peak experiments.

Select the input files. Since \texttt{normR} does not support the usage of biological
replicates, we will showcase the peak calling on one of the CTCF samples.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# full path to the ChIP data file}
\NormalTok{chip_file    =}\StringTok{ }\KeywordTok{file.path}\NormalTok{(data_path, }\StringTok{'GM12878_hg38_CTCF_r1.chr21.bam'}\NormalTok{)}

\CommentTok{# full path to the Control data file}
\NormalTok{control_file =}\StringTok{ }\KeywordTok{file.path}\NormalTok{(data_path, }\StringTok{'GM12878_hg38_Input_r5.chr21.bam'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To understand the dynamic range of enrichment, we will create a scatter plot
showing the strength of signal in the CTCF and Input.

Let us first count the reads in 1-kb windows, and normalize them to counts per
million sequenced reads.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# as previously done, we calculate the cpm for each experiment}
\KeywordTok{library}\NormalTok{(GenomicRanges)}
\KeywordTok{library}\NormalTok{(GenomicAlignments)}

\CommentTok{# select the chromosome}
\NormalTok{hg_chrs =}\StringTok{ }\KeywordTok{getChromInfoFromUCSC}\NormalTok{(}\StringTok{'hg38'}\NormalTok{) }
\NormalTok{hg_chrs =}\StringTok{ }\KeywordTok{subset}\NormalTok{(hg_chrs, }\KeywordTok{grepl}\NormalTok{(}\StringTok{'chr21$'}\NormalTok{,chrom))}

\NormalTok{seqlengths =}\StringTok{ }\KeywordTok{with}\NormalTok{(hg_chrs, }\KeywordTok{setNames}\NormalTok{(size, chrom))}

\CommentTok{# define the windows}
\NormalTok{tilling_window =}\StringTok{ }\KeywordTok{unlist}\NormalTok{(}\KeywordTok{tileGenome}\NormalTok{(seqlengths, }\DataTypeTok{tilewidth=}\DecValTok{1000}\NormalTok{))}

\CommentTok{# count the reads}
\NormalTok{counts         =}\StringTok{ }\KeywordTok{summarizeOverlaps}\NormalTok{(}
    \DataTypeTok{features =}\NormalTok{ tilling_window, }
    \DataTypeTok{reads    =} \KeywordTok{c}\NormalTok{(chip_file, control_file)}
\NormalTok{)}

\CommentTok{# normalize read counts}
\NormalTok{counts         =}\StringTok{ }\KeywordTok{assays}\NormalTok{(counts)[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{cpm =}\StringTok{ }\KeywordTok{t}\NormalTok{(}\KeywordTok{t}\NormalTok{(counts)}\OperatorTok{*}\NormalTok{(}\DecValTok{1000000}\OperatorTok{/}\KeywordTok{colSums}\NormalTok{(counts)))}
\end{Highlighting}
\end{Shaded}

We can now plot the ChIP versus Input signal:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{# convert the matrix into a data.frame for ggplot}
\NormalTok{cpm =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(cpm)}
\KeywordTok{ggplot}\NormalTok{(}
    \DataTypeTok{data =}\NormalTok{ cpm, }
    \KeywordTok{aes}\NormalTok{(}
        \DataTypeTok{x =}\NormalTok{ GM12878_hg38_Input_r5.chr21.bam, }
        \DataTypeTok{y =}\NormalTok{ GM12878_hg38_CTCF_r1.chr21.bam)}
\NormalTok{    ) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_abline}\NormalTok{(}\DataTypeTok{slope =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme_bw}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme_bw}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{'Set2'}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}
        \DataTypeTok{axis.text   =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{10}\NormalTok{, }\DataTypeTok{face=}\StringTok{'bold'}\NormalTok{),}
        \DataTypeTok{axis.title  =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{14}\NormalTok{,}\DataTypeTok{face=}\StringTok{"bold"}\NormalTok{),}
        \DataTypeTok{plot.title  =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{hjust =} \FloatTok{0.5}\NormalTok{),}
        \DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{90}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{xlab}\NormalTok{(}\StringTok{'Input CPM'}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{ylab}\NormalTok{(}\StringTok{'CTCF CPM'}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{'ChIP versus Input'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{09-chip-seq-analysis_files/figure-latex/peak-calling-sharp-plot-1} 

}

\caption{Comparison of CPM values between ChIP and Input experiments. Good ChIP experiments should always show enrichment.}\label{fig:peak-calling-sharp-plot}
\end{figure}

Regions above the diagonal, in Figure \ref{fig:peak-calling-sharp-plot}, show
higher enrichment in the ChIP samples, while the regions below the diagonal
show higher enrichment in the Input samples.

Let us now perform for peak calling. \texttt{normR} usage is deceivingly simple; we need to provide the location ChIP and Control read files, and the genome version to the \texttt{enrichR()} function. The function will automatically create tilling windows (250bp by default), count the number of reads in each window, and fit a mixture of binomial distributions.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(normr)}
\CommentTok{# peak calling using chip and control}
\NormalTok{ctcf_fit =}\StringTok{ }\KeywordTok{enrichR}\NormalTok{(}
    
            \CommentTok{# ChIP file}
            \DataTypeTok{treatment =}\NormalTok{ chip_file,}
            
            \CommentTok{# control file}
            \DataTypeTok{control   =}\NormalTok{ control_file,}
            
            \CommentTok{# genome version}
            \DataTypeTok{genome    =} \StringTok{"hg38"}\NormalTok{,}
            
            \CommentTok{# print intermediary steps during the analysis}
            \DataTypeTok{verbose   =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

With the summary function we can take a look at the results:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(ctcf_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## NormRFit-class object
## 
## Type:                  'enrichR'
## Number of Regions:     12353090
## Number of Components:  2
## Theta* (naive bg):     0.137
## Background component B: 1
## 
## +++ Results of fit +++ 
## Mixture Proportions:
## Background       Class 1    
##     97.72%         2.28%    
## Theta:
## Background       Class 1    
##      0.103         0.695    
## 
## Bayesian Information Criterion:  539882
## 
## +++ Results of binomial test +++ 
## T-Filter threshold: 4
## Number of Regions filtered out: 12267164
## Significantly different from background B based on q-values:
## TOTAL:
##            ***        **         *         .                n.s.
## Bins         0       627       120       195        87     84897
## %        0.000     0.711     0.847     1.068     1.166    96.209
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 '  ' 1 'n.s.'
\end{verbatim}

The summary function shows that most of the regions of chromosome 21 correspond
to the background: \(97.72%
\). In total we have \(1029=(627+120+195+87)\) significantly enriched regions.

We will now extract the regions into a \texttt{GRanges} object.
The \texttt{getRanges()} function extracts the regions from the model. Using the
\texttt{getQvalue()}, and \texttt{getEnrichment()} function we assign to our regions
the statistical significance and calculated enrichment.
In order to identify only highly significant regions,
we keep only ranges where the false discovery rate (q value) is below \(0.01\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# extracts the ranges}
\NormalTok{ctcf_peaks =}\StringTok{ }\KeywordTok{getRanges}\NormalTok{(ctcf_fit)}
    
\CommentTok{# annotates the ranges with the supporting p value}
\NormalTok{ctcf_peaks}\OperatorTok{$}\NormalTok{qvalue     =}\StringTok{ }\KeywordTok{getQvalues}\NormalTok{(ctcf_fit)}
    
\CommentTok{# annotates the ranges with the calculated enrichment}
\NormalTok{ctcf_peaks}\OperatorTok{$}\NormalTok{enrichment =}\StringTok{ }\KeywordTok{getEnrichment}\NormalTok{(ctcf_fit)}
    
\CommentTok{# selects the ranges which correspond to the enriched class}
\NormalTok{ctcf_peaks =}\StringTok{ }\KeywordTok{subset}\NormalTok{(ctcf_peaks, }\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(component))}
    
\CommentTok{# filter by a stringent q value threshold}
\NormalTok{ctcf_peaks =}\StringTok{ }\KeywordTok{subset}\NormalTok{(ctcf_peaks, qvalue }\OperatorTok{<}\StringTok{ }\FloatTok{0.01}\NormalTok{)}
    
\CommentTok{# order the peaks based on the q value}
\NormalTok{ctcf_peaks =}\StringTok{ }\NormalTok{ctcf_peaks[}\KeywordTok{order}\NormalTok{(ctcf_peaks}\OperatorTok{$}\NormalTok{qvalue)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## GRanges object with 724 ranges and 3 metadata columns:
##         seqnames            ranges strand | component       qvalue enrichment
##            <Rle>         <IRanges>  <Rle> | <integer>    <numeric>  <numeric>
##     [1]    chr21 43939251-43939500      * |         1 4.69885e-140    1.37891
##     [2]    chr21 43646751-43647000      * |         1 2.52008e-137    1.42361
##     [3]    chr21 43810751-43811000      * |         1 1.86406e-121    1.30519
##     [4]    chr21 43939001-43939250      * |         1 2.10824e-121    1.19820
##     [5]    chr21 37712251-37712500      * |         1 6.35715e-118    1.70989
##     ...      ...               ...    ... .       ...          ...        ...
##   [720]    chr21 38172001-38172250      * |         1   0.00867374   0.951189
##   [721]    chr21 38806001-38806250      * |         1   0.00867374   0.951189
##   [722]    chr21 42009501-42009750      * |         1   0.00867374   0.656253
##   [723]    chr21 46153001-46153250      * |         1   0.00867374   0.951189
##   [724]    chr21 46294751-46295000      * |         1   0.00867374   0.722822
##   -------
##   seqinfo: 24 sequences from an unspecified genome
\end{verbatim}

After stringent q value filtering we are left with \(724\) peaks. For the ease of downstream analysis, we will limit the sequence levels to
chromosome 21.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{seqlevels}\NormalTok{(ctcf_peaks, }\DataTypeTok{pruning.mode=}\StringTok{'coarse'}\NormalTok{) =}\StringTok{ 'chr21'}
\end{Highlighting}
\end{Shaded}

Let's export the peaks into a .txt file which we can use the downstream in the analysis.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# write the peaks loacations into a txt table}
\KeywordTok{write.table}\NormalTok{(ctcf_peaks, }\KeywordTok{file.path}\NormalTok{(data_path, }\StringTok{'CTCF_peaks.txt'}\NormalTok{), }
            \DataTypeTok{row.names=}\NormalTok{F, }\DataTypeTok{col.names=}\NormalTok{T, }\DataTypeTok{quote=}\NormalTok{F, }\DataTypeTok{sep=}\StringTok{'}\CharTok{\textbackslash{}t}\StringTok{'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can now repeat the CTCF versus Input plot, and label significantly marked peaks. Using the count overlaps we mark which of our 1-kb regions contained significant peaks.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# find enriched tilling windows}
\NormalTok{enriched_regions =}\StringTok{ }\KeywordTok{countOverlaps}\NormalTok{(tilling_window, ctcf_peaks) }\OperatorTok{>}\StringTok{ }\DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\NormalTok{cpm}\OperatorTok{$}\NormalTok{enriched_regions =}\StringTok{ }\NormalTok{enriched_regions }

\KeywordTok{ggplot}\NormalTok{(}
    \DataTypeTok{data =}\NormalTok{ cpm, }
    \KeywordTok{aes}\NormalTok{(}
        \DataTypeTok{x =}\NormalTok{ GM12878_hg38_Input_r5.chr21.bam, }
        \DataTypeTok{y =}\NormalTok{ GM12878_hg38_CTCF_r1.chr21.bam, }
        \DataTypeTok{color =}\NormalTok{ enriched_regions}
\NormalTok{    )) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_abline}\NormalTok{(}\DataTypeTok{slope =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{'Set2'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}
    \DataTypeTok{axis.text   =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{10}\NormalTok{, }\DataTypeTok{face=}\StringTok{'bold'}\NormalTok{),}
    \DataTypeTok{axis.title  =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{14}\NormalTok{,}\DataTypeTok{face=}\StringTok{"bold"}\NormalTok{),}
    \DataTypeTok{plot.title  =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{hjust =} \FloatTok{0.5}\NormalTok{),}
    \DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{90}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{'Input CPM'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{'CTCF CPM'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{'ChIP versus Input'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_manual}\NormalTok{(}\DataTypeTok{values=}\KeywordTok{c}\NormalTok{(}\StringTok{'gray'}\NormalTok{,}\StringTok{'red'}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{09-chip-seq-analysis_files/figure-latex/peak-calling-sharp-peak-calling-plot-1} 

}

\caption{Comparison of signal between ChIP and input samples. Red labeled dots correspond to called peaks..}\label{fig:peak-calling-sharp-peak-calling-plot}
\end{figure}

Figure \ref{fig:peak-calling-sharp-peak-calling-plot} shows that \texttt{normR}
identified all of the regions above the diagonal as statistically significant.
It has, however, labeled a significant number of regions below the diagonal.
Because of the sophisticated statistical model,
\texttt{normR} has greater sensitivity, and these peaks might really be enriched regions,
it is worth investigating the nature of these regions. This is left as an exercise
to the reader.

We can now create a genome browser screenshot around a peak region.
This will show us what kind of signal properties have contributed to the peak calling.
We would expect to see a strong, bell-shaped, enrichment in the ChIP sample, and
uniform noise in the Input sample.

Let us now visualize the signal around the most enriched peak. The following function takes as input a \textbf{.bam} file, and loads the bam into R.
It extends the reads to a size of 200 bp, and creates the coverage vector.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate the coverage for one bam file}
\NormalTok{calculateCoverage =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}
\NormalTok{  bam_file,}
  \DataTypeTok{extend =} \DecValTok{200}
\NormalTok{)\{}
  
  \CommentTok{# load reads into R}
\NormalTok{  reads =}\StringTok{ }\KeywordTok{readGAlignments}\NormalTok{(bam_file)}
  
  \CommentTok{# convert reads into a GRanges object}
\NormalTok{  reads =}\StringTok{ }\KeywordTok{granges}\NormalTok{(reads)}
  
  \CommentTok{# resize the reads to 200bp}
\NormalTok{  reads =}\StringTok{ }\KeywordTok{resize}\NormalTok{(reads, }\DataTypeTok{width=}\NormalTok{extend, }\DataTypeTok{fix=}\StringTok{'start'}\NormalTok{)}
  
  \CommentTok{# get the coverage vector}
\NormalTok{  cov   =}\StringTok{ }\KeywordTok{coverage}\NormalTok{(reads)}
  
  \CommentTok{# normalize the coverage vector to the sequencing depth}
\NormalTok{  cov =}\StringTok{ }\KeywordTok{round}\NormalTok{(cov }\OperatorTok{*}\StringTok{ }\NormalTok{(}\DecValTok{1000000}\OperatorTok{/}\KeywordTok{length}\NormalTok{(reads)),}\DecValTok{2}\NormalTok{)}
  
  \CommentTok{# convert the coverage go a GRanges object}
\NormalTok{  cov   =}\StringTok{ }\KeywordTok{as}\NormalTok{(cov, }\StringTok{'GRanges'}\NormalTok{)}
  
  \CommentTok{# keep only chromosome 21}
  \KeywordTok{seqlevels}\NormalTok{(cov, }\DataTypeTok{pruning.mode=}\StringTok{'coarse'}\NormalTok{) =}\StringTok{ 'chr21'}
  \KeywordTok{return}\NormalTok{(cov)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Let's apply the function to the ChIP and input samples.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate coverage for the ChIP file}
\NormalTok{ctcf_cov =}\StringTok{ }\KeywordTok{calculateCoverage}\NormalTok{(chip_file)}

\CommentTok{# calculate coverage for the control file}
\NormalTok{cont_cov =}\StringTok{ }\KeywordTok{calculateCoverage}\NormalTok{(control_file)}
\end{Highlighting}
\end{Shaded}

Using \texttt{Gviz}, we will construct the layered tracks.
First, we layout the genome coordinates:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load Gviz and get the chromosome coordinates}
\KeywordTok{library}\NormalTok{(Gviz)}
\NormalTok{chr_track  =}\StringTok{ }\KeywordTok{IdeogramTrack}\NormalTok{(}\StringTok{'chr21'}\NormalTok{, }\StringTok{'hg38'}\NormalTok{)}
\NormalTok{axis       =}\StringTok{ }\KeywordTok{GenomeAxisTrack}\NormalTok{(}
    \DataTypeTok{range =} \KeywordTok{GRanges}\NormalTok{(}\StringTok{'chr21'}\NormalTok{, }\KeywordTok{IRanges}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{width=}\NormalTok{seqlengths))}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then, the peak locations:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# peaks track}
\NormalTok{peaks_track =}\StringTok{ }\KeywordTok{AnnotationTrack}\NormalTok{(ctcf_peaks, }\DataTypeTok{name =} \StringTok{"CTCF Peaks"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

And finally, the signal files:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chip_track  =}\StringTok{ }\KeywordTok{DataTrack}\NormalTok{(}
    \DataTypeTok{range =}\NormalTok{ ctcf_cov,   }
    \DataTypeTok{name  =} \StringTok{"CTCF"}\NormalTok{,  }
    \DataTypeTok{type  =} \StringTok{'h'}\NormalTok{, }
    \DataTypeTok{lwd   =} \DecValTok{3}
\NormalTok{)}

\NormalTok{cont_track  =}\StringTok{ }\KeywordTok{DataTrack}\NormalTok{(}
    \DataTypeTok{range =}\NormalTok{ cont_cov,   }
    \DataTypeTok{name  =} \StringTok{"Input"}\NormalTok{, }
    \DataTypeTok{type  =} \StringTok{'h'}\NormalTok{, }
    \DataTypeTok{lwd=}\DecValTok{3}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plotTracks}\NormalTok{(}
    \DataTypeTok{trackList =} \KeywordTok{list}\NormalTok{(chr_track, axis, peaks_track, chip_track, cont_track), }
    \DataTypeTok{sizes     =} \KeywordTok{c}\NormalTok{(.}\DecValTok{2}\NormalTok{,.}\DecValTok{5}\NormalTok{,.}\DecValTok{5}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }
    \DataTypeTok{background.title =} \StringTok{"black"}\NormalTok{,}
    \DataTypeTok{from =} \KeywordTok{start}\NormalTok{(ctcf_peaks)[}\DecValTok{1}\NormalTok{] }\OperatorTok{-}\StringTok{ }\DecValTok{1000}\NormalTok{,}
    \DataTypeTok{to   =} \KeywordTok{end}\NormalTok{(ctcf_peaks)[}\DecValTok{1}\NormalTok{]   }\OperatorTok{+}\StringTok{ }\DecValTok{1000}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{09-chip-seq-analysis_files/figure-latex/peak-calling-signal-profile-plot-1} 

}

\caption{ChIP and Input signal profile around the peak centers.}\label{fig:peak-calling-signal-profile-plot}
\end{figure}

In Figure \ref{fig:peak-calling-signal-profile-plot}, the ChIP sample looks as expected.
Although the Input sample shows an enrichment,
it is important to compare the scales on both samples. The normalized ChIP signal goes up
to \(2500\), while the maximum value in the input sample is only \(60\).

\hypertarget{peak-calling-broad-regions}{%
\subsection{Peak calling: Broad regions}\label{peak-calling-broad-regions}}

\index{Peak calling}

We will now use \texttt{normR} to call peaks for the H3K36me3 histone modification,
which is associated with gene bodies of expressed genes. We define the ChIP and Input files:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fetch the ChIP-file for H3K36me3}
\NormalTok{chip_file    =}\StringTok{ }\KeywordTok{file.path}\NormalTok{(data_path, }\StringTok{'GM12878_hg38_H3K36me3.chr21.bam'}\NormalTok{)}

\CommentTok{# fetch the corresponding input file}
\NormalTok{control_file =}\StringTok{ }\KeywordTok{file.path}\NormalTok{(data_path, }\StringTok{'GM12878_hg38_Input_r5.chr21.bam'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Because H3K36 regions span broad domains, it is necessary to increase the
tilling window size which will be used for counting.
Using the \texttt{countConfiguration()} function, we will set the tilling window size
to 5000 base pairs.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(normr)}
\CommentTok{# define the window width for the counting}
\NormalTok{countConfiguration =}\StringTok{ }\KeywordTok{countConfigSingleEnd}\NormalTok{(}\DataTypeTok{binsize =} \DecValTok{5000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# find broad peaks using enrichR}
\NormalTok{h3k36_fit =}\StringTok{ }\KeywordTok{enrichR}\NormalTok{(}
            
            \CommentTok{# ChIP file}
            \DataTypeTok{treatment   =}\NormalTok{ chip_file,}
            
            \CommentTok{# control file}
            \DataTypeTok{control     =}\NormalTok{ control_file,}
            
            \CommentTok{# genome version}
            \DataTypeTok{genome      =} \StringTok{"hg38"}\NormalTok{,}
            \DataTypeTok{verbose     =} \OtherTok{FALSE}\NormalTok{,}
            
            \CommentTok{# window size for counting}
            \DataTypeTok{countConfig =}\NormalTok{ countConfiguration)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(h3k36_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## NormRFit-class object
## 
## Type:                  'enrichR'
## Number of Regions:     617665
## Number of Components:  2
## Theta* (naive bg):     0.197
## Background component B: 1
## 
## +++ Results of fit +++ 
## Mixture Proportions:
## Background       Class 1    
##      85.4%         14.6%    
## Theta:
## Background       Class 1    
##      0.138         0.442    
## 
## Bayesian Information Criterion:  741525
## 
## +++ Results of binomial test +++ 
## T-Filter threshold: 5
## Number of Regions filtered out: 610736
## Significantly different from background B based on q-values:
## TOTAL:
##           ***       **        *        .              n.s.
## Bins        0     1005      314      381      237     4992
## %        0.00     9.18    12.04    15.52    17.68    45.58
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 '  ' 1 'n.s.'
\end{verbatim}

The summary function shows that we get 1937 enriched regions. We will extract enriched regions, and plot them in the same way we did for the
CTCF.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get the locations of broad peaks}
\NormalTok{h3k36_peaks            =}\StringTok{ }\KeywordTok{getRanges}\NormalTok{(h3k36_fit)}

\CommentTok{# extract the qvalue and enrichment}
\NormalTok{h3k36_peaks}\OperatorTok{$}\NormalTok{qvalue     =}\StringTok{ }\KeywordTok{getQvalues}\NormalTok{(h3k36_fit)}
\NormalTok{h3k36_peaks}\OperatorTok{$}\NormalTok{enrichment =}\StringTok{ }\KeywordTok{getEnrichment}\NormalTok{(h3k36_fit)}

\CommentTok{# select proper peaks}
\NormalTok{h3k36_peaks =}\StringTok{ }\KeywordTok{subset}\NormalTok{(h3k36_peaks, }\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(component))}
\NormalTok{h3k36_peaks =}\StringTok{ }\KeywordTok{subset}\NormalTok{(h3k36_peaks, qvalue }\OperatorTok{<}\StringTok{ }\FloatTok{0.01}\NormalTok{)}
\NormalTok{h3k36_peaks =}\StringTok{ }\NormalTok{h3k36_peaks[}\KeywordTok{order}\NormalTok{(h3k36_peaks}\OperatorTok{$}\NormalTok{qvalue)]}

\CommentTok{# collapse nearby enriched regions}
\NormalTok{h3k36_peaks =}\StringTok{ }\KeywordTok{reduce}\NormalTok{(h3k36_peaks)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# construct the data tracks for the H3K36me3 and Input files}
\NormalTok{h3k36_cov =}\StringTok{ }\KeywordTok{calculateCoverage}\NormalTok{(chip_file)}
\NormalTok{data_tracks =}\StringTok{ }\KeywordTok{list}\NormalTok{(}
    \DataTypeTok{h3k36 =} \KeywordTok{DataTrack}\NormalTok{(h3k36_cov,  }\DataTypeTok{name =} \StringTok{'h3k36_cov'}\NormalTok{,  }\DataTypeTok{type=}\StringTok{'h'}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{),}
    \DataTypeTok{input =} \KeywordTok{DataTrack}\NormalTok{(cont_cov,   }\DataTypeTok{name =} \StringTok{'Input'}\NormalTok{,      }\DataTypeTok{type=}\StringTok{'h'}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define the window for the visualization}
\NormalTok{start =}\StringTok{ }\KeywordTok{min}\NormalTok{(}\KeywordTok{start}\NormalTok{(h3k36_peaks[}\DecValTok{2}\NormalTok{])) }\OperatorTok{-}\StringTok{ }\DecValTok{25000}
\NormalTok{end   =}\StringTok{ }\KeywordTok{max}\NormalTok{(}\KeywordTok{end}\NormalTok{(h3k36_peaks[}\DecValTok{2}\NormalTok{])) }\OperatorTok{+}\StringTok{ }\DecValTok{25000}

\CommentTok{# create the peak track}
\NormalTok{peak_track =}\StringTok{ }\KeywordTok{AnnotationTrack}\NormalTok{(}\KeywordTok{reduce}\NormalTok{(h3k36_peaks), }\DataTypeTok{name=}\StringTok{'H3K36me3'}\NormalTok{)}

\CommentTok{# plots the enriched region}
\KeywordTok{plotTracks}\NormalTok{(}
    \DataTypeTok{trackList =} \KeywordTok{c}\NormalTok{(chr_track, axis, gene_track, peak_track, data_tracks),}
    \DataTypeTok{sizes     =} \KeywordTok{c}\NormalTok{(.}\DecValTok{5}\NormalTok{,.}\DecValTok{5}\NormalTok{,.}\DecValTok{5}\NormalTok{,.}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }
    \DataTypeTok{background.title     =} \StringTok{"black"}\NormalTok{,}
    \DataTypeTok{collapseTranscripts  =} \StringTok{"longest"}\NormalTok{, }
    \DataTypeTok{transcriptAnnotation =} \StringTok{"symbol"}\NormalTok{,}
    \DataTypeTok{from =}\NormalTok{ start,}
    \DataTypeTok{to   =}\NormalTok{ end}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{09-chip-seq-analysis_files/figure-latex/peak-calling-broad-gviz-1} 

}

\caption{Visualization of H3K36me3 ChIP signal on a called broad peak.}\label{fig:peak-calling-broad-gviz}
\end{figure}

The Figure \ref{fig:peak-calling-broad-gviz} shows a highly enriched H3K36me3
region covering the gene body, as expected.

\hypertarget{peak-quality-control}{%
\subsection{Peak quality control}\label{peak-quality-control}}

Peak calling is not a mathematically defined procedure; it is impossible
to unambiguously define what a ``peak'' is. Therefore all of the peak
calling procedures use heuristics, and statistical models which have been
shown to work well in specific use cases.
After peak calling, it is always necessary to check
whether the defined peaks really are located in enriched regions, and in addition,
use prior knowledge to ascertain whether the peaks correspond to known biology.

Peak calling can falsely identify enriched regions if the input
sample is not sequenced to the proper depth. Because the input samples
correspond to \textbf{de facto} whole genome sequencing, and the ChIP procedure
enriches for a subset of the genome, it can often happen that many regions
in the genome are not sufficiently covered by the Input sample.
Such variability in the signal profile of Input samples can cause a region
to be defined as a peak, enriched in the ChIP sample, while in reality it is depleted in the
Input, due to under-sampling. For example, the figure in the previous chapter, showing
an enriched region H3K36me3 over a gene body, shows a large depletion in the Input
sample over the same region. Such depletion should be a concern and merit
further investigation.

The quality of enrichment can be checked by calculating the percentage of reads within peaks for both
ChIP and Input samples. ChIP samples should have a high percentage of reads in peaks,
while for the input samples, the percentage of reads should correspond to the
percentage of genome covered by peaks.

For transcription factor ChIP experiments, an important control is to determine whether
the peak regions contain sequences which are known to be bound
by the corresponding transcription factor - whether they contain
known transcription factor binding motifs.
Transcription factor binding motifs are sequence models which model the propensity
of binding DNA sequences.
Such sequence models can be downloaded from public databases and compared to see
whether there is a positional enrichment around our peaks.

We will now calculate the percentage of reads within peaks for the H3K36me3 experiment.
Subsequently, we will download the known CTCF sequence model, and compare it
to our peak regions.

\hypertarget{percentage-of-reads-in-peaks}{%
\subsubsection{Percentage of reads in peaks}\label{percentage-of-reads-in-peaks}}

To calculate the reads in peaks, we will firstly extract the number of reads
in each tilling window from the \texttt{normR} produced fit object.
This is done using the \texttt{getCounts()} function.
We will then use the q-value to define which tilling windows correspond
to peaks, and count the number of reads within and outside peaks.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# extract, per tilling window, counts from the fit object}
\NormalTok{h3k36_counts =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{getCounts}\NormalTok{(h3k36_fit))}
    
\CommentTok{# change the column names of the data.frame}
\KeywordTok{colnames}\NormalTok{(h3k36_counts) =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'Input'}\NormalTok{,}\StringTok{'H3K36me3'}\NormalTok{)}
    
\CommentTok{# extract the q-value corresponding to each bin}
\NormalTok{h3k36_counts}\OperatorTok{$}\NormalTok{qvalue =}\StringTok{ }\KeywordTok{getQvalues}\NormalTok{(h3k36_fit)}
    
\CommentTok{# define which regions are peaks using a q value cutoff}
\NormalTok{h3k36_counts}\OperatorTok{$}\NormalTok{enriched[}\KeywordTok{is.na}\NormalTok{(h3k36_counts}\OperatorTok{$}\NormalTok{qvalue)]  =}\StringTok{ 'Not Peak'}
\NormalTok{h3k36_counts}\OperatorTok{$}\NormalTok{enriched[h3k36_counts}\OperatorTok{$}\NormalTok{qvalue }\OperatorTok{>}\StringTok{ }\FloatTok{0.05}\NormalTok{]  =}\StringTok{ 'Not Peak'}
\NormalTok{h3k36_counts}\OperatorTok{$}\NormalTok{enriched[h3k36_counts}\OperatorTok{$}\NormalTok{qvalue }\OperatorTok{<=}\StringTok{ }\FloatTok{0.05}\NormalTok{] =}\StringTok{ 'Peak'}
    
\CommentTok{# remove the q value column}
\NormalTok{h3k36_counts}\OperatorTok{$}\NormalTok{qvalue =}\StringTok{ }\OtherTok{NULL} 
    
\CommentTok{# reshape the data.frame into a long format}
\NormalTok{h3k36_counts_df =}\StringTok{ }\NormalTok{tidyr}\OperatorTok{::}\KeywordTok{pivot_longer}\NormalTok{(}
    \DataTypeTok{data      =}\NormalTok{ h3k36_counts, }
    \DataTypeTok{cols      =} \OperatorTok{-}\NormalTok{enriched,}
    \DataTypeTok{names_to  =} \StringTok{'experiment'}\NormalTok{,}
    \DataTypeTok{values_to =} \StringTok{'counts'}
\NormalTok{)}
    
\CommentTok{# sum the number of reads in the Peak and Not Peak regions}
\NormalTok{h3k36_counts_df =}\StringTok{ }\KeywordTok{group_by}\NormalTok{(}\DataTypeTok{.data =}\NormalTok{ h3k36_counts_df, experiment, enriched)}
\NormalTok{h3k36_counts_df =}\StringTok{ }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{.data =}\NormalTok{ h3k36_counts_df, }\DataTypeTok{num_of_reads =} \KeywordTok{sum}\NormalTok{(counts))}
    
\CommentTok{# calculate the percentage of reads.}
\NormalTok{h3k36_counts_df       =}\StringTok{ }\KeywordTok{group_by}\NormalTok{(}\DataTypeTok{.data =}\NormalTok{ h3k36_counts_df, experiment)}
\NormalTok{h3k36_counts_df       =}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{.data =}\NormalTok{ h3k36_counts_df, }\DataTypeTok{total=}\KeywordTok{sum}\NormalTok{(num_of_reads))}
\NormalTok{h3k36_counts_df}\OperatorTok{$}\NormalTok{percentage =}\StringTok{ }\KeywordTok{with}\NormalTok{(h3k36_counts_df, }\KeywordTok{round}\NormalTok{(num_of_reads}\OperatorTok{/}\NormalTok{total,}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 5
## # Groups:   experiment [2]
##   experiment enriched num_of_reads  total percentage
##   <chr>      <chr>           <int>  <int>      <dbl>
## 1 H3K36me3   Not Peak        67623 158616      0.43 
## 2 H3K36me3   Peak            90993 158616      0.570
## 3 Input      Not Peak       492369 648196      0.76 
## 4 Input      Peak           155827 648196      0.24
\end{verbatim}

We can now plot the percentage of reads in peaks:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}
    \DataTypeTok{data =}\NormalTok{ h3k36_counts_df, }
    \KeywordTok{aes}\NormalTok{(}
        \DataTypeTok{x =}\NormalTok{ experiment, }
        \DataTypeTok{y =}\NormalTok{ percentage, }
        \DataTypeTok{fill =}\NormalTok{ enriched}
\NormalTok{    )) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat=}\StringTok{'identity'}\NormalTok{, }\DataTypeTok{position=}\StringTok{'dodge'}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme_bw}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}
        \DataTypeTok{axis.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{10}\NormalTok{, }\DataTypeTok{face=}\StringTok{'bold'}\NormalTok{),}
        \DataTypeTok{axis.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{12}\NormalTok{,}\DataTypeTok{face=}\StringTok{"bold"}\NormalTok{),}
        \DataTypeTok{plot.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{hjust =} \FloatTok{0.5}\NormalTok{)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{xlab}\NormalTok{(}\StringTok{'Experiment'}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{ylab}\NormalTok{(}\StringTok{'Percetage of reads in region'}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{'Percentage of reads in peaks for H3K36me3'}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_fill_manual}\NormalTok{(}\DataTypeTok{values=}\KeywordTok{c}\NormalTok{(}\StringTok{'gray'}\NormalTok{,}\StringTok{'red'}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{09-chip-seq-analysis_files/figure-latex/peak-quality-counts-plot-1} 

}

\caption{Percentage of ChIP reads in called peaks. Higher percentage indicates higher ChIP quality.}\label{fig:peak-quality-counts-plot}
\end{figure}

The Figure \ref{fig:peak-quality-counts-plot} shows that the ChIP sample is
clearly enriched in the peak regions.
The percentage of reads in peaks will depend on the quality of the antibody (strength of
enrichment), and the size of peaks which are bound by the protein of interest.
If the total size of peaks is small, relative to the genome size, we can expect that
the percentage of reads in peaks will be small.

\hypertarget{dna-motifs-on-peaks}{%
\subsubsection{DNA motifs on peaks}\label{dna-motifs-on-peaks}}

\index{DNA motif}

Well-studied transcription factors have publicly available transcription
factor binding motifs.
If such a model is available for our transcription factor of interest, we
can use it to check the quality of our ChIP data.
Two common measures are used for this purpose:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Percentage of peaks containing the motif of interest.
\item
  Positional distribution of the motif - the distribution of motif locations should be centered on the peak centers.
\end{enumerate}

\hypertarget{representing-motifs-as-matrices}{%
\paragraph{Representing motifs as matrices}\label{representing-motifs-as-matrices}}

In order to calculate the percentage of CTCF peaks which contain a known CTCF
motif. We need to find the CTCF motif and have the computational tools to search for that motif. The DNA binding motifs can be extracted from the \texttt{MotifDB} Bioconductor
database\index{R Packages!\texttt{MotifDB}}. The \texttt{MotifDB} is an agglomeration of multiple motif databases.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the MotifDB package}
\KeywordTok{library}\NormalTok{(MotifDb)}

\CommentTok{# fetch the CTCF motif from the data base}
\NormalTok{motifs =}\StringTok{ }\KeywordTok{query}\NormalTok{(}\KeywordTok{query}\NormalTok{(MotifDb, }\StringTok{'Hsapiens'}\NormalTok{), }\StringTok{'CTCF'}\NormalTok{)}

\CommentTok{# show all available ctcf motifs}
\NormalTok{motifs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## MotifDb object of length 12
## | Created from downloaded public sources: 2013-Aug-30
## | 12 position frequency matrices from 8 sources:
## |        HOCOMOCOv10:    2
## | HOCOMOCOv11-core-A:    2
## |        JASPAR_2014:    1
## |        JASPAR_CORE:    1
## |       SwissRegulon:    2
## |         jaspar2016:    1
## |         jaspar2018:    2
## |          jolma2013:    1
## | 1 organism/s
## |           Hsapiens:   12
## Hsapiens-SwissRegulon-CTCFL.SwissRegulon 
## Hsapiens-SwissRegulon-CTCF.SwissRegulon 
## Hsapiens-HOCOMOCOv10-CTCFL_HUMAN.H10MO.A 
## Hsapiens-HOCOMOCOv10-CTCF_HUMAN.H10MO.A 
## Hsapiens-HOCOMOCOv11-core-A-CTCFL_HUMAN.H11MO.0.A 
## ...
## Hsapiens-JASPAR_2014-CTCF-MA0139.1 
## Hsapiens-jaspar2016-CTCF-MA0139.1 
## Hsapiens-jaspar2018-CTCF-MA0139.1 
## Hsapiens-jaspar2018-CTCFL-MA1102.1 
## Hsapiens-jolma2013-CTCF
\end{verbatim}

We will extract the CTCF from the \texttt{MotifDB} \citep{khan_2018} database.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# based on the MotifDB version, the location of the CTCF motif}
\CommentTok{# might change, if you do not get the expected results please try}
\CommentTok{# to subset with different indices}
\NormalTok{ctcf_motif  =}\StringTok{ }\NormalTok{motifs[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

The motifs are usually represented as matrices of 4-by-N dimensions. In the matrix, each of 4 rows corresponds to one nucleotide (A, C, G, T).
The number of columns designates the width of the region bound by the transcription factor or the length of the motif that the protein recognizes.
Each element of the matrix contains the probability of observing the corresponding
nucleotide on this position.
For example, for following the CTCF matrix in Table \ref{tab:peakqualityshow}, the probability of observing a thymine at
the first position of the motif,\(p_{i=1,k=4}\) , is 0.57 (1st column, 4th row).
Such a matrix, where each column is a probability distribution over a sequence of nucleotides,
is called a position frequency matrix (PFM)\index{position frequency matrix (PFM)}. In some sources, this matrix is also called ``position probability matrix (PPM)''. One way to construct such matrices is to get experimentally verified sequences that are bound by the protein of interest and then to use a motif-finding algorithm.

\begin{table}

\caption{\label{tab:peakqualityshow}Position Frequency Matrix (PFM) for the CTCF motif}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lrrrrrrrrrrrrrrrrrrrr}
\toprule
  & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 & 18 & 19 & 20\\
\midrule
A & 0.17 & 0.23 & 0.29 & 0.10 & 0.33 & 0.06 & 0.05 & 0.04 & 0.02 & 0 & 0.25 & 0.00 & 0 & 0.05 & 0.25 & 0.00 & 0.17 & 0 & 0.02 & 0.19\\
C & 0.42 & 0.28 & 0.30 & 0.32 & 0.11 & 0.33 & 0.56 & 0.00 & 0.96 & 1 & 0.67 & 0.69 & 1 & 0.04 & 0.07 & 0.42 & 0.15 & 0 & 0.06 & 0.43\\
G & 0.25 & 0.23 & 0.26 & 0.27 & 0.42 & 0.55 & 0.05 & 0.83 & 0.01 & 0 & 0.03 & 0.00 & 0 & 0.02 & 0.53 & 0.55 & 0.05 & 1 & 0.87 & 0.15\\
T & 0.16 & 0.27 & 0.15 & 0.31 & 0.14 & 0.06 & 0.33 & 0.13 & 0.00 & 0 & 0.06 & 0.31 & 0 & 0.89 & 0.15 & 0.03 & 0.62 & 0 & 0.05 & 0.23\\
\bottomrule
\end{tabular}}
\end{table}

Such a matrix can be used to calculate the probability that the transcription
factor will bind to any given sequence. However, computationally, it is easier to work with summation rather than multiplication. In addition, the simple probabilistic model does not take the background probability of observing a certain base in a given position. We can correct for background base frequencies by dividing the individual probability, \(p_{i,k}\) in each cell of the matrix by the background base probability for a given base, \(B_k\). We can then take the logarithm of that quantity to calculate a log-likelihood and bring everything to log-scale as follows \(Score_{i,k}=log_2(p_{i,k}/B_k)\). We can now calculate a score for any given
sequence by summing up the base-position-specific scores we obtain from the log-scaled matrix. This matrix is formally called "position-specific scoring matrix (PSSM) or position-specific weight matrix (PWM). We can use this matrix to scan the genome in a sliding window manner and calculate a score for each window. Usually, a cutoff value is needed to call a motif hit. The higher the score you get from the PWM for a particular sequence, the better it is. The traditional algorithms we will use in the following sections use 80\% of the maximum rescaled score you can obtain from a PWM as the default cutoff for a hit. The rescaling is simple min-max rescaling where you rescale the score by subtracting the minimum score and dividing that by \(max(PWMscore)-min(PWMscore)\). The motif scanning approach is illustrated in Figure \ref{fig:FigurePWMScanning}. In this example, ACACT is not considered a hit because its score only corresponds to only \(15.6\) \% of the rescaled maximum score.



\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{./Figures/PWMScanning} 

}

\caption{PWM scanning principle. A genomic sequence is scanned by a PWM matrix. This matrix is used to measure how likely it is that the transcription factor will bind each nucleotide in each position. Here we are looking at how likely it is that our TF will bind to the sequence ACACT. The score for this sequence is -3.6. The maximal score obtainable by the PWM is 7.2 and minimum is -5.6. After min-max rescaling, -3.6 corresponds to a 15\% score and ACACT is not considered a hit.}\label{fig:FigurePWMScanning}
\end{figure}

\hypertarget{representing-motifs-as-sequence-logos}{%
\paragraph{Representing motifs as sequence logos}\label{representing-motifs-as-sequence-logos}}

Using the PFM, we can calculate the information content of each position in the matrix.
The information content quantifies the contribution of each nucleotide to the
cumulative binding preference. This tells us how important each nucleotide is for the binding. It additionally allows us to visually represent the probability matrices as sequence logos.
The information content is quantified as relative entropy. It ranges from \(0\), no information,
to \(2\), maximal information. For a column in the PFM, the entropy is calculated as follows:

\[
entropy = -\sum\limits_{k=1}^n p_{i,k}\log_2(p_{i,k}) 
\]
\(p_{i,k}\) is the probability of observing base \(k\) in the column \(i\) of the PFM. In other words, \(p_{i,k}\) is simply the value of the cell in the PFM. The entropy value is high when the probabilities of each base are similar and low when it is much more probable that only one base occur in a given column. The relative portion comes from the fact that we compare the entropy we calculated for a column to the maximum entropy we can obtain. If the all bases are equally likely for a position in the PFM, then we will have the maximum entropy and we compare our original entropy to that maximum entropy. The maximum entropy is simply \(log_2{n}\) where \(n\) is number of letters in the alphabet. In our case we have 4 letters A,C,G and T. The information content is then simply subtracting the observed entropy for a column from the maximum entropy, which translates to the following equation:

\[
IC=log_2(n)+\sum\limits_{k=1}^n p_{i,k}\log_2(p_{i,k}) 
\]
The information content, \(IC\), in the preceding equation, will be high if a base has a high probability of occurrence and low if all bases are more or less equally likely to occur.

We can visualize the matrix by visualizing the letters weighted by their probabilities in the PFM. This approach is shown on the left-hand side of Figure \ref{fig:peak-quality-seqLogo-plot}. In addition, we can also calculate the information content per column to weight the probabilities. This means that the columns that have very frequent letters will be higher. This approach is shown on the right-hand side of Figure \ref{fig:peak-quality-seqLogo-plot}. We will use below the \texttt{seqLogo} package to visualize the CTCF motif in the two different ways we described above.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{./Figures/CTCF_Motif} 

}

\caption{CTCF sequence motif visualized as a sequence logo. Y-axis ranges from zero to two, and corresponds to the amount of information each base in the motif contributes to the overall motif. The larger the letter, the greater the probability of observing just one defined base on the designated position.}\label{fig:peak-quality-seqLogo-plot}
\end{figure}

\hypertarget{percentage-of-peaks-with-the-motif}{%
\paragraph{Percentage of peaks with the motif}\label{percentage-of-peaks-with-the-motif}}

Since we now understand how DNA motifs are used we can start annotating the CTCF peaks with the motif. First, we will extend the peak
regions to +/- 200 bp around the peak center.
Because the average fragment size is 200 bp, 400 nucleotides is the
expected variation in the position of the true binding location.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# extend the peak regions}
\NormalTok{ctcf_peaks_resized =}\StringTok{ }\KeywordTok{resize}\NormalTok{(ctcf_peaks, }\DataTypeTok{width =} \DecValTok{400}\NormalTok{, }\DataTypeTok{fix =} \StringTok{'center'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we use the \texttt{BSgenome}\index{R Packages!\texttt{BSgenome}} package to
extract the sequences corresponding to the peak regions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the human genome sequence}
\KeywordTok{library}\NormalTok{(BSgenome.Hsapiens.UCSC.hg38)}

\CommentTok{# extract the sequences around the peaks}
\NormalTok{seq =}\StringTok{ }\KeywordTok{getSeq}\NormalTok{(BSgenome.Hsapiens.UCSC.hg38, ctcf_peaks_resized)}
\end{Highlighting}
\end{Shaded}

Once we have extracted the sequences, we can use the CTCF motif to
scan each sequence and determine the probability of CTCF binding.
For this we use the \texttt{TFBSTools}\index{R Packages!\texttt{TFBSTools}} \citep{TFBSTools} package.

We first convert the raw probability matrix into a \texttt{PWMMatrix} object,
which can then be used for efficient scanning.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the TFBS tools package}
\KeywordTok{library}\NormalTok{(TFBSTools)}

\CommentTok{# convert the matrix into a PWM object}
\NormalTok{ctcf_pwm =}\StringTok{ }\KeywordTok{PWMatrix}\NormalTok{(}
    \DataTypeTok{ID =} \StringTok{'CTCF'}\NormalTok{, }
    \DataTypeTok{profileMatrix =}\NormalTok{ ctcf_motif}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can now use the \texttt{searchSeq()} function to scan each sequence for the motif occurrence.
Because the motif matrices are given a continuous binding score, we need to set a cutoff to
determine when a sequence contains the motif, and when it doesn't.
The cutoff is set by determining the maximal possible score produced by the motif matrix;
a percentage of that score is then taken as the threshold value.
For example, if the best sequence would have a score of 1.4 of being bound,
then we define a threshold of 80\% of 1.4, which is 1.12; and any sequence which
scores less than 1.12 would not be marked as being bound by the protein.

For the CTCF, we mark any peak containing a sequence with \textgreater{} 80\% of the maximal rescaled score or ``relative score'' as a positive hit.

\begin{verbatim}
##   seqnames source feature start end absScore relScore strand   ID
## 1        1   TFBS    TFBS    44  63     11.9    0.921      - CTCF
## 2        1   TFBS    TFBS   102 121     11.0    0.839      - CTCF
## 3        2   TFBS    TFBS   151 170     11.5    0.881      + CTCF
## 4        4   TFBS    TFBS   294 313     11.9    0.921      - CTCF
## 5        4   TFBS    TFBS   352 371     11.0    0.839      - CTCF
## 6        5   TFBS    TFBS   164 183     10.9    0.831      - CTCF
\end{verbatim}

A common diagnostic plot is to graph a reverse cumulative distribution of
peak occurrences.
On the x-axis we rank the peaks, with the most highly enriched peak in the
first position, and the least enriched peak in the last position.
We then walk from the lowest to the highest ranking and measure the
percentage of peaks containing the motif.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# label which peaks contain CTCF motifs}
\NormalTok{motif_hits_df =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \DataTypeTok{peak_order     =} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(ctcf_peaks)}
\NormalTok{)}
\NormalTok{motif_hits_df}\OperatorTok{$}\NormalTok{contains_motif =}\StringTok{ }\NormalTok{motif_hits_df}\OperatorTok{$}\NormalTok{peak_order }\OperatorTok{%in%}\StringTok{ }\NormalTok{hits}\OperatorTok{$}\NormalTok{seqnames}
\NormalTok{motif_hits_df =}\StringTok{ }\NormalTok{motif_hits_df[}\KeywordTok{order}\NormalTok{(}\OperatorTok{-}\NormalTok{motif_hits_df}\OperatorTok{$}\NormalTok{peak_order),]}

\CommentTok{# calculate the percentage of peaks with motif for peaks of descending strength}
\NormalTok{motif_hits_df}\OperatorTok{$}\NormalTok{perc_peaks =}\StringTok{ }\KeywordTok{with}\NormalTok{(motif_hits_df, }
                                \KeywordTok{cumsum}\NormalTok{(contains_motif) }\OperatorTok{/}\StringTok{ }\KeywordTok{max}\NormalTok{(peak_order))}
\NormalTok{motif_hits_df}\OperatorTok{$}\NormalTok{perc_peaks =}\StringTok{ }\KeywordTok{round}\NormalTok{(motif_hits_df}\OperatorTok{$}\NormalTok{perc_peaks, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can now visualize the percentage of peaks with matching CTCF motif.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the cumulative distribution of motif hit percentages}
\KeywordTok{ggplot}\NormalTok{(}
\NormalTok{    motif_hits_df, }
    \KeywordTok{aes}\NormalTok{(}
        \DataTypeTok{x =}\NormalTok{ peak_order, }
        \DataTypeTok{y =}\NormalTok{ perc_peaks}
\NormalTok{    )) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{size=}\DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}
    \DataTypeTok{axis.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{10}\NormalTok{, }\DataTypeTok{face=}\StringTok{'bold'}\NormalTok{),}
    \DataTypeTok{axis.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{14}\NormalTok{,}\DataTypeTok{face=}\StringTok{"bold"}\NormalTok{),}
    \DataTypeTok{plot.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{hjust =} \FloatTok{0.5}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{'Peak rank'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{'Percetage of peaks with motif'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{'Percentage of CTCF peaks with the CTCF motif'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{09-chip-seq-analysis_files/figure-latex/peak-quality-scan-dist-plot-1} 

}

\caption{Percentage of peaks containing the motif. Higher percentage indicates a better ChIP-experiment, and a better peak calling procedure.}\label{fig:peak-quality-scan-dist-plot}
\end{figure}

Figure \ref{fig:peak-quality-scan-dist-plot}
shows that, when we take all peaks into account, \textasciitilde45\% of
the peaks contain a CTCF motif.
This is an excellent percentage and indicates a high-quality ChIP experiment.
Our inability to locate the motif in \textasciitilde50\% of the sequences does not
necessarily need to be a consequence of a poor experiment; sometimes
it is a result of the molecular mechanism by which the transcription factor
binds. If a transcription factor has multiple binding modes, which are context
dependent, for example, if the transcription factor binds indirectly to
a subset of regions, through
an interacting partner, we do not have to observe a motif.

\hypertarget{motif-localization}{%
\subsubsection{Motif localization}\label{motif-localization}}

If the ChIP experiment was performed properly, we would expect the motif
to be localized just below the summit of each peak.
By plotting the motif localization around ChIP peaks, we are quantifying
the uncertainty in the peak location.

We will firstly resize our peaks into regions around +/−1-kb around the peak
center.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# resize the region around peaks to +/- 1kb}
\NormalTok{ctcf_peaks_resized =}\StringTok{ }\KeywordTok{resize}\NormalTok{(ctcf_peaks, }\DataTypeTok{width =} \DecValTok{2000}\NormalTok{, }\DataTypeTok{fix=}\StringTok{'center'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we perform the motif localization, as before.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fetch the sequence}
\NormalTok{seq =}\StringTok{ }\KeywordTok{getSeq}\NormalTok{(BSgenome.Hsapiens.UCSC.hg38,ctcf_peaks_resized)}

\CommentTok{# convert the motif matrix to PWM, and scan the peaks}
\NormalTok{ctcf_pwm    =}\StringTok{ }\KeywordTok{PWMatrix}\NormalTok{(}\DataTypeTok{ID =} \StringTok{'CTCF'}\NormalTok{, }\DataTypeTok{profileMatrix =}\NormalTok{ ctcf_motif)}
\NormalTok{hits =}\StringTok{ }\KeywordTok{searchSeq}\NormalTok{(ctcf_pwm, seq, }\DataTypeTok{min.score=}\StringTok{"80%"}\NormalTok{, }\DataTypeTok{strand=}\StringTok{"*"}\NormalTok{)}
\NormalTok{hits =}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(hits)}
\end{Highlighting}
\end{Shaded}

We now construct a plot, where the
X-axis represents the +/- 1000 nucleotides around the peak, while the
Y-axis shows the motif enrichment at each position.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set the position relative to the start}
\NormalTok{hits}\OperatorTok{$}\NormalTok{position =}\StringTok{ }\NormalTok{hits}\OperatorTok{$}\NormalTok{start }\OperatorTok{-}\StringTok{ }\DecValTok{1000} 

\CommentTok{# plot the motif hits around peaks}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{hits, }\KeywordTok{aes}\NormalTok{(position)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{size=}\DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_vline}\NormalTok{(}\DataTypeTok{xintercept =} \DecValTok{0}\NormalTok{, }\DataTypeTok{linetype=}\DecValTok{2}\NormalTok{, }\DataTypeTok{color=}\StringTok{'red'}\NormalTok{, }\DataTypeTok{size=}\DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{'Position around the CTCF peaks'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{'Per position percentage}\CharTok{\textbackslash{}n}\StringTok{of motif occurence'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}
    \DataTypeTok{axis.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{10}\NormalTok{, }\DataTypeTok{face=}\StringTok{'bold'}\NormalTok{),}
    \DataTypeTok{axis.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{14}\NormalTok{,}\DataTypeTok{face=}\StringTok{"bold"}\NormalTok{),}
    \DataTypeTok{plot.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{hjust =} \FloatTok{0.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{09-chip-seq-analysis_files/figure-latex/chip-quality-motifloc-plot-1} 

}

\caption{Transcription factor sequence motif localization with respect to the defined binding sites.}\label{fig:chip-quality-motifloc-plot}
\end{figure}

We can in Figure \ref{fig:chip-quality-motifloc-plot}, see that the bulk of motif
hits are found in a region of \(+/-\) 250 bp around the peak centers.
This means that the peak calling procedure was quite precise.

\hypertarget{peak-annotation}{%
\subsection{Peak annotation}\label{peak-annotation}}

As the final step of quality control we will visualize the distribution
of peaks in different functional genomic regions.
The purpose of the analysis is to check whether the location of the peaks
conforms our prior knowledge.
This analysis is equivalent to constructing distributions for reads.

Firstly we download the human gene models and construct the annotation hierarchy\index{annotation hierarchy}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# download the annotation}
\NormalTok{hub =}\StringTok{ }\KeywordTok{AnnotationHub}\NormalTok{()}
\NormalTok{gtf =}\StringTok{ }\NormalTok{hub[[}\StringTok{'AH61126'}\NormalTok{]]}
\KeywordTok{seqlevels}\NormalTok{(gtf, }\DataTypeTok{pruning.mode=}\StringTok{'coarse'}\NormalTok{) =}\StringTok{ '21'}
\KeywordTok{seqlevels}\NormalTok{(gtf, }\DataTypeTok{pruning.mode=}\StringTok{'coarse'}\NormalTok{) =}\StringTok{ }\KeywordTok{paste0}\NormalTok{(}\StringTok{'chr'}\NormalTok{, }\KeywordTok{seqlevels}\NormalTok{(gtf))}

\CommentTok{# create the annotation hierarchy}
\NormalTok{annotation_list =}\StringTok{ }\KeywordTok{GRangesList}\NormalTok{(}
  \DataTypeTok{tss    =} \KeywordTok{promoters}\NormalTok{(}\KeywordTok{subset}\NormalTok{(gtf, type}\OperatorTok{==}\StringTok{'gene'}\NormalTok{), }\DecValTok{1000}\NormalTok{, }\DecValTok{1000}\NormalTok{),}
  \DataTypeTok{exon   =} \KeywordTok{subset}\NormalTok{(gtf, type}\OperatorTok{==}\StringTok{'exon'}\NormalTok{),}
  \DataTypeTok{intron =} \KeywordTok{subset}\NormalTok{(gtf, type}\OperatorTok{==}\StringTok{'gene'}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The following function finds the genomic location of each peak, annotates
the peaks using the hierarchical prioritization,
and calculates the summary statistics.

The function contains four major parts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Creating a disjoint set of peak regions.
\item
  Finding the overlapping annotation for each peak.
\item
  Annotating each peak with the corresponding annotation class.
\item
  Calculating summary statistics
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# function which annotates the location of each peak}
\NormalTok{annotatePeaks =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(peaks, annotation_list, name)\{}
  
  \CommentTok{# ------------------------------------------------ #}
  \CommentTok{# 1. getting disjoint regions}
  \CommentTok{# collapse touching enriched regions}
\NormalTok{  peaks =}\StringTok{ }\KeywordTok{reduce}\NormalTok{(peaks)}
  
  \CommentTok{# ------------------------------------------------ #}
  \CommentTok{# 2. overlapping peaks and annotation}
  \CommentTok{# find overlaps between the peaks and annotation_list}
\NormalTok{  result =}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{findOverlaps}\NormalTok{(peaks, annotation_list))}
  
  \CommentTok{# ------------------------------------------------ #}
  \CommentTok{# 3. annotating peaks}
  \CommentTok{# fetch annotation names}
\NormalTok{  result}\OperatorTok{$}\NormalTok{annotation =}\StringTok{ }\KeywordTok{names}\NormalTok{(annotation_list)[result}\OperatorTok{$}\NormalTok{subjectHits]}
  
  \CommentTok{# rank by annotation precedence}
\NormalTok{  result =}\StringTok{ }\NormalTok{result[}\KeywordTok{order}\NormalTok{(result}\OperatorTok{$}\NormalTok{subjectHits),]    }
  
  \CommentTok{# remove overlapping annotations}
\NormalTok{  result =}\StringTok{ }\KeywordTok{subset}\NormalTok{(result, }\OperatorTok{!}\KeywordTok{duplicated}\NormalTok{(queryHits))}
  
  \CommentTok{# ------------------------------------------------ #}
  \CommentTok{# 4. calculating statistics}
  \CommentTok{# count the number of peaks in each annotation category}
\NormalTok{  result =}\StringTok{ }\KeywordTok{group_by}\NormalTok{(}\DataTypeTok{.data =}\NormalTok{ result, annotation)}
\NormalTok{  result =}\StringTok{ }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{.data =}\NormalTok{ result, }\DataTypeTok{counts =} \KeywordTok{length}\NormalTok{(annotation))}
  
  \CommentTok{# fetch the number of intergenic peaks}
\NormalTok{  result =}\StringTok{ }\KeywordTok{rbind}\NormalTok{(result, }
                 \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{annotation =} \StringTok{'intergenic'}\NormalTok{, }
                            \DataTypeTok{counts     =} \KeywordTok{length}\NormalTok{(peaks) }\OperatorTok{-}\StringTok{ }\KeywordTok{sum}\NormalTok{(result}\OperatorTok{$}\NormalTok{counts)))}
  
\NormalTok{  result}\OperatorTok{$}\NormalTok{frequency  =}\StringTok{ }\KeywordTok{with}\NormalTok{(result, }\KeywordTok{round}\NormalTok{(counts}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(counts),}\DecValTok{2}\NormalTok{))}
\NormalTok{  result}\OperatorTok{$}\NormalTok{experiment =}\StringTok{ }\NormalTok{name}
  
  \KeywordTok{return}\NormalTok{(result)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Using the above defined \texttt{annotatePeaks()} function we will now annotate CTCF
and H3K36me3 peaks. Firstly we create a list which contains both CTCF and H3K36me3 peaks.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{peak_list =}\StringTok{ }\KeywordTok{list}\NormalTok{(}
    \DataTypeTok{CTCF     =}\NormalTok{ ctcf_peaks, }
    \DataTypeTok{H3K36me3 =}\NormalTok{ h3k36_peaks}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Using the \texttt{lapply()} function we apply the \texttt{annotatePeaks()} function
on each element of the list.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate the distribution of peaks in annotation for each experiment}
\NormalTok{annot_peaks_list =}\StringTok{ }\KeywordTok{lapply}\NormalTok{(}\KeywordTok{names}\NormalTok{(peak_list), }\ControlFlowTok{function}\NormalTok{(peak_name)\{}
  \KeywordTok{annotatePeaks}\NormalTok{(peak_list[[peak_name]], annotation_list, peak_name)}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

We use the \texttt{dplyr::bind\_rows()} function to combine the CTCF and H3K36me3 annotation
statistics into one data frame.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# combine a list of data.frames into one data.frame}
\NormalTok{annot_peaks_df =}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{bind_rows}\NormalTok{(annot_peaks_list)}
\end{Highlighting}
\end{Shaded}

And visualize the results as bar plots. Resulting plot is in Figure \ref{fig:peak-annotation-plot}, which shows that the H3K36me3 peaks are
located preferentially in gene bodies, as expected, while the CTCF peaks are
found preferentially in introns.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the distribution of peaks in genomic features}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ annot_peaks_df, }
       \KeywordTok{aes}\NormalTok{(}
           \DataTypeTok{x    =}\NormalTok{ experiment, }
           \DataTypeTok{y    =}\NormalTok{ frequency, }
           \DataTypeTok{fill =}\NormalTok{ annotation}
\NormalTok{        )) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat=}\StringTok{'identity'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{'Set2'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}
    \DataTypeTok{axis.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{18}\NormalTok{, }\DataTypeTok{face=}\StringTok{'bold'}\NormalTok{),}
    \DataTypeTok{axis.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size=}\DecValTok{14}\NormalTok{,}\DataTypeTok{face=}\StringTok{"bold"}\NormalTok{),}
    \DataTypeTok{plot.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{hjust =} \FloatTok{0.5}\NormalTok{))  }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{'Peak distribution in}\CharTok{\textbackslash{}n}\StringTok{genomic regions'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{'Experiment'}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{'Frequency'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{09-chip-seq-analysis_files/figure-latex/peak-annotation-plot-1} 

}

\caption{Enrichment of transcription factor or histone modifications in functional genomic features.}\label{fig:peak-annotation-plot}
\end{figure}

\hypertarget{motif-discovery}{%
\section{Motif discovery}\label{motif-discovery}}

The first analysis step downstream of peak calling is motif discovery.
Motif discovery is a procedure of finding enriched sets of similar short sequences
in a large sequence dataset. In our case the large sequence dataset are
sequences around ChIP peaks, while the short sequence sets are the transcription
factor binding sites.

There are two types of motif discovery tools: supervised and unsupervised.
Supervised tools require explicit positive (we are certain that the motif is enriched), and negative sequence sets (we are certain that the motif is not enriched), and
then search for relative enrichment of short motifs in the foreground versus
the background.
Unsupervised models, on the other hand, require only a set of positive sequences,
and then compare motif abundance to a statistically constructed background set.

Due to the combinatorial nature of the procedure, motif discovery is
computationally expensive. It is therefore often performed on a subset of the
highest-quality peaks. In this tutorial we will use the \texttt{rGADEM}\index{R Packages!\texttt{rGADEM}}
package for motif discovery.
\texttt{rGADEM} is an unsupervised, stochastic motif discovery tools, which uses
sampling with subsequent enrichment analysis to find over-represented sequence
motifs.

We will firstly load our CTCF peaks, and convert them to a GRanges object.
We will then select the top 500 peaks, and extract the DNA sequence, which
will be used as input for the motif discovery. Nearby ChIP peaks can have overlapping coordinates. After selection, overlapping CTCF peaks have to be merged using the \texttt{reduce()} function from the \texttt{GenomicRanges} package. If we do not execute this step, we will include the same sequence multiple times in the sequence set, and artificially enrich DNA patterns.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read the CTCF peaks created in the peak calling part of the tutorial}
\NormalTok{ctcf_peaks =}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_path, }\StringTok{'CTCF_peaks.txt'}\NormalTok{), }\DataTypeTok{header=}\OtherTok{TRUE}\NormalTok{)}

\CommentTok{# convert the peaks into a GRanges object}
\NormalTok{ctcf_peaks =}\StringTok{ }\KeywordTok{makeGRangesFromDataFrame}\NormalTok{(ctcf_peaks, }\DataTypeTok{keep.extra.columns =} \OtherTok{TRUE}\NormalTok{)}

\CommentTok{# order the peaks by qvalue, and take top 250 peaks}
\NormalTok{ctcf_peaks =}\StringTok{ }\NormalTok{ctcf_peaks[}\KeywordTok{order}\NormalTok{(ctcf_peaks}\OperatorTok{$}\NormalTok{qvalue)]}
\NormalTok{ctcf_peaks =}\StringTok{ }\KeywordTok{head}\NormalTok{(ctcf_peaks, }\DataTypeTok{n =} \DecValTok{500}\NormalTok{)}

\CommentTok{# merge nearby CTCF peaks}
\NormalTok{ctcf_peaks =}\StringTok{ }\KeywordTok{reduce}\NormalTok{(ctcf_peaks)}
\end{Highlighting}
\end{Shaded}

Create a region of \(+/-\) 50 bp around the center of the peaks,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# expand the CTCF peaks}
\NormalTok{ctcf_peaks_resized =}\StringTok{ }\KeywordTok{resize}\NormalTok{(ctcf_peaks, }\DataTypeTok{width =} \DecValTok{50}\NormalTok{, }\DataTypeTok{fix=}\StringTok{'center'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

and extract the genomic sequence.

We are now ready to run the motif discovery.Firstly we load the \texttt{rGADEM} package:

To run the motif discovery, we call the \texttt{GADEM()} function. with the
extracted DNA sequences. In addition to the DNA sequences, we need to
specify two parameters:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{seed} - the random number generator seed, which will make the analysis
  reproducible.
\item
  \textbf{nmotifs} - the number of motifs to look for.
\end{enumerate}

\begin{verbatim}
## top 3  4, 5-mers: 12 40 52
## top 3  4, 5-mers: 12 36 42
\end{verbatim}

The \texttt{rGADEM} package contains a convenient \texttt{plot()} function for
motif visualization. We will use the plot function to visualize the most enriched DNA motif:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# visualize the resulting motif}
\KeywordTok{plot}\NormalTok{(novel_motifs[}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{09-chip-seq-analysis_files/figure-latex/motif-discovery-logo-1} 

}

\caption{Motif with highest enrichment in top 500 CTCF peaks.}\label{fig:motif-discovery-logo}
\end{figure}

The motif shown in Figure \ref{fig:motif-discovery-logo} corresponds to the
previously visualized CTCF motif. Nevertheless, we will computationally
annotate our motif by querying the JASPAR \citep{khan_2018} database in the next section.

\hypertarget{motif-comparison}{%
\subsection{Motif comparison}\label{motif-comparison}}

We will now compare our unknown motif with the JASPAR2018 \citep{khan_2018} database,
to figure out to which transcription factor it corresponds.
Firstly we convert the frequency matrix into a \texttt{PWMatrix} object, and
then use this object to query the database.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the TFBSTools library}
\KeywordTok{library}\NormalTok{(TFBSTools)}

\CommentTok{# extract the motif of interest from the GADEM object}
\NormalTok{unknown_motif =}\StringTok{ }\KeywordTok{getPWM}\NormalTok{(novel_motifs)[[}\DecValTok{1}\NormalTok{]]}

\CommentTok{# convert the motif to a PWM matrix}
\NormalTok{unknown_pwm   =}\StringTok{ }\KeywordTok{PWMatrix}\NormalTok{(}
    \DataTypeTok{ID =} \StringTok{'unknown'}\NormalTok{, }
    \DataTypeTok{profileMatrix =}\NormalTok{ unknown_motif}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Using the \texttt{getMatrixSet()} function we extract all motifs which
correspond to known human transcription factors.
The \texttt{opts} parameter defines which \texttt{PWM} database to use for comparison.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the JASPAR motif database}
\KeywordTok{library}\NormalTok{(JASPAR2018)}

\CommentTok{# extract motifs corresponding to human transcription factors}
\NormalTok{pwm_library =}\StringTok{ }\KeywordTok{getMatrixSet}\NormalTok{(}
\NormalTok{  JASPAR2018,}
  \DataTypeTok{opts=}\KeywordTok{list}\NormalTok{(}
    \DataTypeTok{collection =} \StringTok{'CORE'}\NormalTok{,}
    \DataTypeTok{species    =} \StringTok{'Homo sapiens'}\NormalTok{,}
    \DataTypeTok{matrixtype =} \StringTok{'PWM'}
\NormalTok{  ))}
\end{Highlighting}
\end{Shaded}

The \texttt{PWMSimilarity()} function calculates the Pearson correlation between
the database, and our discovered motif via \texttt{rGADEM}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# find the most similar motif to our motif}
\NormalTok{pwm_sim =}\StringTok{ }\KeywordTok{PWMSimilarity}\NormalTok{(}
  
  \CommentTok{# JASPAR library}
\NormalTok{  pwm_library, }
  
  \CommentTok{# out motif}
\NormalTok{  unknown_pwm,}
  
  \CommentTok{# measure for comparison}
  \DataTypeTok{method =} \StringTok{'Pearson'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We extract the motif names from the PWM library. For each motif
in the library we append the Pearson correlation with our unknown motif, and
look at the topmost candidates.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# extract the motif names from the pwm library}
\NormalTok{pwm_library_list =}\StringTok{ }\KeywordTok{lapply}\NormalTok{(pwm_library, }\ControlFlowTok{function}\NormalTok{(x)\{}
  \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{ID =} \KeywordTok{ID}\NormalTok{(x), }\DataTypeTok{name =} \KeywordTok{name}\NormalTok{(x))}
\NormalTok{\})}

\CommentTok{# combine the list into one data frame}
\NormalTok{pwm_library_dt =}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{bind_rows}\NormalTok{(pwm_library_list)}

\CommentTok{# fetch the similarity of each motif to our unknown motif}
\NormalTok{pwm_library_dt}\OperatorTok{$}\NormalTok{similarity =}\StringTok{ }\NormalTok{pwm_sim[pwm_library_dt}\OperatorTok{$}\NormalTok{ID]}

\CommentTok{# find the most similar motif in the library}
\NormalTok{pwm_library_dt =}\StringTok{ }\NormalTok{pwm_library_dt[}\KeywordTok{order}\NormalTok{(}\OperatorTok{-}\NormalTok{pwm_library_dt}\OperatorTok{$}\NormalTok{similarity),]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(pwm_library_dt)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           ID  name similarity
## 24  MA0139.1  CTCF  0.7033789
## 370 MA1100.1 ASCL1  0.4769023
## 281 MA0807.1  TBX5  0.4762250
## 101 MA0033.2 FOXL1  0.4605249
## 302 MA0825.1   MNT  0.4370585
## 277 MA0803.1 TBX15  0.4317270
\end{verbatim}

As expected, the topmost candidate is CTCF.

\hypertarget{what-to-do-next}{%
\section{What to do next?}\label{what-to-do-next}}

One of the first next steps after you have your peaks is to find out what kind of genes they might be associated with. This is very similar to the gene set analysis \index{gene set analysis} we introduced for RNA-seq in Chapter \ref{rnaseqanalysis}. The same tools, such as \texttt{gProfileR} package\index{R Packages!\texttt{gProfileR}}, can be used on the genes associated with the peaks. However, associating peaks to genes is not always trivial due to long-range gene regulation. Many enhancers can regulate genes that are far away and their targets are not always the nearest gene. However, associating peaks to nearest genes is a generally practiced strategy in ChIP-seq analysis. We have introduced how to find the nearest genes in Chapter \ref{genomicIntervals}. There are also other R packages that will do the association to genes and the gene set analysis in a single workflow. One such package is \texttt{rGREAT} from Bioconductor. This package relies on a web-based tool called \href{http://great.stanford.edu/public/html/}{\emph{GREAT}}.

Knowing every location in the genome bound by a protein can provide a lot
of mechanistic information. However, quite often it is hard to make
biologically relevant conclusions just from one ChIP-seq experiment (i.e.~if
we want to explain how our protein causes a disease, it is hard to guess
which of the tens of thousands of binding places is relevant for the phenotype).
Therefore, it is customary to integrate the results with data which is already available
for our system of interest - ChIP-seq of different proteins, genome wide measurements of
expression, or assays of 3D genome structure.

The choice of downstream analysis is guided by the biological question of interest.
Often we want to compare our samples to other available ChIP-seq experiments.
It is possible to look at the pairwise differences between samples using
differential peak calling \citep{zhang_2014, lun_2014, allhoff_2014, allhoff_2016}.
It is a procedure analogous to the differential expression analysis, except it
results in sets of coordinates that are differentially bound in two biological
conditions. We can then search for a specific DNA binding motif in such regions,
or correlate changes in the binding with changes in gene expression.
With an increase in the number of ChIP experiments, pairwise comparisons become
combinatorially complex. In this case we can segment the genome into multiple classes, where
each class corresponds to a combination of bound transcription factors.
Genome segmentation is usually done using probabilistic models (such as hidden
Markov models \citep{ernst_2012, hoffman_2012}), or machine learning algorithms \citep{mortazavi_2013}.

\hypertarget{exercises-7}{%
\section{Exercises}\label{exercises-7}}

\hypertarget{quality-control}{%
\subsection{Quality control}\label{quality-control}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Apply the fragment size estimation procedure to all ChIP and Input available datasets. {[}Difficulty: \textbf{Beginner}{]}
\item
  Visualize the resulting distributions. {[}Difficulty: \textbf{Beginner}{]}
\item
  How does the Input sample distribution differ from the ChIP samples? {[}Difficulty: \textbf{Beginner}{]}
\item
  Write a function which converts the bam files into bigWig files. {[}Difficulty: \textbf{Beginner}{]}
\item
  Apply the function to all files, and visualize them in the genome browser.
  Observe the signal profiles. What can you notice, about the similarity of the samples? {[}Difficulty: \textbf{Beginner}{]}
\item
  Use \texttt{GViz} to visualize the profiles for CTCF, SMC3 and ZNF143 {[}Difficulty: \textbf{Beginner/Intermediate}{]}
\item
  Calculate the cross correlation for both CTCF replicates, and
  the input samples. How does the profile look for the control samples? {[}Difficulty: \textbf{Intermediate}{]}
\item
  Calculate the cross correlation coefficients for all samples and
  visualize them as a heatmap. {[}Difficulty: \textbf{Intermediate}{]}
\end{enumerate}

\hypertarget{peak-calling-1}{%
\subsubsection{Peak calling}\label{peak-calling-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Use \texttt{normR} to call peaks for all SMC3, CTCF, and ZNF143 samples. {[}Difficulty: \textbf{Beginner}{]}
\item
  Calculate the percentage of reads in peaks for the CTCF experiment. {[}Difficulty: \textbf{Intermediate}{]}
\item
  Download the blacklisted regions corresponding to the hg38 human genome, and calculate
  the percentage of CTCF peaks falling in such regions. {[}Difficulty: \textbf{Advanced}{]}
\item
  Unify the biological replicates by taking an intersection of peaks.
  How many peaks are specific to each biological replicate, and how many peaks overlap. {[}Difficulty: \textbf{Intermediate}{]}
\item
  Plot a scatter plot of signal strengths for biological replicates. Do intersecting
  peaks have equal signal strength in both samples? {[}Difficulty: \textbf{Intermediate}{]}
\item
  Quantify the combinatorial binding of all three proteins. Find
  the number of places which are bound by all three proteins, by
  a combination of two proteins, and exclusively by one protein.
  Annotate the different regions based on their genomic location. {[}Difficulty: \textbf{Advanced}{]}
\item
  Correlate the normR enrichment score for CTCF with peak presence/absence
  (create boxplots of enrichment for peaks which contain and do not contain CTCF motifs). {[}Difficulty: \textbf{Advanced}{]}
\item
  Explore the co-localization of CTCF and ZNF143. Where are the co-bound
  regions located? Which sequence motifs do they contain? Download the ChIA-pet
  data for the GM12878 cell line, and look at the 3D interaction between different
  classes of binding sites. {[}Difficulty: \textbf{Advanced}{]}
\end{enumerate}

\hypertarget{motif-discovery-1}{%
\subsubsection{Motif discovery}\label{motif-discovery-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Repeat the motif discovery analysis on peaks from the ZNF143 transcription factor.
  How many motifs do you observe? How do the motifs look (visualize the motif logs)? {[}Difficulty: \textbf{Intermediate}{]}
\item
  Scan the ZNF143 peaks with the top motifs found in the previous exercise.
  Where are the motifs located? {[}Difficulty: \textbf{Advanced}{]}
\item
  Scan the CTCF peaks with the top motifs identified in the \textbf{ZNF143} peaks.
  Where are the motifs located? What can you conclude from the previous exercises?
  {[}Difficulty: \textbf{Advanced}{]}
\end{enumerate}

\hypertarget{bsseq}{%
\chapter{DNA methylation analysis using bisulfite sequencing data}\label{bsseq}}

The epigenome consists of chemical modifications of DNA and histones. These modifications are shown to be associated with gene regulation in various settings (see Chapter \ref{intro} for an intro). These modifications in turn have specific importance for cell type identification. There are many different ways of measuring such modifications. We have shown how histone modifications can be measured in a genome-wide manner in Chapter \ref{chipseq} using ChIP-seq. In this chapter we will focus on the analysis of DNA methylation data using data from a technique called bisulfite sequencing (BS-seq). We will introduce how to process data and data quality checks, as well as statistical analysis relevant for BS-seq data.

\hypertarget{what-is-dna-methylation}{%
\section{What is DNA methylation ?}\label{what-is-dna-methylation}}

Cytosine methylation (5-methylcytosine, 5mC) is one of the main covalent base modifications in eukaryotic genomes, generally observed on CpG dinucleotides. Methylation can also rarely occur in a non-CpG context, but this was mainly observed in human embryonic stem and neuronal cells \citep{Lister2009-sd, Lister2013-vs}. DNA methylation is a part of the epigenetic regulation mechanism of gene expression. It is cell-type-specific DNA modification. \index{DNA methylation}It is reversible but mostly remains stable through cell division. There are roughly 28 million CpGs in the human genome, 60--80\% are generally methylated. Less than 10\% of CpGs occur in CG-dense regions that are termed CpG islands in the human genome \citep{Smith2013-jh}. It has been demonstrated that DNA methylation is also not uniformly distributed over the genome, but rather is associated with CpG density. In vertebrate genomes, cytosine bases are usually unmethylated in CpG-rich regions such as CpG islands and tend to be methylated in CpG-deficient regions. Vertebrate genomes are largely CpG deficient except at CpG islands. Conversely, invertebrates such as \emph{Drosophila melanogaster} and \emph{Caenorhabditis elegans} do not exhibit cytosine methylation and consequently do not have CpG rich and poor regions but rather a steady CpG frequency over their genomes \citep{Deaton2011-pm}.

\hypertarget{how-dna-methylation-is-set}{%
\subsection{How DNA methylation is set ?}\label{how-dna-methylation-is-set}}

DNA methylation is established by DNA methyltransferases DNMT3A and DNMT3B in combination with DNMT3L and maintained through cell division by the methyltransferase DNMT1 and associated proteins. DNMT3a and DNMT3b are in charge of the de novo methylation during early development. Loss of 5mC can be achieved passively by dilution during replication or exclusion of DNMT1 from the nucleus. Recent discoveries of the ten-eleven translocation (TET) family of proteins and their ability to convert 5-methylcytosine (5mC) into 5-hydroxymethylcytosine (5hmC) in vertebrates provide a path for catalyzed active DNA demethylation \citep{Tahiliani2009-ar}. Iterative oxidations of 5hmC catalyzed by TET result in 5-formylcytosine (5fC) and 5-carboxylcytosine (5caC). 5caC mark is excised from DNA by G/T mismatch-specific thymine-DNA glycosylase (TDG), which as a result reverts cytosine residue to its unmodified state \citep{He2011-pw}. Apart from these, mainly bacteria, but possibly higher eukaryotes, contain base modifications on bases other than cytosine, such as methylated adenine or guanine \citep{Clark2011-sc}.

\hypertarget{how-to-measure-dna-methylation-with-bisulfite-sequencing}{%
\subsection{How to measure DNA methylation with bisulfite sequencing}\label{how-to-measure-dna-methylation-with-bisulfite-sequencing}}

One of the most reliable and popular ways to measure DNA methylation is high-throughput bisulfite sequencing. This method, and the related ones, allow measurement of DNA methylation at the single nucleotide resolution. The bisulfite conversion turns unmethylated Cs to Ts and methylated Cs remain intact. Then, the only thing to do is to align the reads with those C-\textgreater T conversions and count C-\textgreater T mutations to calculate fraction of methylated bases. In the end, we can get quantitative genome-wide measurements for DNA methylation.

\hypertarget{analyzing-dna-methylation-data}{%
\section{Analyzing DNA methylation data}\label{analyzing-dna-methylation-data}}

For the remainder of this chapter, we will explain how to do DNA methylation analysis using R. The analysis process is somewhat similar to the analysis patterns observed in other sequencing data analyses. The process can be chunked to four main parts with further sub-chunks:\index{DNA methylation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Processing raw data
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Quality check
\item
  Alignment and post-alignment processing
\item
  Methylation calling
\item
  Filtering bases
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Exploratory analysis
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Clustering
\item
  PCA \index{principal component analysis (PCA)}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Finding interesting regions
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Differential methylation
\item
  Methylation segmentation
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Annotating interesting regions
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Nearest genes
\item
  Annotation with other genomic features
\item
  Integration with other quantitative genomics data
\end{itemize}

\hypertarget{processing-raw-data-and-getting-data-into-r}{%
\section{Processing raw data and getting data into R}\label{processing-raw-data-and-getting-data-into-r}}

The rawest form of data that most users get is probably in the form of fastq files obtained from the sequencing experiments. We will describe the necessary steps and the tools that can be used for raw data processing and if they exist, we will mention their R equivalents. However, the data processing is usually done outside of the R framework, and for the following sections we will assume that the data processing is done and our analysis is starting from methylation call files.

The typical data processing step starts with a data quality check. The fastq files are first run through quality check software that shows the quality of the sequencing run. We would typically use \href{https://www.bioinformatics.babraham.ac.uk/projects/fastqc/}{fastQC} for this. However, there are several bioconductor packages that could be of use, such as \href{https://bioconductor.org/packages/release/bioc/html/Rqc.html}{\texttt{Rqc}} and \href{https://bioconductor.org/packages/release/bioc/html/QuasR.html}{\texttt{QuasR}}. We have introduced how to use some of these tools for sequencing quality check in Chapter \ref{processingReads}. Following the quality check, provided everything is OK, the reads can be aligned to the genome. Before the alignment , adapters or low-quality ends of the reads can be trimmed to increase number of alignments. Low-quality ends mostly likely have poor basecalls, which will lead to many mismatches. Reads with non-trimmed adapters will also not align to the genome. We would use adapter trimming tools such as \href{https://cutadapt.readthedocs.io/en/stable/}{cutadapt} or \href{https://github.com/seqan/flexbar}{flexbar} for this purpose, although there are a bunch of them to choose from. Following this, reads are aligned to the genome with a bisulfite-treatment-aware aligner. For our own purposes, we use Bismark\citep{Krueger2011-vv}, however there are other equally accurate aligners, and some are reviewed \href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3378906/}{here}. In addition, the Bioconductor package \href{https://bioconductor.org/packages/release/bioc/html/QuasR.html}{\texttt{QuasR}} can align BS-seq reads within the R framework.

After alignment, we need to call C-\textgreater T conversions and calculate the fraction/percentage of methylation. Most of the time, aligners come with auxiliary tools that calculate per-base methylation values. Normally, they output a tabular format containing the location of the Cs and methylation value and strand. Within R, \texttt{QuasR}\index{R Packages!\texttt{QuasR}} and \texttt{methylKit} \index{R Packages!\texttt{methylKit}}can call methylation values from BAM files albeit with some limitations. In essence, these methylation call files can be easily read into R and downstream analysis within R starts from that point. An important quality measure at this stage is to look at the conversion rate. This simply means how many unmethylated Cs are converted to Ts. Since we expect non-CpG methylation to be rare, we can simply count number of C-\textgreater T conversions in the non-CpG context and calculate conversion rate. The best way to do this would be via spike-in sequences where we expect no methylation at all. Since non-CpG methylation is tissue specific, calculating the conversion rate using non-CpG Cs might be misleading in some cases.

\hypertarget{data-filtering-and-exploratory-analysis}{%
\section{Data filtering and exploratory analysis}\label{data-filtering-and-exploratory-analysis}}

We assume that we start the analysis in R with the methylation call files. We will read those files in and carry out exploratory analysis, and we will show how to filter bases or regions from the data and in what circumstances we might need to do so. We will use the \href{https://bioconductor.org/packages/release/bioc/html/methylKit.html}{methylKit}\citep{Akalin2012-af} package for the bulk of the analysis. \index{R Packages!\texttt{methylKit}}

\hypertarget{reading-methylation-call-files}{%
\subsection{Reading methylation call files}\label{reading-methylation-call-files}}

A typical methylation call file looks like this:

\begin{verbatim}
##         chrBase   chr    base strand coverage freqC  freqT
## 1 chr21.9764539 chr21 9764539      R       12 25.00  75.00
## 2 chr21.9764513 chr21 9764513      R       12  0.00 100.00
## 3 chr21.9820622 chr21 9820622      F       13  0.00 100.00
## 4 chr21.9837545 chr21 9837545      F       11  0.00 100.00
## 5 chr21.9849022 chr21 9849022      F      124 72.58  27.42
\end{verbatim}

Most of the time bisulfite sequencing experiments have test and control samples. The test samples can be from a disease tissue while the control samples can be from a healthy tissue. You can read a set of methylation call files that have test/control conditions giving a \texttt{treatment} vector option. The treatment vector defines the sample groups and it is very important for the differential methylation analysis. For the sake of subsequent analysis, file.list, sample.id and treatment option should have the same order. In the following example, the first two files have the sample IDs ``test1'' and ``test2'' and as determined by the treatment vector they belong to the same group. The third and fourth files have sample IDs ``ctrl1'' and ``ctrl2'' and they belong to the same group as indicated by the treatment vector. We will first get a list of file paths and have a look at the content.

If you look what is inside the \texttt{file.list} variable, you will see that it is a simple list of file paths. Each file contains methylation calls for a given sample. Now, we can read the files with the \texttt{methRead()} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read the files to a methylRawList object: myobj}
\NormalTok{myobj=}\KeywordTok{methRead}\NormalTok{(file.list,}
           \DataTypeTok{sample.id=}\KeywordTok{list}\NormalTok{(}\StringTok{"test1"}\NormalTok{,}\StringTok{"test2"}\NormalTok{,}\StringTok{"ctrl1"}\NormalTok{,}\StringTok{"ctrl2"}\NormalTok{),}
           \DataTypeTok{assembly=}\StringTok{"hg18"}\NormalTok{,}
           \DataTypeTok{treatment=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}
           \DataTypeTok{context=}\StringTok{"CpG"}
\NormalTok{           )}
\end{Highlighting}
\end{Shaded}

Tab-separated bedgraph like formats from Bismark methylation caller can also be read in by methylkit. In those cases, we have to provide either \texttt{pipeline="bismarkCoverage"} or \texttt{pipeline="bismarkCytosineReport"} to the \texttt{methRead()} function. In addition to the options we mentioned above,
any tab-separated text file with a generic format can be read in using methylKit,
such as methylation ratio files from \href{http://code.google.com/p/bsmap/}{BSMAP}.
See \href{http://zvfak.blogspot.com/2012/10/how-to-read-bsmap-methylation-ratio.html}{here} for an example.

Before we move on, let us have a look at what kind of information is stored in \texttt{myobj}. This is technically a \texttt{methylRawList} object, which is essentially a list of \texttt{methylRaw} objects. These objects hold
the information for the genomic location of Cs, and methylated Cs and unmethylated Cs.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## inside the methylRawList object}
\KeywordTok{length}\NormalTok{(myobj)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(myobj[[}\DecValTok{1}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     chr   start     end strand coverage numCs numTs
## 1 chr21 9764513 9764513      -       12     0    12
## 2 chr21 9764539 9764539      -       12     3     9
## 3 chr21 9820622 9820622      +       13     0    13
## 4 chr21 9837545 9837545      +       11     0    11
## 5 chr21 9849022 9849022      +      124    90    34
## 6 chr21 9853296 9853296      +       17    10     7
\end{verbatim}

\hypertarget{further-quality-check}{%
\subsection{Further quality check}\label{further-quality-check}}

It is always a good idea to check how the data looks before proceeding further. For example, the methylation values should have bimodal distribution generally. This can be checked via the
\texttt{getMethylationStats()} function. Normally, we should see bimodal
distributions. Strong deviations from the bimodality may be due poor experimental quality, such as problems with bisulfite treatment. Below we show how to get these plots using the \texttt{getMethylationStats()} function. The result is shown in Figure \ref{fig:methStats}. As expected, it has a bimodal distribution where most CpGs have either high methylation or low methylation.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{getMethylationStats}\NormalTok{(myobj[[}\DecValTok{2}\NormalTok{]],}\DataTypeTok{plot=}\OtherTok{TRUE}\NormalTok{,}\DataTypeTok{both.strands=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{10-bs-seq-analysis_files/figure-latex/methStats-1} 

}

\caption{Histogram for methylation values for all CpGs in the dataset.}\label{fig:methStats}
\end{figure}

In addition, we might want to see coverage values. By default, methylkit handles bases with at least 10X coverage but that can be changed. The bases with unusually high coverage are usually alarming. It might indicate a PCR bias issue in the experimental procedure. The general coverage statistics can be checked with the
\texttt{getCoverageStats()} function shown below. The resulting plot is shown in Figure \ref{fig:coverageStats}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{getCoverageStats}\NormalTok{(myobj[[}\DecValTok{2}\NormalTok{]],}\DataTypeTok{plot=}\OtherTok{TRUE}\NormalTok{,}\DataTypeTok{both.strands=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{10-bs-seq-analysis_files/figure-latex/coverageStats-1} 

}

\caption{Histogram for log10 read counts per CpG.}\label{fig:coverageStats}
\end{figure}

It might be useful to filter samples based on coverage. Particularly, if our samples are suffering from PCR bias, it would be useful to discard bases with very high read coverage. Furthermore, we would also like to discard bases that have low read coverage; a high enough read coverage will increase the power of the statistical tests. The code below filters a \texttt{methylRawList}, discards bases that have coverage below 10X, and also discards the bases that have more than 99.9th percentile of coverage in each sample.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filtered.myobj=}\KeywordTok{filterByCoverage}\NormalTok{(myobj,}\DataTypeTok{lo.count=}\DecValTok{10}\NormalTok{,}\DataTypeTok{lo.perc=}\OtherTok{NULL}\NormalTok{,}
                                      \DataTypeTok{hi.count=}\OtherTok{NULL}\NormalTok{,}\DataTypeTok{hi.perc=}\FloatTok{99.9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{merging-samples-into-a-single-table}{%
\subsection{Merging samples into a single table}\label{merging-samples-into-a-single-table}}

When we first read the files, each file is stored as its own entity. If we want to compare samples in any way, we need to make a unified data structure that contains CpGs that are covered in most samples. The \texttt{unite()} function creates a new object using the CpGs covered in each sample.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## we use :: notation to make sure unite() function from methylKit is called}
\NormalTok{meth=methylKit}\OperatorTok{::}\KeywordTok{unite}\NormalTok{(myobj, }\DataTypeTok{destrand=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let us take a look at the data content of the \texttt{methylBase} object:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(meth)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     chr   start     end strand coverage1 numCs1 numTs1 coverage2 numCs2 numTs2
## 1 chr21 9853296 9853296      +        17     10      7       333    268     65
## 2 chr21 9853326 9853326      +        17     12      5       329    249     79
## 3 chr21 9860126 9860126      +        39     38      1        83     78      5
## 4 chr21 9906604 9906604      +        68     42     26       111     97     14
## 5 chr21 9906616 9906616      +        68     52     16       111    104      7
## 6 chr21 9906619 9906619      +        68     59      9       111    109      2
##   coverage3 numCs3 numTs3 coverage4 numCs4 numTs4
## 1        18     16      2       395    341     54
## 2        16     14      2       379    284     95
## 3        83     83      0        41     40      1
## 4        23     18      5        37     33      4
## 5        23     14      9        37     27     10
## 6        22     18      4        37     29      8
\end{verbatim}

By default, the \texttt{unite()} function produces bases/regions covered in all samples. That requirement can be relaxed using the \texttt{min.per.group} option in the \texttt{unite()} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# creates a methylBase object, }
\CommentTok{# where only CpGs covered with at least 1 sample per group will be returned}

\CommentTok{# there were two groups defined by the treatment vector, }
\CommentTok{# given during the creation of myobj: treatment=c(1,1,0,0)}
\NormalTok{meth.min=}\KeywordTok{unite}\NormalTok{(myobj,}\DataTypeTok{min.per.group=}\NormalTok{1L)}
\end{Highlighting}
\end{Shaded}

\hypertarget{filtering-cpgs}{%
\subsection{Filtering CpGs}\label{filtering-cpgs}}

We might need to filter the CpGs further before exploratory analysis or even before the downstream analysis such as differential methylation. For exploratory analysis, it is of general interest to see how samples relate to each other and we might want to remove CpGs that are not variable before doing that. Or we might remove Cs that are potentially C-\textgreater T mutations. First, we show how to
filter based on variation. Below, we extract percent methylation values from CpGs as a matrix. Calculate the standard deviation for each CpG and filter based on standard deviation. We also plot the distribution of per-CpG standard deviations with the \texttt{hist()} function. The resulting plot is shown in Figure \ref{fig:methVar}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pm=}\KeywordTok{percMethylation}\NormalTok{(meth) }\CommentTok{# get percent methylation matrix}
\NormalTok{mds=matrixStats}\OperatorTok{::}\KeywordTok{rowSds}\NormalTok{(pm) }\CommentTok{# calculate standard deviation of CpGs}
\KeywordTok{head}\NormalTok{(meth[mds}\OperatorTok{>}\DecValTok{20}\NormalTok{,])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      chr   start     end strand coverage1 numCs1 numTs1 coverage2 numCs2 numTs2
## 11 chr21 9906681 9906681      +        21     12      9        60     56      4
## 12 chr21 9906694 9906694      +        21      9     12        60     53      7
## 13 chr21 9906700 9906700      +        13      6      7        53     43     10
## 14 chr21 9906714 9906714      +        14      3     11        41     37      4
## 18 chr21 9906873 9906873      +        12      8      4        41     33      8
## 23 chr21 9927527 9927527      +        17      5     12        40     22     18
##    coverage3 numCs3 numTs3 coverage4 numCs4 numTs4
## 11        37     14     23        26     11     15
## 12        39     16     23        26     15     11
## 13        30      8     22        23     10     13
## 14        25     19      6        21     19      2
## 18        15      4     11        22      7     15
## 23        32     32      0        14     11      3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(mds,}\DataTypeTok{col=}\StringTok{"cornflowerblue"}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"Std. dev. per CpG"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{10-bs-seq-analysis_files/figure-latex/methVar-1} 

}

\caption{Histogram of per-CpG standard deviations.}\label{fig:methVar}
\end{figure}

Now, let's assume we know the locations of C-\textgreater T mutations. These locations should be removed from the analysis as they do not represent
bisulfite-treatment-associated conversions. Mutation locations are
stored in a \texttt{GRanges} object, and we can use that to remove CpGs
overlapping with mutations. In order to do the overlap operation, we will convert the methylKit object to a \texttt{GRanges} object and do the filtering with the \texttt{\%over\%} function within \texttt{{[}\ {]}}. The returned object will still be a methylKit object.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(GenomicRanges)}
\CommentTok{# example SNP}
\NormalTok{mut=}\KeywordTok{GRanges}\NormalTok{(}\DataTypeTok{seqnames=}\KeywordTok{c}\NormalTok{(}\StringTok{"chr21"}\NormalTok{,}\StringTok{"chr21"}\NormalTok{),}
            \DataTypeTok{ranges=}\KeywordTok{IRanges}\NormalTok{(}\DataTypeTok{start=}\KeywordTok{c}\NormalTok{(}\DecValTok{9853296}\NormalTok{, }\DecValTok{9853326}\NormalTok{),}
                           \DataTypeTok{end=}\KeywordTok{c}\NormalTok{( }\DecValTok{9853296}\NormalTok{,}\DecValTok{9853326}\NormalTok{)))}

\CommentTok{# select CpGs that do not overlap with mutations}
\NormalTok{sub.meth=meth[}\OperatorTok{!}\StringTok{ }\KeywordTok{as}\NormalTok{(meth,}\StringTok{"GRanges"}\NormalTok{) }\OperatorTok{%over%}\StringTok{ }\NormalTok{mut,]}
\KeywordTok{nrow}\NormalTok{(meth)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 963
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{nrow}\NormalTok{(sub.meth)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 961
\end{verbatim}

\hypertarget{clustering-samples}{%
\subsection{Clustering samples}\label{clustering-samples}}

Clustering is used for grouping data points by their similarity. It is a general concept that can be achieved by many different algorithms and we introduced clustering and multiple prominent clustering algorithms in Chapter \ref{unsupervisedLearning}. In the context of DNA methylation, we are trying to find samples that are similar to each other. For example, if we sequenced 3 heart samples and 4 liver samples, we would expect liver samples will be more similar to each other than heart samples on the DNA methylation space.

The following function will cluster the samples and draw a dendrogram.
It will use correlation distance, which is \(1-\rho\) , where \(\rho\) is the correlation coefficient between two pairs of samples. The cluster tree will be drawn using the ``ward'' method. \index{clustering!
hierarchical clustering}This specific variant uses a ``bottom up'' approach: each data point starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. In Ward's method, two clusters are merged if the variance is minimized compared to other possible merge operations. This bottom up approach helps build the dendrogram showing the relationship between clusters. The result of the clustering is shown in Figure \ref{fig:clusterMethPlot}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{clusterSamples}\NormalTok{(meth, }\DataTypeTok{dist=}\StringTok{"correlation"}\NormalTok{, }\DataTypeTok{method=}\StringTok{"ward"}\NormalTok{, }\DataTypeTok{plot=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{10-bs-seq-analysis_files/figure-latex/clusterMethPlot-1} 

}

\caption{Dendrogram for samples using correlation distance and Ward's method for hierarchical clustering.}\label{fig:clusterMethPlot}
\end{figure}

\begin{verbatim}
## 
## Call:
## hclust(d = d, method = HCLUST.METHODS[hclust.method])
## 
## Cluster method   : ward.D 
## Distance         : pearson 
## Number of objects: 4
\end{verbatim}

Setting the \texttt{plot=FALSE} will return a dendrogram object which can be manipulated by users or fed in to other user functions that can work with dendrograms.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hc =}\StringTok{ }\KeywordTok{clusterSamples}\NormalTok{(meth, }\DataTypeTok{dist=}\StringTok{"correlation"}\NormalTok{, }\DataTypeTok{method=}\StringTok{"ward"}\NormalTok{, }\DataTypeTok{plot=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{principal-component-analysis-1}{%
\subsection{Principal component analysis}\label{principal-component-analysis-1}}

Principal component analysis (PCA) \index{principal component analysis (PCA)}is a mathematical transformation of (possibly) correlated variables into a number of uncorrelated variables called principal components. The resulting components from this transformation is defined in such a way that the first principal component has the highest variance and accounts for as most of the variability in the data. We have introduced PCA and other similar methods in Chapter \ref{unsupervisedLearning}. The following function will plot a scree plot for importance of components and the result is shown in Figure \ref{fig:pcaMethScree}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{PCASamples}\NormalTok{(meth, }\DataTypeTok{screeplot=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{10-bs-seq-analysis_files/figure-latex/pcaMethScree-1} 

}

\caption{Scree plot for explained variance for principal components.}\label{fig:pcaMethScree}
\end{figure}

We can also plot the PC1 and PC2 axes and a scatter plot of our samples on those axes which will reveal how they cluster within these new dimensions. Similar to the clustering dendrogram, we would like to see samples that are similar to be close to each other on the scatter plot. If they are not, it might indicate problems with the experiment such as batch effects. The function below plots the samples in such a scatter plot on principal component axes. The resulting plot is shown in Figure \ref{fig:pcaMethScatter}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pc=}\KeywordTok{PCASamples}\NormalTok{(meth,}\DataTypeTok{obj.return =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{adj.lim=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{10-bs-seq-analysis_files/figure-latex/pcaMethScatter-1} 

}

\caption{Samples plotted on principal components.}\label{fig:pcaMethScatter}
\end{figure}

In this case, we also returned an object from the plotting function. This is the output of the \texttt{prcomp()} function, which includes loadings and eigenvectors which might be useful. You can also do your own PCA analysis using \texttt{percMethylation()} and \texttt{prcomp()}. In the case above, the methylation matrix is transposed. This allows us to compare distances between samples on the PCA scatter plot.

\hypertarget{extracting-interesting-regions-differential-methylation-and-segmentation}{%
\section{Extracting interesting regions: Differential methylation and segmentation}\label{extracting-interesting-regions-differential-methylation-and-segmentation}}

When analyzing DNA methylation data, we usually look for regions that are different than the rest of the methylome or different from a reference methylome. These regions are so-called ``interesting regions''. They usually mark important genomic features that are related to gene regulation, which in turn defines the cell type. Therefore, it is a general interest to find such regions and analyze them further to understand our biological sample or to answer specific research questions. Below we will describe two ways of defining ``regions of interest''.

\hypertarget{differential-methylation}{%
\subsection{Differential methylation}\label{differential-methylation}}

Once methylation proportions per base are obtained, generally, the differences between methylation profiles are considered next. When there are multiple sample groups where each group defines a separate biological entity or treatment, it is usually of interest to locate bases or regions with different methylation proportions across the sample groups. The bases or regions with different methylation proportions across samples are called differentially methylated CpG sites (DMCs) and differentially methylated regions (DMRs). They have been shown to play a role in many different diseases due to their association with epigenetic control of gene regulation. In addition, DNA methylation profiles can be highly tissue-specific due to their role in gene regulation \citep{Schubeler2015-ai}. DNA methylation is highly informative when studying normal and diseased cells, because it can also act as a biomarker. For example, the presence of large-scale abnormally methylated genomic regions is a hallmark feature of many types of cancers \citep{Ehrlich2002-hv}. Because of the aforementioned reasons, investigating differential methylation is usually one of the primary goals of doing bisulfite sequencing.

\hypertarget{fishers-exact-test}{%
\subsubsection{Fisher's exact test}\label{fishers-exact-test}}

Differential DNA methylation is usually calculated by comparing the proportion of methylated Cs in a test sample relative to a control. In simple comparisons between such pairs of samples (i.e.~test and control), methods such as Fisher's exact test can be used. If there are replicates, replicates can be pooled within groups to a single sample per group. This strategy, however, does not take into account biological variability between replicates. We will now show how to compare pairs of samples via the \texttt{calculateDiffMeth()} function in \texttt{methylKit}. When there is only one sample per sample group, \texttt{calculateDiffMeth()} automatically applies Fisher's exact test. We will now extract one sample from each group and run \texttt{calculateDiffMeth()}, which will automatically run Fisher's exact test.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{getSampleID}\NormalTok{(meth)}
\NormalTok{new.meth=}\KeywordTok{reorganize}\NormalTok{(meth,}\DataTypeTok{sample.ids=}\KeywordTok{c}\NormalTok{(}\StringTok{"test1"}\NormalTok{,}\StringTok{"ctrl1"}\NormalTok{),}\DataTypeTok{treatment=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\NormalTok{dmf=}\KeywordTok{calculateDiffMeth}\NormalTok{(new.meth)}
\end{Highlighting}
\end{Shaded}

As mentioned, we can also pool the samples from the same group by adding up the number of Cs and Ts per group. This way even if we have replicated experiments we treat them as single experiments, and can apply Fisher's exact test. We will now pool the samples and apply the \texttt{calculateDiffMeth()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pooled.meth=}\KeywordTok{pool}\NormalTok{(meth,}\DataTypeTok{sample.ids=}\KeywordTok{c}\NormalTok{(}\StringTok{"test"}\NormalTok{,}\StringTok{"control"}\NormalTok{))}
\NormalTok{dm.pooledf=}\KeywordTok{calculateDiffMeth}\NormalTok{(pooled.meth)}
\end{Highlighting}
\end{Shaded}

The \texttt{calculateDiffMeth()} function returns the P-values for all bases or regions in the input methylBase object. We need to filter to get differentially methylated CpGs. This can be done via the \texttt{getMethlyDiff()} function or simple filtering via \texttt{{[}\ {]}} notation. Below we show how to filter the \texttt{methylDiff} object output by the \texttt{calculateDiffMeth()} function in order to get differentially methylated CpGs. The function arguments define cutoff values for the methylation difference between groups and q-value. In these cases, we require a methylation difference of 25\% and a q-value of at least \(0.01\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get differentially methylated bases/regions with specific cutoffs}
\NormalTok{all.diff=}\KeywordTok{getMethylDiff}\NormalTok{(dm.pooledf,}\DataTypeTok{difference=}\DecValTok{25}\NormalTok{,}\DataTypeTok{qvalue=}\FloatTok{0.01}\NormalTok{,}\DataTypeTok{type=}\StringTok{"all"}\NormalTok{)}

\CommentTok{# get hyper-methylated}
\NormalTok{hyper=}\KeywordTok{getMethylDiff}\NormalTok{(dm.pooledf,}\DataTypeTok{difference=}\DecValTok{25}\NormalTok{,}\DataTypeTok{qvalue=}\FloatTok{0.01}\NormalTok{,}\DataTypeTok{type=}\StringTok{"hyper"}\NormalTok{)}

\CommentTok{# get hypo-methylated}
\NormalTok{hypo=}\KeywordTok{getMethylDiff}\NormalTok{(dm.pooledf,}\DataTypeTok{difference=}\DecValTok{25}\NormalTok{,}\DataTypeTok{qvalue=}\FloatTok{0.01}\NormalTok{,}\DataTypeTok{type=}\StringTok{"hypo"}\NormalTok{)}

\CommentTok{#using [ ] notation}
\NormalTok{hyper2=dm.pooledf[dm.pooledf}\OperatorTok{$}\NormalTok{qvalue }\OperatorTok{<}\StringTok{ }\FloatTok{0.01} \OperatorTok{&}\StringTok{ }\NormalTok{dm.pooledf}\OperatorTok{$}\NormalTok{meth.diff }\OperatorTok{>}\StringTok{ }\DecValTok{25}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\hypertarget{logistic-regression-based-tests}{%
\subsubsection{Logistic regression based tests}\label{logistic-regression-based-tests}}

Regression-based methods are generally used to model methylation levels in relation to the sample groups and variation between replicates. Differences between currently available regression methods stem from the choice of distribution to model the data and the variation associated with it. In the simplest case, linear regression\index{linear regression} can be used to model methylation per given CpG or loci across sample groups. The model fits regression coefficients to model the expected methylation proportion values for each CpG site across sample groups. Hence, the null hypothesis of the model coefficients being zero could be tested using t-statistics. However, linear-regression-based methods might produce fitted methylation levels outside the range \([0,1]\) unless the values are transformed before regression. An alternative is logistic regression\index{logistic regression}, which can deal with data strictly bounded between 0 and 1 and with non-constant variance, such as methylation proportion/fraction values. In the logistic regression, it is assumed that fitted values have variation \(np(1-p)\), where \(p\) is the fitted methylation proportion for a given sample and \(n\) is the read coverage. If the observed variance is larger or smaller than assumed by the model, one speaks of under- or over-dispersion. This over/under-dispersion can be corrected by calculating a scaling factor and using that factor to adjust the variance estimates as in \(np(1-p)s\), where \(s\) is the scaling factor. MethylKit can apply logistic regression to test the methylation difference with or without the over-dispersion correction. In this case, Chi-square or F-test can be used to compare the difference in the deviances of the null model and the alternative model. The null model assumes there is no relationship between sample groups and methylation, and the alternative model assumes that there is a relationship where sample groups are predictive of methylation values for a given CpG or region for which the model is constructed. Next, we are going to use the logistic-regression-based model with over-dispersion correction and Chi-square test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dm.lr=}\KeywordTok{calculateDiffMeth}\NormalTok{(meth,}\DataTypeTok{overdispersion =} \StringTok{"MN"}\NormalTok{,}\DataTypeTok{test =}\StringTok{"Chisq"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{betabinomial-distribution-based-tests}{%
\subsubsection{Betabinomial-distribution-based tests}\label{betabinomial-distribution-based-tests}}

More complex regression models use beta binomial distribution and are particularly useful for better modeling the variance. Similar to logistic regression, their observation follows binomial distribution (number of reads), but methylation proportion itself can vary across samples, according to a beta distribution.\index{betabinomial distribution} It can deal with fitting values in the \([0,1]\) range and performs better when there is greater variance than expected by the simple logistic model. In essence, these models have a different way of calculating a scaling factor when there is over-dispersion in the model. Further enhancements are made to these models by using the empirical Bayes methods that can better estimate hyper parameters of the beta distribution (variance-related parameters) by borrowing information between loci or regions within the genome to aid with inference about each individual loci or region. We are now going to use a beta-binomial based model called DSS \citep{Feng2014-pd} to calculate differential methylation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dm.dss=}\KeywordTok{calculateDiffMethDSS}\NormalTok{(meth)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Using internal DSS code...
\end{verbatim}

\hypertarget{differential-methylation-for-regions-rather-than-base-pairs}{%
\subsubsection{Differential methylation for regions rather than base-pairs}\label{differential-methylation-for-regions-rather-than-base-pairs}}

Until now, we have worked on differentially methylated cytosines. However,
working with base-pair resolution data has its problems. Not all the CpGs will be covered in all samples. If covered they may have low coverage, which reduces the power of the tests. Instead of base-pairs, we can choose to work with regions. So, it might be desirable to summarize methylation information over pre-defined regions rather than doing base-pair resolution analysis. \texttt{methylKit} provides functionality to do such analysis. We can either tile the whole genome to tiles with predefined length, or we can use pre-defined regions such as promoters or CpG islands. This kind of regional analysis is carried out by adding up C and T counts from each covered cytosine and returning a total C and T count for each region.

The function below tiles the genome with windows of \(1000\) bp length and \(1000\) bp step-size and summarizes the methylation information on those tiles. In this case, it returns a \texttt{methylRawList} object which can be fed into \texttt{unite()} and \texttt{calculateDiffMeth()} functions consecutively to get differentially methylated regions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tiles=}\KeywordTok{tileMethylCounts}\NormalTok{(myobj,}\DataTypeTok{win.size=}\DecValTok{1000}\NormalTok{,}\DataTypeTok{step.size=}\DecValTok{1000}\NormalTok{)}
\KeywordTok{head}\NormalTok{(tiles[[}\DecValTok{1}\NormalTok{]],}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     chr   start     end strand coverage numCs numTs
## 1 chr21 9764001 9765000      *       24     3    21
## 2 chr21 9820001 9821000      *       13     0    13
## 3 chr21 9837001 9838000      *       11     0    11
\end{verbatim}

In addition, if we are interested in particular regions, we can also get those regions as methylKit objects after summarizing the methylation information as described above. The code below summarizes the methylation information over a given set of promoter regions and outputs a \texttt{methylRaw} or \texttt{methylRawList} object depending on the input. We are using the output of
\texttt{genomation} functions used above to provide the locations of promoters. For regional summary functions, we need to
provide regions of interest as GRanges objects\index{R Packages!\texttt{genomation}}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(genomation)}

\CommentTok{# read the gene BED file}
\NormalTok{gene.obj=}\KeywordTok{readTranscriptFeatures}\NormalTok{(}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{, }\StringTok{"refseq.hg18.bed.txt"}\NormalTok{, }
                                           \DataTypeTok{package =} \StringTok{"methylKit"}\NormalTok{))}
\NormalTok{promoters=}\KeywordTok{regionCounts}\NormalTok{(myobj,gene.obj}\OperatorTok{$}\NormalTok{promoters)}

\KeywordTok{head}\NormalTok{(promoters[[}\DecValTok{1}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     chr    start      end strand coverage numCs numTs
## 1 chr21 10011791 10013791      -     7953  6662  1290
## 2 chr21 10119796 10121796      -     1725  1171   554
## 3 chr21 10119808 10121808      -     1725  1171   554
## 4 chr21 13903368 13905368      +       10    10     0
## 5 chr21 14273636 14275636      -      282   220    62
## 6 chr21 14509336 14511336      +     1058    55  1003
\end{verbatim}

In addition, it is possible to cluster DMCs based on their proximity and direction of differential methylation. This can be achieved by the \texttt{methSeg()} function in methylKit. We will see more about the \texttt{methSeg()} function in the following section.
But it can take the output of \texttt{getMethylDiff()} function therefore can work on DMCs to get differentially methylated regions.

\hypertarget{adding-covariates}{%
\subsubsection{Adding covariates}\label{adding-covariates}}

Covariates can be included in the analysis as well in methylKit. The \texttt{calculateDiffMeth()} function will then try to
separate the influence of the covariates from the
treatment effect via the logistic regression model. In this case, we will test
if the full model (model with treatment and covariates) is better than the model with
the covariates only. If there is no effect due to the treatment (sample groups),
the full model will not explain the data better than the model with covariates
only. In \texttt{calculateDiffMeth()}, this is achieved by
supplying the \texttt{covariates} argument in the format of a \texttt{data.frame}.
Below, we simulate methylation data and add a \texttt{data.frame} for the age.
The data frame can include more columns, and those columns can also be
\texttt{factor} variables. The row order of the data.frame should match the order
of samples in the \texttt{methylBase} object. Below we are showing an example
of this using a simulated data set where methylation values of CpGs will be affected by the age of the sample.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{covariates=}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{age=}\KeywordTok{c}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DecValTok{80}\NormalTok{,}\DecValTok{34}\NormalTok{,}\DecValTok{30}\NormalTok{,}\DecValTok{80}\NormalTok{,}\DecValTok{40}\NormalTok{))}
\NormalTok{sim.methylBase=}\KeywordTok{dataSim}\NormalTok{(}\DataTypeTok{replicates=}\DecValTok{6}\NormalTok{,}\DataTypeTok{sites=}\DecValTok{1000}\NormalTok{,}
                        \DataTypeTok{treatment=}\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{),}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{3}\NormalTok{)),}
                        \DataTypeTok{covariates=}\NormalTok{covariates,}
                        \DataTypeTok{sample.ids=}\KeywordTok{c}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"test"}\NormalTok{,}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{),}\KeywordTok{paste0}\NormalTok{(}\StringTok{"ctrl"}\NormalTok{,}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{)))}

\NormalTok{my.diffMeth3=}\KeywordTok{calculateDiffMeth}\NormalTok{(sim.methylBase,}
                               \DataTypeTok{covariates=}\NormalTok{covariates,}
                               \DataTypeTok{overdispersion=}\StringTok{"MN"}\NormalTok{,}
                               \DataTypeTok{test=}\StringTok{"Chisq"}\NormalTok{,}\DataTypeTok{mc.cores=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{methylation-segmentation}{%
\subsection{Methylation segmentation}\label{methylation-segmentation}}

The analysis of methylation dynamics is not exclusively restricted to differentially methylated regions across samples. Apart from this there is also an interest in examining the methylation profiles within the same sample. Usually, depressions in methylation profiles pinpoint regulatory regions like gene promoters that co-localize with CG-dense CpG islands. On the other hand, many gene-body regions are extensively methylated and CpG-poor \citep{Bock2012-oh}. These observations would describe a bimodal model of either hyper- or hypomethylated regions depending on the local density of CpGs \citep{Lovkvist2016-ky}. However, given the detection of CpG-poor regions with locally reduced levels of methylation (on average 30\%) in pluripotent embryonic stem cells and in neuronal progenitors in both mouse and human, a different model also seems reasonable \citep{Stadler2011-iu}. These low-methylated regions (LMRs) are located distal to promoters, have little overlap with CpG islands, and are associated with enhancer marks such as p300 binding sites and H3K27ac enrichment.

Now we are going to try to segment a portion for the H1 human embryonic stem cell line. MethylKit \index{R Packages!\texttt{methylKit}}uses change-point analysis to segment the methylome. In change-point analysis, the change-points of a genome-wide methylation signal are recorded and the genome is partitioned into regions between consecutive change points. CpGs in each segment are similar to each other more than the following segment.
After segmentation, methylKit function \texttt{methSeg()} identifies segments that are further clustered into segment classes using a mixture modeling approach. This clustering is based on only the average methylation level of the segments and allows the detection of distinct methylome features comparable to unmethylated regions (UMRs), lowly methylated regions (LMRs), and fully methylated regions (FMRs) mentioned in Stadler et al.~\citep{Stadler2011-yv}. The code snippet below reads the methylation data from the H1 cell line as a \texttt{GRanges} object, and runs the segmentation with potentially up to classes of segments. Mixture modeling determines the optimal number of segments using a statistic called Bayesian information criterion (BIC). The BIC is a statistic based on model likelihood and helps us select the model that fits the data better. We have set the number of segment classes to try using the \texttt{G=1:4} argument. The \texttt{minSeg} arguments are related to the minimum number of CpGs in the segments. The function \texttt{methSeg()} outputs a diagnostic plot for segmentation. This plot is shown in Figure \ref{fig:segDiag}. It shows methylation values and lengths of segments in each segment class, as well as the BIC for different numbers of segments.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read methylation data}

\NormalTok{methFile=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}\StringTok{"H1.chr21.chr22.rds"}\NormalTok{,}
                     \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{mbw=}\KeywordTok{readRDS}\NormalTok{(methFile)}

\CommentTok{# segment the methylation data}
\NormalTok{res=}\KeywordTok{methSeg}\NormalTok{(mbw,}\DataTypeTok{minSeg=}\DecValTok{10}\NormalTok{,}\DataTypeTok{G=}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{,}
            \DataTypeTok{join.neighbours =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{10-bs-seq-analysis_files/figure-latex/segDiag-1} 

}

\caption{Segmentation characteristics shown in different plots. Top left: Mean methylation values per segment in each segment class. Top middle: Length of each segment as boxplots for each segment class. Top right: Number of segments in each segment class. Bottom left: Distribution of segment methylation values. Bottom right: BIC for different number of segment classes}\label{fig:segDiag}
\end{figure}

In this case, we know that BIC does not improve much after 4 segment classes. Now, we will not have a look at the characteristics of the segment classes. We are going to plot mean the methylation value and the length of the segment as a scatter plot; the result of this plot is shown in Figure \ref{fig:segplot}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot }
\KeywordTok{plot}\NormalTok{(res}\OperatorTok{$}\NormalTok{seg.mean,}
     \KeywordTok{log10}\NormalTok{(}\KeywordTok{width}\NormalTok{(res)),}\DataTypeTok{pch=}\DecValTok{20}\NormalTok{,}
     \DataTypeTok{col=}\NormalTok{scales}\OperatorTok{::}\KeywordTok{alpha}\NormalTok{(}\KeywordTok{rainbow}\NormalTok{(}\DecValTok{4}\NormalTok{)[}\KeywordTok{as.numeric}\NormalTok{(res}\OperatorTok{$}\NormalTok{seg.group)], }\FloatTok{0.2}\NormalTok{),}
     \DataTypeTok{ylab=}\StringTok{"log10(length)"}\NormalTok{,}
     \DataTypeTok{xlab=}\StringTok{"methylation proportion"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{10-bs-seq-analysis_files/figure-latex/segplot-1} 

}

\caption{Scatter plot of segment mean, methylation values versus segment length. Each dot is a segment identified by the `methSeg()` function.}\label{fig:segplot}
\end{figure}

The highly methylated segment classes that have more than 70\% methylation are usually longer; the median length is 17889 bp. The segment class that has the lowest methylation values have the median length of 1376 bp and the shortest segment class has low to medium methylation level, with median length of 412 bp.

\hypertarget{working-with-large-files}{%
\subsection{Working with large files}\label{working-with-large-files}}

We might want to perform differential methylation analysis in R using whole genome methylation data of multiple samples. The problem is that for genome-wide experiments, file sizes can easily range from hundreds of megabytes to gigabytes and processing multiple instances of those files in memory (RAM) might become unfeasible unless we have access to a high-performance compute cluster (HPC) with extensive RAM. If we want to use a desktop computer or laptop with limited RAM, we either need to restrict our analysis to a subset of the data or use packages that can handle this situation.

The methylKit package provides the capability of dealing with large files and high numbers of samples by exploiting flat file databases to substitute in-memory objects. The internal data, apart from meta information, has a tabular structure storing chromosome, start/end position, and strand information of the associated CpG base just like many other biological formats like BED, GFF or SAM. By exporting this tabular data into a TAB-delimited file and making sure it is accordingly position-sorted, it can be indexed using the generic \href{http://www.htslib.org/doc/tabix.html}{tabix tool}. In general, tabix indexing is a generalization of BAM\index{BAM file} indexing for generic TAB-delimited files. It inherits all the advantages of BAM indexing, including data compression and efficient random access in terms of few seek function calls per query \citep{Li2011-wc}. \texttt{MethylKit} relies on \href{http://bioconductor.org/packages/release/bioc/html/Rsamtools.html}{\texttt{Rsamtools}} which implements tabix functionality for R. This way internal methylKit objects can be efficiently stored as a compressed file on the disk and still \index{R Packages!\texttt{Rsamtools}}be quickly accessed. Another advantage is that existing compressed files can be loaded in interactive sessions, allowing the backup and transfer of intermediate analysis results.

\texttt{methylKit} provides the capability for storing objects in tabix format within various functions. Every methylKit object has its tabix-based flat-file database equivalent. For example, when reading a methylation call file, the \texttt{dbtype} argument can be provided, which will create tabix-based objects.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ myobj=}\KeywordTok{methRead}\NormalTok{( file.list,}
               \DataTypeTok{sample.id=}\KeywordTok{list}\NormalTok{(}\StringTok{"test1"}\NormalTok{,}\StringTok{"test2"}\NormalTok{,}\StringTok{"ctrl1"}\NormalTok{,}\StringTok{"ctrl2"}\NormalTok{),}
               \DataTypeTok{assembly=}\StringTok{"hg18"}\NormalTok{,}\DataTypeTok{treatment=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}
               \DataTypeTok{dbtype=}\StringTok{"tabix"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

The advantage of tabix-based objects is of course saving memory and more efficient parallelization for differential methylation calculation. However, since the data is written to a file and indexed whenever a new object is created, working with tabix-based objects will be slower at certain steps of the analysis compared to in-memory objects.

\hypertarget{annotation-of-dmrsdmcs-and-segments}{%
\section{Annotation of DMRs/DMCs and segments}\label{annotation-of-dmrsdmcs-and-segments}}

The regions of interest obtained through differential methylation or segmentation analysis often need to be integrated with genome annotation datasets. Without this type of integration, differential methylation or segmentation results will be hard to interpret in biological terms. The most common annotation task is to see where regions of interest land in relation to genes and gene parts and regulatory regions: Do they mostly occupy promoter, intronic or exonic regions? Do they overlap with repeats? Do they overlap with other epigenomic markers or long-range regulatory regions? These questions are not specific to methylation −nearly all regions of interest obtained via genome-wide studies have to deal with such questions. Thus, there are already multiple software tools that can produce such annotations. One is the Bioconductor package \href{http://bioconductor.org/packages/release/bioc/html/genomation.html}{\texttt{genomation}}\citep{Akalin2015-yk}. \index{R Packages!\texttt{genomation}}It can be used to annotate DMRs/DMCs and it can also be used to integrate methylation proportions over the genome with other quantitative information and produce meta-gene plots or heatmaps. Below, we are reading a BED file for transcripts and using that to annotate DMCs with promoter/intron/exon/intergenic annotation. The \texttt{genomation::readTranscriptFeatures()} function reads a BED12 file, calculates the coordinates of promoters, exons, and introns and the subsequent function uses that information for annotation.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(genomation)}

\CommentTok{# read the gene BED file}
\NormalTok{transcriptBED=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{, }\StringTok{"refseq.hg18.bed.txt"}\NormalTok{, }
                                           \DataTypeTok{package =} \StringTok{"methylKit"}\NormalTok{)}
\NormalTok{gene.obj=}\KeywordTok{readTranscriptFeatures}\NormalTok{(transcriptBED)}
\CommentTok{#}
\CommentTok{# annotate differentially methylated CpGs with }
\CommentTok{# promoter/exon/intron using annotation data}
\CommentTok{#}
\KeywordTok{annotateWithGeneParts}\NormalTok{(}\KeywordTok{as}\NormalTok{(all.diff,}\StringTok{"GRanges"}\NormalTok{),gene.obj)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   promoter       exon     intron intergenic 
##      28.24      15.27      33.59      58.02 
##   promoter       exon     intron intergenic 
##      28.24       0.00      13.74      58.02 
## promoter     exon   intron 
##     0.29     0.03     0.17 
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##       5     815   49918   52410   94644  313528
\end{verbatim}

Similarly, we can read the CpG island annotation and annotate our differentially methylated bases/regions with them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read the shores and flanking regions and name the flanks as shores }
\CommentTok{# and CpG islands as CpGi}
\NormalTok{cpg.file=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{, }\StringTok{"cpgi.hg18.bed.txt"}\NormalTok{, }
                                        \DataTypeTok{package =} \StringTok{"methylKit"}\NormalTok{)}
\NormalTok{cpg.obj=}\KeywordTok{readFeatureFlank}\NormalTok{(cpg.file,}
                           \DataTypeTok{feature.flank.name=}\KeywordTok{c}\NormalTok{(}\StringTok{"CpGi"}\NormalTok{,}\StringTok{"shores"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: 'GenomicRangesList' is deprecated.
## Use 'GRangesList(..., compress=FALSE)' instead.
## See help("Deprecated")
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#}
\CommentTok{# convert methylDiff object to GRanges and annotate}
\NormalTok{diffCpGann=}\KeywordTok{annotateWithFeatureFlank}\NormalTok{(}\KeywordTok{as}\NormalTok{(all.diff,}\StringTok{"GRanges"}\NormalTok{),}
\NormalTok{                                    cpg.obj}\OperatorTok{$}\NormalTok{CpGi,cpg.obj}\OperatorTok{$}\NormalTok{shores,}
                         \DataTypeTok{feature.name=}\StringTok{"CpGi"}\NormalTok{,}\DataTypeTok{flank.name=}\StringTok{"shores"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Besides these, DMRs/DMCs might be associated with changes in gene regulation. It might be desirable to overlap them with known transcription binding sites or motifs or histone modifications. These are simply overlap operations for these kinds of analysis. You can use the \texttt{genomation::annotateWithFeature()} function or any other approach shown in Chapter \ref{genomicIntervals}, and you can also do motif discovery with methods shown in Chapter \ref{chipseq}.

\hypertarget{further-annotation-with-genes-or-gene-sets}{%
\subsection{Further annotation with genes or gene sets}\label{further-annotation-with-genes-or-gene-sets}}

The next obvious steps for annotating your DMRs/DMCs are figuring out which genes they are associated with. Figuring out which genes are associated with your regions of interest can give a better idea of the biological implications of the methylation changes. Once you have your gene set, you can do gene set analysis as shown in Chapter \ref{rnaseqanalysis} or in Chapter \ref{multiomics}. There are also packages such as \href{https://www.bioconductor.org/packages/release/bioc/html/rGREAT.html}{\texttt{rGREAT}} that can simultaneously associate DMRs or any other region of interest to genes and do gene set analysis.

\hypertarget{other-r-packages-that-can-be-used-for-methylation-analysis}{%
\section{Other R packages that can be used for methylation analysis}\label{other-r-packages-that-can-be-used-for-methylation-analysis}}

\begin{itemize}
\tightlist
\item
  \href{http://bioconductor.org/packages/release/bioc/html/genomation.html}{DSS} beta-binomial models with empirical Bayes for moderating dispersion.
\item
  \href{http://bioconductor.org/packages/release/bioc/html/BSseq.html}{BSseq} Regional differential methylation analysis using smoothing and linear-regression-based tests.
\item
  \href{http://bioconductor.org/packages/release/bioc/html/BiSeq.html}{BiSeq} Regional differential methylation analysis using beta-binomial models.
\item
  \href{http://bioconductor.org/packages/release/bioc/html/MethylSeekR.html}{MethylSeekR}: Methylome segmentation using HMM and cutoffs.
\item
  \href{http://bioconductor.org/packages/release/bioc/html/QuasR.html}{QuasR}: Methylation aware alignment and methylation calling, as well as fastQC-like fastq raw data quality check features.
\end{itemize}

\hypertarget{exercises-8}{%
\section{Exercises}\label{exercises-8}}

\hypertarget{differential-methylation-1}{%
\subsection{Differential methylation}\label{differential-methylation-1}}

The main objective of this exercise is getting differential methylated cytosines between two groups of samples: IDH-mut (AML patients with IDH mutations) vs.~NBM (normal bone marrow samples).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Download methylation call files from GEO. These files are readable by methlKit using default \texttt{methRead} arguments. {[}Difficulty: \textbf{Beginner}{]}
\end{enumerate}

\begin{longtable}[]{@{}ll@{}}
\toprule
samples & Link\tabularnewline
\midrule
\endhead
IDH1\_rep1 & \href{https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSM919990\&format=file\&file=GSM919990\%5FIDH\%2Dmut\%5F1\%5FmyCpG\%2Etxt\%2Egz}{link}\tabularnewline
IDH1\_rep2 & \href{https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSM919991\&format=file\&file=GSM919991\%5FIDH\%5Fmut\%5F2\%5FmyCpG\%2Etxt\%2Egz}{link}\tabularnewline
NBM\_rep1 & \href{https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSM919982\&format=file\&file=GSM919982\%5FNBM\%5F1\%5FmyCpG\%2Etxt\%2Egz}{link}\tabularnewline
NBM\_rep2 & \href{https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSM919984\&format=file\&file=GSM919984\%5FNBM\%5F2\%5FRep1\%5FmyCpG\%2Etxt\%2Egz}{link}\tabularnewline
\bottomrule
\end{longtable}

Example code for reading a file:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(methylKit)}
\NormalTok{m=}\KeywordTok{methRead}\NormalTok{(}\StringTok{"~/Downloads/GSM919982_NBM_1_myCpG.txt.gz"}\NormalTok{,}
           \DataTypeTok{sample.id =} \StringTok{"idh"}\NormalTok{,}\DataTypeTok{assembly=}\StringTok{"hg18"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Find differentially methylated cytosines. Use chr1 and chr2 only if you need to save time. You can subset it after you download the files either in R or Unix. The files are for hg18 assembly of human genome. {[}Difficulty: \textbf{Beginner}{]}
\item
  Describe the general differential methylation trend, what is the main effect for most CpGs? {[}Difficulty: \textbf{Intermediate}{]}
\item
  Annotate differentially methylated cytosines (DMCs) as promoter/intron/exon? {[}Difficulty: \textbf{Beginner}{]}
\item
  Which genes are the nearest to DMCs? {[}Difficulty: \textbf{Intermediate}{]}
\item
  Can you do gene set analysis either in R or via web-based tools? {[}Difficulty: \textbf{Advanced}{]}
\end{enumerate}

\hypertarget{methylome-segmentation}{%
\subsection{Methylome segmentation}\label{methylome-segmentation}}

The main objective of this exercise is to learn how to do methylome segmentation and the downstream analysis for annotation and data integration.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Download the human embryonic stem-cell (H1 Cell Line) methylation bigWig files from the \href{http://egg2.wustl.edu/roadmap/web_portal/processed_data.html\#MethylData}{Roadmap Epigenomics website}. It may take a while to understand how the website is structured and which bigWig file to use. That is part of the exercise. The files you will download are for hg19 assembly unless stated otherwise. {[}Difficulty: \textbf{Beginner}{]}
\item
  Do segmentation on hESC methylome. You can only use chr1 if using the whole genome takes too much time. {[}Difficulty: \textbf{Intermediate}{]}
\item
  Annotate segments and the kinds of gene-based features each segment class overlaps with (promoter/exon/intron) {[}Difficulty: \textbf{Beginner}{]}
\item
  For each segment type, annotate the segments with chromHMM annotations from the Roadmap Epigenome database available \href{https://egg2.wustl.edu/roadmap/web_portal/chr_state_learning.html\#core_15state}{here}. The specific file you should use is \href{https://egg2.wustl.edu/roadmap/data/byFileType/chromhmmSegmentations/ChmmModels/coreMarks/jointModel/final/E003_15_coreMarks_mnemonics.bed.gz}{here}. This is a bed file with chromHMM annotations. chromHMM annotations are parts of the genome identified by a hidden-markov-model-based machine learning algorithm. The segments correspond to active promoters, enhancers, active transcription, insulators, etc. The chromHMM model uses histone modification ChIP-seq and potentially other ChIP-seq data sets to annotate the genome.{[}Difficulty: \textbf{Advanced}{]}
\end{enumerate}

\hypertarget{multiomics}{%
\chapter{Multi-omics Analysis}\label{multiomics}}

\emph{Chapter Author}: \textbf{Jonathan Ronen}

\index{multi-omics}Living cells are a symphony of complex processes. Modern sequencing technology has led to many comprehensive assays being routinely available to experimenters, giving us different ways to peek at the internal doings of the cells, each experiment revealing a different part of some underlying processes. As an example, most cells have the same DNA, but sequencing the genome of a cell allows us to find mutations and structural alterations that drive tumerogenesis in cancer. If we treat the DNA with bisulfite prior to sequencing, cytosine residues are converted to uracil, but 5-methylcytosine residues are unaffected. This allows us to probe the methylation patterns of the genome, or its methylome. By sequencing the mRNA molecules in a cell, we can calculate the abundance, in different samples, of different mRNA transcripts, or uncover its transcriptome. Performing different experiments on the same samples, for instance RNA-seq, DNA-seq, and BS-seq, results in multi-dimensional omics datasets, which enable the study of relationships between different biological processes, e.g.~DNA methylation, mutations, and gene expression, and the leveraging of multiple data types to draw inferences about biological systems. This chapter provides an overview of some of the available methods for such analyses, focusing on matrix factorization approaches. In the examples in this chapter we will demonstrate how these methods are applicable to cancer molecular subtyping, i.e.~finding tumors which are driven by the same molecular processes.

\hypertarget{use-case-multi-omics-data-from-colorectal-cancer}{%
\section{Use case: Multi-omics data from colorectal cancer}\label{use-case-multi-omics-data-from-colorectal-cancer}}

\index{multi-omics}\index{colorectal cancer}The examples in this chapter will use the following data: a set of 121 tumors from the TCGA \citep{tcga_pan_cancer} colorectal cancer cohort. The tumors have been profiled for gene expression using RNA-seq, mutations using Exome-seq, and copy number variations using genotyping arrays. Projects such as TCGA have turbocharged efforts to sub-divide cancer into subtypes. Although two tumors arise in the colon, they may have distinct molecular profiles, which is important for treatment decisions. The subset of tumors used in this chapter belong to two distinct molecular subtypes defined by the Colorectal Cancer Subtyping Consortium \citep{cmscc}, \emph{CMS1} and \emph{CMS3}. The following code snippets load this multi-omics data from the companion package, starting with gene expression data from RNA-seq (see Chapter \ref{rnaseqanalysis}). Below we are reading the RNA-seq data from the \texttt{compGenomRData} package.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read in the csv from the companion package as a data frame}
\NormalTok{csvfile <-}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{, }\StringTok{"multi-omics"}\NormalTok{, }\StringTok{"COREAD_CMS13_gex.csv"}\NormalTok{, }
                       \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{x1 <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(csvfile, }\DataTypeTok{row.names=}\DecValTok{1}\NormalTok{)}
\CommentTok{# Fix the gene names in the data frame}
\KeywordTok{rownames}\NormalTok{(x1) <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\KeywordTok{strsplit}\NormalTok{(}\KeywordTok{rownames}\NormalTok{(x1), }\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{|"}\NormalTok{), }\ControlFlowTok{function}\NormalTok{(x) x[}\DecValTok{1}\NormalTok{])}
\CommentTok{# Output a table}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\KeywordTok{head}\NormalTok{(}\KeywordTok{t}\NormalTok{(}\KeywordTok{head}\NormalTok{(x1))), }\DataTypeTok{caption=}\StringTok{"Example gene expression data (head)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:moloadMultiomicsGE}Example gene expression data (head)}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r}
\hline
  & RNF113A & S100A13 & AP3D1 & ATP6V1G1 & UBQLN4 & TPPP3\\
\hline
TCGA.A6.2672 & 21.19567 & 19.72600 & 11.53022 & 0.00000 & 15.35637 & 12.76747\\
\hline
TCGA.A6.3809 & 21.50866 & 18.65729 & 12.98830 & 14.12675 & 19.62208 & 0.00000\\
\hline
TCGA.A6.5661 & 20.08072 & 18.97034 & 10.83759 & 15.31325 & 0.00000 & 0.00000\\
\hline
TCGA.A6.5665 & 0.00000 & 11.88336 & 10.24248 & 19.79300 & 0.00000 & 0.00000\\
\hline
TCGA.A6.6653 & 0.00000 & 12.07753 & 0.00000 & 0.00000 & 0.00000 & 0.00000\\
\hline
TCGA.A6.6780 & 0.00000 & 12.99128 & 0.00000 & 19.96976 & 13.17618 & 11.58742\\
\hline
\end{tabular}
\end{table}

Table \ref{tab:moloadMultiomicsGE} shows the head of the gene expression matrix. The rows correspond to patients, referred to by their TCGA identifier, as the first column of the table. Columns represent the genes, and the values are RPKM expression values. The column names are the names or symbols of the genes.
The details about how these expression values are calculated are in Chapter \ref{rnaseqanalysis}.

We first \textbf{read mutation data} with the following code snippet.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read in the csv from the companion package as a data frame}
\NormalTok{csvfile <-}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{, }\StringTok{"multi-omics"}\NormalTok{, }\StringTok{"COREAD_CMS13_muts.csv"}\NormalTok{, }
                       \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{x2 <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(csvfile, }\DataTypeTok{row.names=}\DecValTok{1}\NormalTok{)}
\CommentTok{# Set mutation data to be binary (so if a gene has more than 1 mutation,}
\CommentTok{# we only count one)}
\NormalTok{x2[x2}\OperatorTok{>}\DecValTok{0}\NormalTok{]=}\DecValTok{1}
\CommentTok{# output a table}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\KeywordTok{head}\NormalTok{(}\KeywordTok{t}\NormalTok{(}\KeywordTok{head}\NormalTok{(x2))), }\DataTypeTok{caption=}\StringTok{"Example mutation data (head)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:moloadMultiomicsMUT}Example mutation data (head)}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r}
\hline
  & TTN & TP53 & APC & KRAS & SYNE1 & MUC16\\
\hline
TCGA.A6.2672 & 1 & 0 & 0 & 0 & 1 & 1\\
\hline
TCGA.A6.3809 & 1 & 0 & 0 & 0 & 0 & 0\\
\hline
TCGA.A6.5661 & 1 & 0 & 0 & 0 & 1 & 1\\
\hline
TCGA.A6.5665 & 1 & 0 & 0 & 0 & 1 & 1\\
\hline
TCGA.A6.6653 & 1 & 0 & 0 & 1 & 0 & 0\\
\hline
TCGA.A6.6780 & 1 & 0 & 0 & 0 & 0 & 1\\
\hline
\end{tabular}
\end{table}

Table \ref{tab:moloadMultiomicsMUT} shows the mutations of these tumors (mutations were introduced in Chapter \ref{intro}). In the mutation matrix, each cell is a binary 1/0, indicating whether or not a tumor has a non-synonymous mutation in the gene indicated by the column. These types of mutations change the aminoacid sequence, therefore they are likely to change the function of the protein.

Next, we \textbf{read copy number data} with the following code snippet.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read in the csv from the companion package as a data frame}
\NormalTok{csvfile <-}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{, }\StringTok{"multi-omics"}\NormalTok{, }\StringTok{"COREAD_CMS13_cnv.csv"}\NormalTok{, }
                       \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{x3 <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(csvfile, }\DataTypeTok{row.names=}\DecValTok{1}\NormalTok{)}
\CommentTok{# output a table}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\KeywordTok{head}\NormalTok{(}\KeywordTok{t}\NormalTok{(}\KeywordTok{head}\NormalTok{(x3))), }
             \DataTypeTok{caption=}\StringTok{"Example copy number data for CRC samples"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:moloadMultiomicsCNV}Example copy number data for CRC samples}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r}
\hline
  & 8p23.2 & 8p23.3 & 8p23.1 & 8p21.3 & 8p12 & 8p22\\
\hline
TCGA.A6.2672 & 0 & 0 & 0 & 0 & 0 & 0\\
\hline
TCGA.A6.3809 & 0 & 0 & 0 & 0 & 0 & 0\\
\hline
TCGA.A6.5661 & 0 & 0 & 0 & 0 & 0 & 0\\
\hline
TCGA.A6.5665 & 0 & 0 & 0 & 0 & 0 & 0\\
\hline
TCGA.A6.6653 & 0 & 0 & 0 & 0 & 0 & 0\\
\hline
TCGA.A6.6780 & 0 & 0 & 0 & 0 & 0 & 0\\
\hline
\end{tabular}
\end{table}

Finally, table \ref{tab:moloadMultiomicsCNV} shows GISTIC scores \citep{mermel2011gistic2} for copy number alterations in these tumors. During transformation from healthy cells to cancer cells, the genome sometimes undergoes large-scale instability; large segments of the genome might be replicated or lost. This will be reflected in each segment's ``copy number''. In this matrix, each column corresponds to a chromosome segment, and the value of the cell is a real-valued score indicating if this segment has been amplified (copied more) or lost, relative to a non-cancer control from the same patient.

Each of the data types (gene expression, mutations, copy number variation) on its own, provides some signal which allows us to somewhat separate the samples into the two different subtypes. In order to explore these relations, we must first obtain the subtypes of these tumors. The following code snippet reads these, also from the companion package:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read in the csv from the companion package as a data frame}
\NormalTok{csvfile <-}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{, }\StringTok{"multi-omics"}\NormalTok{, }\StringTok{"COREAD_CMS13_subtypes.csv"}\NormalTok{,}
                       \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{covariates <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(csvfile, }\DataTypeTok{row.names=}\DecValTok{1}\NormalTok{)}
\CommentTok{# Fix the TCGA identifiers so they match up with the omics data}
\KeywordTok{rownames}\NormalTok{(covariates) <-}\StringTok{ }\KeywordTok{gsub}\NormalTok{(}\DataTypeTok{pattern =} \StringTok{'-'}\NormalTok{, }\DataTypeTok{replacement =} \StringTok{'}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{.'}\NormalTok{,}
                             \KeywordTok{rownames}\NormalTok{(covariates))}
\NormalTok{covariates <-}\StringTok{ }\NormalTok{covariates[}\KeywordTok{colnames}\NormalTok{(x1),]}
\CommentTok{# create a dataframe which will be used to annotate later graphs}
\NormalTok{anno_col <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{cms=}\KeywordTok{as.factor}\NormalTok{(covariates}\OperatorTok{$}\NormalTok{cms_label))}
\KeywordTok{rownames}\NormalTok{(anno_col) <-}\StringTok{ }\KeywordTok{rownames}\NormalTok{(covariates)}
\end{Highlighting}
\end{Shaded}

Before proceeding with any multi-omics integration analysis which might obscure the underlying data, it is important to take a look at each omic data type on its own, and in this case in particular, to examine their relation to the underlying condition, i.e.~the cancer subtype. A great way to get an eagle-eye view of such large data is using heatmaps (see Chapter \ref{unsupervisedLearning} for more details).

We will first check the gene expression data in relation to the subtypes. One way of doing that is plotting a heatmap and clustering the tumors, while displaying a color annotation atop the heatmap, indicating which subtype each tumor belongs to. This is shown in Figure \ref{fig:mogeneExpressionHeatmap}, which is generated by the following code snippet:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pheatmap}\OperatorTok{::}\KeywordTok{pheatmap}\NormalTok{(x1,}
                   \DataTypeTok{annotation_col =}\NormalTok{ anno_col,}
                   \DataTypeTok{show_colnames =} \OtherTok{FALSE}\NormalTok{,}
                   \DataTypeTok{show_rownames =} \OtherTok{FALSE}\NormalTok{,}
                   \DataTypeTok{main=}\StringTok{"Gene expression data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{11-multiomics-analysis_files/figure-latex/mogeneExpressionHeatmap-1} 

}

\caption{Heatmap of gene expression data for colorectal cancers}\label{fig:mogeneExpressionHeatmap}
\end{figure}

In Figure \ref{fig:mogeneExpressionHeatmap}, each column is a tumor, and each row is a gene. The values in the cells are FPKM values. There is another band above the heatmap annotating each column (tumor) with its corresponding subtype. The tumors are clustered using hierarchical clustering denoted by the dendrogram above the heatmap, according to which the columns (tumors) are ordered. While this ordering corresponds somewhat to the subtypes, it would not be possible to cut this dendrogram in a way which achieves perfect separation between the subtypes.

Next we repeat the same exercise using the mutation data. The following snippet generates Figure \ref{fig:momutationsHeatmap}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pheatmap}\OperatorTok{::}\KeywordTok{pheatmap}\NormalTok{(x2,}
                   \DataTypeTok{annotation_col =}\NormalTok{ anno_col,}
                   \DataTypeTok{show_colnames =} \OtherTok{FALSE}\NormalTok{,}
                   \DataTypeTok{show_rownames =} \OtherTok{FALSE}\NormalTok{,}
                   \DataTypeTok{main=}\StringTok{"Mutation data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{11-multiomics-analysis_files/figure-latex/momutationsHeatmap-1} 

}

\caption{Heatmap of mutation data for colorectal cancers}\label{fig:momutationsHeatmap}
\end{figure}

An examination of Figure \ref{fig:momutationsHeatmap} shows that tumors clustered and ordered by mutation data correspond very closely to their CMS subtypes. However, one should be careful in drawing conclusions about this result. Upon closer examination, you might notice that the separating factor seems to be that CMS1 tumors have significantly more mutations than do CMS3 tumors. This, rather than mutations in a specific genes, seems to be driving this clustering result. Nevertheless, this hyper-mutated status is an important indicator for this subtype.

Finally, we look into copy number variation data and try to see if clustered samples are in concordance with subtypes. The following code snippet generates Figure \ref{fig:moCNVHeatmap}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pheatmap}\OperatorTok{::}\KeywordTok{pheatmap}\NormalTok{(x3,}
                   \DataTypeTok{annotation_col =}\NormalTok{ anno_col,}
                   \DataTypeTok{show_colnames =} \OtherTok{FALSE}\NormalTok{,}
                   \DataTypeTok{show_rownames =} \OtherTok{FALSE}\NormalTok{,}
                   \DataTypeTok{main=}\StringTok{"Copy number data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{11-multiomics-analysis_files/figure-latex/moCNVHeatmap-1} 

}

\caption{Heatmap of copy number variation data, colorectal cancers}\label{fig:moCNVHeatmap}
\end{figure}

The interpretation of Figure \ref{fig:moCNVHeatmap} is left as an exercise for the reader.

It is clear that while there is some ``signal'' in each of these omics types, as is evident from these heatmaps, it is equally clear that none of these omics types completely and on its own explains the subtypes. Each omics type provides but a glimpse into what makes each of these tumors different from a healthy cell. Through the rest of this chapter, we will demonstrate how analyzing the gene expression, mutations, and copy number variations, in tandem, we will be able to get a better picture of what separates these cancer subtypes.

The next section will describe latent variable models for multi-omics integrations. Latent variable models are a form of dimensionality reduction (see Chapter \ref{unsupervisedLearning}). Each omics data type is ``big data'' in its own right; a typical RNA-seq experiment profiles upwards of 50 thousand different transcripts. The difficulties in handling large data matrices are only exacerbated by the introduction of more omics types into the analysis, as we are suggesting here. In order to overcome these challenges, latent variable models are a powerful way to reduce the dimensionality of the data down to a manageable size.

\hypertarget{latent-variable-models-for-multi-omics-integration}{%
\section{Latent variable models for multi-omics integration}\label{latent-variable-models-for-multi-omics-integration}}

\index{unsupervised learning}Unsupervised multi-omics integration methods are methods that look for patterns within and across data types, in a label-agnostic fashion, i.e.~without knowledge of the identity or label of the analyzed samples (e.g.~cell type, tumor/normal). This chapter focuses on latent variable models, a form of dimensionality reduction technique (see Chapter \ref{unsupervisedLearning}). Latent variable models make an assumption that the high-dimensional data we observe (e.g.~counts of tens of thousands of mRNA molecules) arise from a lower dimension description. The variables in that lower dimensional description are termed \emph{latent variables}, as they are believed to be latent in the data, but not directly observable through experimentation. Therefore, there is a need for methods to infer the latent variables from the data. For instance, (see Chapter \ref{rnaseqanalysis} for details of RNA-seq analysis) the relative abundance of different mRNA molecules in a cell is largely determined by the cell type. There are other experiments which may be used to discern the cell type of cells (e.g.~looking at them under a microscope), but an RNA-seq experiment does not, directly, reveal whether the analyzed sample was taken from one organ or another. A latent variable model would set the cell type as a latent variable, and the observable abundance of mRNA molecules to be dependent on the value of the latent variable (e.g.~if the latent variable is ``Regulatory T-cell'', we would expect to find high expression of CD4, FOXP3, and CD25).

\hypertarget{matrix-factorization-methods-for-unsupervised-multi-omics-data-integration}{%
\section{Matrix factorization methods for unsupervised multi-omics data integration}\label{matrix-factorization-methods-for-unsupervised-multi-omics-data-integration}}

\index{dimensionality reduction}\index{matrix factorization}Matrix factorization techniques attempt to infer a set of latent variables from the data by finding factors of a data matrix. Principal Component Analysis (introduced in Chapter \ref{unsupervisedLearning}) is a form of matrix factorization which finds factors based on the covariance structure of the data. Generally, matrix factorization methods may be formulated as

\[
X = WH,
\]
where \(X\) is the \emph{data matrix}, \([M \times N]\) where \(M\) is the number of features (typically genes), and \(N\) is the number of samples. \(W\) is an \([M \times K]\) \emph{factors} matrix, and \(H\) is the \([K \times N]\) \emph{latent variable coefficient matrix}. Tying this back to PCA, where \(X = U \Sigma V^T\), we may formulate the factorization in the same terms by setting \(W=U\Sigma\) and \(H=V^T\). If \(K=rank(X)\), this factorization is lossless, i.e.~\(X=WH\). However if we choose \(K<rank(X)\), the factorization is lossy, i.e.~\(X \approx WH\). In that case, matrix factorization methods normally opt to minimize the error

\[
min~\|X-WH\|.
\]
As we normally seek a latent variable model with a considerably lower dimensionality than \(X\), this is the more common case.

The loss function we choose to minimize may be further subject to some constraints or regularization terms\index{regularization}\index{loss function}\index{optimization}. Regularization has been introduced in Chapter \ref{supervisedLearning}. In the current context of latent factor models, a regularization term might be added to the loss function, i.e.~we might choose to minimize \(min~\|X-WH\| + \lambda \|W\|^2\) (this is called \(L_2\)-regularization) instead of merely the reconstruction error. Adding such a term to our loss function here will push the \(W\) matrix entries towards 0, in effect balancing between better reconstruction of the data and a more parsimonious model. A more parsimonious latent factor model is one with more sparsity in the latent factors. This sparsity is desirable for model interpretation, as will become evident in later sections.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{images/matrix_factorization} 

}

\caption{General matrix factorization framework. The data matrix on the left-hand side is decomposed into factors on the right-hand side. The equality may be an approximation as some matrix factorization methods are lossless (exact), while others are an approximation.}\label{fig:momatrixFactorization}
\end{figure}

In Figure \ref{fig:momatrixFactorization}, the \(5 \times 4\) data matrix \(X\) is decomposed to a 2-dimensional latent variable model.

\hypertarget{multiple-factor-analysis}{%
\subsection{Multiple factor analysis}\label{multiple-factor-analysis}}

\index{multiple factor analysis}Multiple factor analysis is a natural starting point for a discussion about matrix factorization methods for integrating multiple data types. It is a straightforward extension of PCA into the domain of multiple data types \footnote{When dealing with categorical variables, MFA uses MCA (Multiple Correspondence Analysis). This is less relevant to biological data analysis and will not be discussed here}.

Figure \ref{fig:moMFA} sketches a naive extension of PCA to a multi-omics context.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/mfa} 

}

\caption{A naive extension of PCA to multi-omics; data matrices from different platforms are stacked, before applying PCA.}\label{fig:moMFA}
\end{figure}

Formally, we have
\[
X = \begin{bmatrix}
           X_{1} \\
           X_{2} \\
           \vdots \\
           X_{L}
         \end{bmatrix} = WH,
\]
a joint decomposition of the different data matrices (\(X_i\)) into the factor matrix \(W\) and the latent variable matrix \(H\). This way, we can leverage the ability of PCA to find the highest variance decomposition of the data, when the data consists of different omics types. As a reminder, PCA finds the linear combinations of the features which, when the data is projected onto them, preserve the most variance of any \(K\)-dimensional space. But because measurements from different experiments have different scales, they will also have variance (and co-variance) at different scales.

Multiple Factor Analysis addresses this issue and achieves balance among the data types by normalizing each of the data types, before stacking them and passing them on to PCA. Formally, MFA is given by

\[
X_n = \begin{bmatrix}
           X_{1} / \lambda^{(1)}_1 \\
           X_{2} / \lambda^{(2)}_1 \\
           \vdots \\
           X_{L} / \lambda^{(L)}_1
         \end{bmatrix} = WH,
\]
where \(\lambda^{(i)}_1\) is the first eigenvalue of the principal component decomposition of \(X_i\).

Following this normalization step, we apply PCA to \(X_n\). From there on, MFA analysis is the same as PCA analysis, and we refer the reader to Chapter \ref{unsupervisedLearning} for more details.

\hypertarget{mfa-in-r}{%
\subsubsection{MFA in R}\label{mfa-in-r}}

MFA is available through the CRAN package \texttt{FactoMineR}. The code snippet below shows how to run it:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# run the MFA function from the FactoMineR package}
\NormalTok{r.mfa <-}\StringTok{ }\NormalTok{FactoMineR}\OperatorTok{::}\KeywordTok{MFA}\NormalTok{(}
  \KeywordTok{t}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(x1,x2,x3)), }\CommentTok{# binding the omics types together}
  \KeywordTok{c}\NormalTok{(}\KeywordTok{dim}\NormalTok{(x1)[}\DecValTok{1}\NormalTok{], }\KeywordTok{dim}\NormalTok{(x2)[}\DecValTok{1}\NormalTok{], }\KeywordTok{dim}\NormalTok{(x3)[}\DecValTok{1}\NormalTok{]), }\CommentTok{# specifying the dimensions of each}
  \DataTypeTok{graph=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Since this generates a two-dimensional factorization of the multi-omics data, we can now plot each tumor as a dot in a 2D scatter plot to see how well the MFA factors separate the cancer subtypes. The following code snippet generates Figure \ref{fig:momfascatterplot}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# first, extract the H and W matrices from the MFA run result}
\NormalTok{mfa.h <-}\StringTok{ }\NormalTok{r.mfa}\OperatorTok{$}\NormalTok{global.pca}\OperatorTok{$}\NormalTok{ind}\OperatorTok{$}\NormalTok{coord}
\NormalTok{mfa.w <-}\StringTok{ }\NormalTok{r.mfa}\OperatorTok{$}\NormalTok{quanti.var}\OperatorTok{$}\NormalTok{coord}

\CommentTok{# create a dataframe with the H matrix and the CMS label}
\NormalTok{mfa_df <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(mfa.h)}
\NormalTok{mfa_df}\OperatorTok{$}\NormalTok{subtype <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(covariates[}\KeywordTok{rownames}\NormalTok{(mfa_df),]}\OperatorTok{$}\NormalTok{cms_label)}

\CommentTok{# create the plot}
\NormalTok{ggplot2}\OperatorTok{::}\KeywordTok{ggplot}\NormalTok{(mfa_df, ggplot2}\OperatorTok{::}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Dim}\FloatTok{.1}\NormalTok{, }\DataTypeTok{y=}\NormalTok{Dim}\FloatTok{.2}\NormalTok{, }\DataTypeTok{color=}\NormalTok{subtype)) }\OperatorTok{+}
\NormalTok{ggplot2}\OperatorTok{::}\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\NormalTok{ggplot2}\OperatorTok{::}\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Scatter plot of MFA"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{11-multiomics-analysis_files/figure-latex/momfascatterplot-1} 

}

\caption{Scatter plot of 2-dimensional MFA for multi-omics data shows separation between the subtypes.}\label{fig:momfascatterplot}
\end{figure}

Figure \ref{fig:momfascatterplot} shows remarkable separation between the cancer subtypes; it is easy enough to draw a line separating the tumors to CMS subtypes with good accuracy.

Another way to examine the MFA factors, which is also useful for factor models with more than two components, is a heatmap, as shown in Figure \ref{fig:momfaheatmap}, generated by the following code snippet:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pheatmap}\OperatorTok{::}\KeywordTok{pheatmap}\NormalTok{(}\KeywordTok{t}\NormalTok{(mfa.h)[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,], }\DataTypeTok{annotation_col =}\NormalTok{ anno_col,}
                  \DataTypeTok{show_colnames =} \OtherTok{FALSE}\NormalTok{,}
                  \DataTypeTok{main=}\StringTok{"MFA for multi-omics integration"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{11-multiomics-analysis_files/figure-latex/momfaheatmap-1} 

}

\caption{A heatmap of the two MFA components shows separation between the cancer subtypes.}\label{fig:momfaheatmap}
\end{figure}

Figure \ref{fig:momfaheatmap} shows that indeed, when tumors are clustered and ordered using the two MFA factors we learned above, their separation into CMS clusters is nearly trivial.

\begin{rmdtip}
\begin{rmdtip}

\textbf{Want to know more ?}

\begin{itemize}
\tightlist
\item
  Learn more about FactoMineR on the website: \url{http://factominer.free.fr/}
\item
  Learn more about MFA on the Wikipedia page \url{https://en.wikipedia.org/wiki/Multiple_factor_analysis}
\end{itemize}

\end{rmdtip}
\end{rmdtip}

\hypertarget{joint-non-negative-matrix-factorization}{%
\subsection{Joint non-negative matrix Factorization}\label{joint-non-negative-matrix-factorization}}

\index{non-negative matrix factorization (NMF)}As introduced in Chapter \ref{unsupervisedLearning}, NMF (Non-negative Matrix Factorization) is an algorithm from 2000 that seeks to find a non-negative additive decomposition for a non-negative data matrix. It takes the familiar form \(X \approx WH\), with \(X \ge 0\), \(W \ge 0\), and \(H \ge 0\). The non-negative constraints make a lossless decomposition (i.e.~\(X=WH\)) generally impossible. Hence, NMF attempts to find a solution which minimizes the Frobenius norm of the reconstruction:

\[
min~\|X-WH\|_F \\
W \ge 0, \\
H \ge 0,
\]

where the Frobenius norm \(\|\cdot\|_F\) is the matrix equivalent of the Euclidean distance:

\[
\|X\|_F = \sqrt{\sum_i\sum_jx_{ij}^2}.
\]

This is typically solved for \(W\) and \(H\) using random initializations followed by iterations of a multiplicative update rule:

\begin{align}
    W_{t+1} &= W_t^T \frac{XH_t^T}{XH_tH_t^T} \\
    H_{t+1} &= H_t \frac{W_t^TX}{W^T_tW_tX}.
\end{align}

Since this algorithm is guaranteed only to converge to a local minimum, it is typically run several times with random initializations, and the best result is kept.

In the multi-omics context, we will, as in the MFA case, wish to find a decomposition for an integrated data matrix of the form

\[
X = \begin{bmatrix}
    X_{1} \\
    X_{2} \\
    \vdots \\
    X_{L}
\end{bmatrix},
\]

with \(X_i\)s denoting data from different omics platforms.

As NMF seeks to minimize the reconstruction error \(\|X-WH\|_F\), some care needs to be taken with regards to data normalization. Different omics platforms may produce data with different scales (i.e.~real-valued gene expression quantification, binary mutation data, etc.), and so will have different baseline Frobenius norms. To address this, when doing Joint NMF, we first feature-normalize each data matrix, and then normalize by the Frobenius norm of the data matrix. Formally, we run NMF on

\[
X = \begin{bmatrix}
    X_{1}^N / \alpha_1 \\
    X_{2}^N / \alpha_2 \\
    \vdots \\
    X_{L}^N / \alpha_L
\end{bmatrix},
\]

where \(X_i^N\) is the feature-normalized data matrix \(X_i^N = \frac{x^{ij}}{\sum_jx^{ij}}\), and \(\alpha_i = \|X_{i}^N\|_F\).

Another consideration with NMF is the non-negativity constraint. Different omics data types may have negative values, for instance, copy-number variations (CNVs) may be positive, indicating gains, or negative, indicating losses, as in Table \ref{tab:mocnvsplitcolshow1}. In order to turn such data into a non-negative form, we will split each feature into two features, one new feature holding all the non-negative values of the original feature, and another feature holding the absolute value of the negative ones, as in Table \ref{tab:mocnvsplitcolshow2}.

\begin{table}

\caption{\label{tab:mocnvsplitcolshow1}Example copy number data. Data can be both positive (amplified regions) or negative (deleted regions).}
\centering
\begin{tabular}[t]{l|r|r}
\hline
  & seg1 & seg2\\
\hline
samp1 & 1 & 0\\
\hline
samp2 & 2 & 1\\
\hline
samp3 & 1 & -2\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:mocnvsplitcolshow2}Example copy number data after splitting each column into a column representing copy number gains (+) and a column representing deletions (-). This data matrix is non-negative, and thus suitable for NMF algorithms.}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & seg1+ & seg1- & seg2+ & seg2-\\
\hline
samp1 & 1 & 0 & 0 & 0\\
\hline
samp2 & 2 & 0 & 1 & 0\\
\hline
samp3 & 1 & 0 & 0 & 2\\
\hline
\end{tabular}
\end{table}

\hypertarget{nmf-in-r}{%
\subsubsection{NMF in R}\label{nmf-in-r}}

Many NMF algorithms are available through the CRAN package \texttt{NMF}. The following code chunk demonstrates how it may be run:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Feature-normalize the data}
\NormalTok{x1.featnorm <-}\StringTok{ }\NormalTok{x1 }\OperatorTok{/}\StringTok{ }\KeywordTok{rowSums}\NormalTok{(x1)}
\NormalTok{x2.featnorm <-}\StringTok{ }\NormalTok{x2 }\OperatorTok{/}\StringTok{ }\KeywordTok{rowSums}\NormalTok{(x2)}
\NormalTok{x3.featnorm <-}\StringTok{ }\NormalTok{x3 }\OperatorTok{/}\StringTok{ }\KeywordTok{rowSums}\NormalTok{(x3)}

\CommentTok{# Normalize by each omics type's frobenius norm}
\NormalTok{x1.featnorm.frobnorm <-}\StringTok{ }\NormalTok{x1.featnorm }\OperatorTok{/}\StringTok{ }\KeywordTok{norm}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(x1.featnorm), }\DataTypeTok{type=}\StringTok{"F"}\NormalTok{)}
\NormalTok{x2.featnorm.frobnorm <-}\StringTok{ }\NormalTok{x2.featnorm }\OperatorTok{/}\StringTok{ }\KeywordTok{norm}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(x2.featnorm), }\DataTypeTok{type=}\StringTok{"F"}\NormalTok{)}
\NormalTok{x3.featnorm.frobnorm <-}\StringTok{ }\NormalTok{x3.featnorm }\OperatorTok{/}\StringTok{ }\KeywordTok{norm}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(x3.featnorm), }\DataTypeTok{type=}\StringTok{"F"}\NormalTok{)}

\CommentTok{# Split the features of the CNV matrix into two non-negative features each}
\NormalTok{x3.featnorm.frobnorm.nonneg <-}\StringTok{ }\KeywordTok{t}\NormalTok{(}\KeywordTok{split_neg_columns}\NormalTok{(}\KeywordTok{t}\NormalTok{(x3.featnorm.frobnorm)))}

\CommentTok{# run the nmf function from the NMF package}
\KeywordTok{require}\NormalTok{(NMF)}
\NormalTok{r.nmf <-}\StringTok{ }\KeywordTok{nmf}\NormalTok{(}\KeywordTok{t}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(x1.featnorm.frobnorm,}
\NormalTok{                     x2.featnorm.frobnorm,}
\NormalTok{                     x3.featnorm.frobnorm.nonneg)),}
             \DecValTok{2}\NormalTok{,}
             \DataTypeTok{method=}\StringTok{'Frobenius'}\NormalTok{)}

\CommentTok{# exctract the H and W matrices from the nmf run result}
\NormalTok{nmf.h <-}\StringTok{ }\NormalTok{NMF}\OperatorTok{::}\KeywordTok{basis}\NormalTok{(r.nmf)}
\NormalTok{nmf.w <-}\StringTok{ }\NormalTok{NMF}\OperatorTok{::}\KeywordTok{coef}\NormalTok{(r.nmf)}
\NormalTok{nmfw <-}\StringTok{ }\KeywordTok{t}\NormalTok{(nmf.w)}
\end{Highlighting}
\end{Shaded}

As with MFA, we can examine how well 2-factor NMF splits tumors into subtypes by looking at the scatter plot in Figure \ref{fig:monmfscatterplot}, generated by the following code chunk:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a dataframe with the H matrix and the CMS label (subtype)}
\NormalTok{nmf_df <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(nmf.h)}
\KeywordTok{colnames}\NormalTok{(nmf_df) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"dim1"}\NormalTok{, }\StringTok{"dim2"}\NormalTok{)}
\NormalTok{nmf_df}\OperatorTok{$}\NormalTok{subtype <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(covariates[}\KeywordTok{rownames}\NormalTok{(nmf_df),]}\OperatorTok{$}\NormalTok{cms_label)}

\CommentTok{# create the scatter plot}
\NormalTok{ggplot2}\OperatorTok{::}\KeywordTok{ggplot}\NormalTok{(nmf_df, ggplot2}\OperatorTok{::}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{dim1, }\DataTypeTok{y=}\NormalTok{dim2, }\DataTypeTok{color=}\NormalTok{subtype)) }\OperatorTok{+}
\NormalTok{ggplot2}\OperatorTok{::}\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\NormalTok{ggplot2}\OperatorTok{::}\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Scatter plot of 2-component NMF for multi-omics integration"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{11-multiomics-analysis_files/figure-latex/monmfscatterplot-1} 

}

\caption{NMF creates a disentangled representation of the data using two components which allow for separation between tumor sub-types CMS1 and CMS3 based on NMF factors learned from multi-omics data.}\label{fig:monmfscatterplot}
\end{figure}

Figure \ref{fig:monmfscatterplot} shows an important difference between NMF and MFA (PCA). It shows the tendency of samples to lie close to the X or Y axes, that is, the tendency of each sample to be high in only one of the factors. This will be discussed more in the later section on disentangledness.\index{disentangled representations}

Again, should we choose to run NMF with more than two factors, a more useful plot might be the heatmap shown in Figure \ref{fig:monmfheatmap}, generated by the following code snippet:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pheatmap}\OperatorTok{::}\KeywordTok{pheatmap}\NormalTok{(}\KeywordTok{t}\NormalTok{(nmf_df[,}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{]),}
                   \DataTypeTok{annotation_col =}\NormalTok{ anno_col,}
                   \DataTypeTok{show_colnames=}\OtherTok{FALSE}\NormalTok{,}
                   \DataTypeTok{main=}\StringTok{"Heatmap of 2-component NMF"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{11-multiomics-analysis_files/figure-latex/monmfheatmap-1} 

}

\caption{A heatmap of NMF factors shows separability of tumors into subtype clusters. This plot is more useful than a scatter plot when there are more than two factors.}\label{fig:monmfheatmap}
\end{figure}

\begin{rmdtip}
\begin{rmdtip}

\textbf{Want to know more ?}

\begin{itemize}
\tightlist
\item
  Joint NMF to uncover gene regulatory networks: Zhang S., Li Q., Liu J., Zhou X. J. (2011). A novel computational framework for simultaneous integration of multiple types of genomic data to identify microRNA-gene regulatory modules. \emph{Bioinformatics} 27, i401--i409. 10.1093/bioinformatics/btr206 \url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3117336/}
\item
  Joint NMF for cancer research: Zhang S., Liu C.-C., Li W., Shen H., Laird P. W., Zhou X. J. (2012). Discovery of multi-dimensional modules by integrative analysis of cancer genomic data. \emph{Nucleic Acids Res.} 40, 9379--9391. 10.1093/nar/gks725 \url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3479191/}
\end{itemize}

\end{rmdtip}
\end{rmdtip}

\hypertarget{icluster}{%
\subsection{iCluster}\label{icluster}}

\index{iCluster}iCluster takes a Bayesian approach to the latent variable model. In Bayesian statistics, we infer distributions over model parameters, rather than finding a single maximum-likelihood parameter estimate. In iCluster, we model the data as

\[
X_{(i)} = W_{(i)}Z + \epsilon_i,
\]

where \(X_{(i)}\) is a data matrix from a single omics platform, \(W_{(i)}\) are model parameters, \(Z\) is a latent variable matrix, and is shared among the different omics platforms, and \(\epsilon_i\) is a ``noise'' random variable, \(\epsilon \sim N(0,\Psi)\), with \(\Psi = diag(\psi_1,\dots \psi_M)\) is a diagonal covariance matrix.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{images/icluster} 

}

\caption{Sketch of iCluster model. Each omics datatype is decomposed to a coefficient matrix and a shared latent variable matrix, plus noise.}\label{fig:moiCluster}
\end{figure}

Note that with this construction, the omics measurements \(X\) are expected to be the same for samples with the same latent variable representation, up to Gaussian noise. Further, we assume a Gaussian prior distribution on the latent variables \(Z \sim N(0,I)\), which means we assume \(X_{(i)} \sim N \big( 0,W_{(i)} W_{(i)}^T + \Psi_{(i)} \big)\). In order to find suitable values for \(W\), \(Z\), and \(\Psi\), we can write down the multivariate normal log-likelihood function and optimize it. For a multivariate normal distribution with mean \(0\) and covariance \(\Sigma\), the log-likelihood function is given by

\[
\ell = -\frac{1}{2} \bigg( \ln (|\Sigma|) + X^T \Sigma^{-1} X + k\ln (2 \pi) \bigg)
\]

(this is simply the log of the Probability Density Function of a multivariate Gaussian). For the multi-omics iCluster case, we have \(X=\big( X_{(1)}, \dots, X_{(L)} \big)^T\), \(W = \big( W_{(1)}, \dots, W_{(L)} \big)^T\), where \(X\) is a multivariate normal with \(0\)-mean and \(\Sigma = W W^T + \Psi\) covariance. Hence, the log-likelihood function for the iCluster model is given by:

\[
\ell_{iC}(W,\Sigma) = -\frac{1}{2} \bigg( \sum_{i=1}^L \ln (|\Sigma|) + X^T\Sigma^{-1}X + p_i \ln (2 \pi) \bigg)
\]

where \(p_i\) is the number of features in omics data type \(i\). Because this model has more parameters than we typically have samples, we need to push the model to use fewer parameters than it has at its disposal, by using regularization. iCluster uses Lasso regularization, which is a direct penalty on the absolute value of the parameters. I.e., instead of optimizing \(\ell_{iC}(W,\Sigma)\), we will optimize the regularized log-likelihood:\index{loss function}

\[
\ell = \ell_{iC}(W,\Sigma) - \lambda\|W\|_1.
\]

The parameter \(\lambda\) acts as a dial to weigh the trade-off between better model fits (higher log-likelihood) and a sparser model, with more \(w_{ij}\)s set to \(0\), which gives models which generalize better and are more interpretable.

In order to solve this problem, iCluster employs the Expectation Maximization (EM) algorithm. The full details are beyond the scope of this textbook. We will introduce a short sketch instead. The intuition behind the EM algorithm is a more general case of the k-means clustering algorithm (Chapter 4). The basic \textbf{EM algorithm} is as follows.

\begin{itemize}
\tightlist
\item
  Initialize \(W\) and \(\Psi\).
\item
  \textbf{Until convergence of \(W\), \(\Psi\)}

  \begin{itemize}
  \tightlist
  \item
    E-step: Calculate the expected value of \(Z\) given the current estimates of \(W\) and \(\Psi\) and the data \(X\)
  \item
    M-step: Calculate maximum likelihood estimates for the parameters \(W\) and \(\Psi\) based on the current estimate of \(Z\) and the data \(X\).
  \end{itemize}
\end{itemize}

\hypertarget{icluster-extending-icluster}{%
\subsubsection{iCluster+: Extending iCluster}\label{icluster-extending-icluster}}

iCluster+ is an extension of the iCluster framework, which allows for omics types to arise from distributions other than a Gaussian. While normal distributions are a good assumption for log-transformed, centered gene expression data, it is a poor model for binary mutations data, or for copy number variation data, which can typically take the values \((-2, 1, 0, 1, 2)\) for heterozygous / monozygous deletions or amplifications. iCluster+ allows the different \(X\)s to have different distributions:

\begin{itemize}
\tightlist
\item
  for binary mutations, \(X\) is drawn from a multivariate binomial
\item
  for normal, continuous data, \(X\) is drawn from a multivariate Gaussian
\item
  for copy number variations, \(X\) is drawn from a multinomial
\item
  for count data, \(X\) is drawn from a Poisson.
\end{itemize}

In that way, iCluster+ allows us to explicitly model our assumptions about the distributions of our different omics data types, and leverage the strengths of Bayesian inference.

Both iCluster and iCluster+ make use of sophisticated Bayesian inference algorithms (EM for iCluster, Metropolis-Hastings MCMC for iCluster+), which means they do not scale up trivially. Therefore, it is recommended to filter down the features to a manageable size before inputting data to the algorithm. The exact size of ``manageable'' data depends on your hardware, but a rule of thumb is that dimensions in the thousands are ok, but in the tens of thousands might be too slow.

\hypertarget{running-icluster}{%
\subsubsection{Running iCluster+}\label{running-icluster}}

iCluster+ is available through the Bioconductor package \texttt{iClusterPlus}. The following code snippet demonstrates how it can be run with two components:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# run the iClusterPlus function}
\NormalTok{r.icluster <-}\StringTok{ }\NormalTok{iClusterPlus}\OperatorTok{::}\KeywordTok{iClusterPlus}\NormalTok{(}
  \KeywordTok{t}\NormalTok{(x1), }\CommentTok{# Providing each omics type}
  \KeywordTok{t}\NormalTok{(x2),}
  \KeywordTok{t}\NormalTok{(x3),}
  \DataTypeTok{type=}\KeywordTok{c}\NormalTok{(}\StringTok{"gaussian"}\NormalTok{, }\StringTok{"binomial"}\NormalTok{, }\StringTok{"multinomial"}\NormalTok{), }\CommentTok{# Providing the distributions}
  \DataTypeTok{K=}\DecValTok{2}\NormalTok{, }\CommentTok{# provide the number of factors to learn}
  \DataTypeTok{alpha=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }\CommentTok{# as well as other model parameters}
  \DataTypeTok{lambda=}\KeywordTok{c}\NormalTok{(.}\DecValTok{03}\NormalTok{,.}\DecValTok{03}\NormalTok{,.}\DecValTok{03}\NormalTok{))}

\CommentTok{# extract the H and W matrices from the run result}
\CommentTok{# here, we refer to H as z, to keep with iCluster terminology}
\NormalTok{icluster.z <-}\StringTok{ }\NormalTok{r.icluster}\OperatorTok{$}\NormalTok{meanZ}
\KeywordTok{rownames}\NormalTok{(icluster.z) <-}\StringTok{ }\KeywordTok{rownames}\NormalTok{(covariates) }\CommentTok{# fix the row names}
\NormalTok{icluster.ws <-}\StringTok{ }\NormalTok{r.icluster}\OperatorTok{$}\NormalTok{beta}

\CommentTok{# construct a dataframe with the H matrix (z) and the cancer subtypes}
\CommentTok{# for later plotting}
\NormalTok{icp_df <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(icluster.z)}
\KeywordTok{colnames}\NormalTok{(icp_df) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"dim1"}\NormalTok{, }\StringTok{"dim2"}\NormalTok{)}
\KeywordTok{rownames}\NormalTok{(icp_df) <-}\StringTok{ }\KeywordTok{colnames}\NormalTok{(x1)}
\NormalTok{icp_df}\OperatorTok{$}\NormalTok{subtype <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(covariates[}\KeywordTok{rownames}\NormalTok{(icp_df),]}\OperatorTok{$}\NormalTok{cms_label)}
\end{Highlighting}
\end{Shaded}

As with other methods, we examine the iCluster results by looking at the scatter plot in Figure \ref{fig:moiclusterplusscatter} and the heatmap in Figure \ref{fig:moiclusterplusheatmap}. Both figures show that iCluster learns two factors which nearly perfectly discriminate between tumors of the two subtypes.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{11-multiomics-analysis_files/figure-latex/moiclusterplusscatter-1} 

}

\caption{iCluster+ learns factors which allow tumor sub-types CMS1 and CMS3 to be discriminated.}\label{fig:moiclusterplusscatter}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{11-multiomics-analysis_files/figure-latex/moiclusterplusheatmap-1} 

}

\caption{iCluster+ factors, shown in a heatmap, separate tumors into their subtypes well.}\label{fig:moiclusterplusheatmap}
\end{figure}

\begin{rmdtip}
\begin{rmdtip}

\textbf{Want to know more ?}

\begin{itemize}
\tightlist
\item
  Read the original iCluster paper: Shen R., Olshen A. B., Ladanyi M. (2009). Integrative clustering of multiple genomic data types using a joint latent variable model with application to breast and lung cancer subtype analysis. \emph{Bioinformatics} 25, 2906--2912. 10.1093/bioinformatics/btp543 \url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2800366/}
\item
  Read the original iClusterPlus paper: an extension of iCluster: Shen R., Mo Q., Schultz N., Seshan V. E., Olshen A. B., Huse J., et al.~(2012). Integrative subtype discovery in glioblastoma using iCluster. \emph{PLoS ONE} 7:e35236. 10.1371/journal.pone.0035236 \url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3335101/}
\item
  Learn more about the LASSO for model regularization: Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. \emph{J. Royal. Statist. Soc B.}, Vol. 58, No.~1, pages 267-288: \url{http://www-stat.stanford.edu/\%7Etibs/lasso/lasso.pdf}
\item
  Learn more about the EM algorithm: Dempster, A. P., et al.~``Maximum likelihood from incomplete data via the EM algorithm.'' \emph{Journal of the Royal Statistical Society. Series B (Methodological)}, vol.~39, no. 1, 1977, pp.~1--38. JSTOR, JSTOR: \url{http://www.jstor.org/stable/2984875}
\item
  Read about MCMC algorithms: Hastings, W.K. (1970). ``Monte Carlo sampling methods using markov chains and their applications''. \emph{Biometrika.} 57 (1): 97--109. \url{doi:10.1093/biomet/57.1.97}: \url{https://www.jstor.org/stable/2334940}
\end{itemize}

\end{rmdtip}
\end{rmdtip}

\hypertarget{clustering-using-latent-factors}{%
\section{Clustering using latent factors}\label{clustering-using-latent-factors}}

\index{clustering}\index{unsupervised learning}A common analysis in biological investigations is clustering. This is often interesting in cancer studies as one hopes to find groups of tumors (clusters) which behave similarly, i.e.~have similar risks and/or respond to the same drugs. PCA is a common step in clustering analyses, and so it is easy to see how the latent variable models above may all be a useful pre-processing step before clustering. In the examples below, we will use the latent variables inferred by the algorithms in the previous section on the set of colorectal cancer tumors from the TCGA. For a more complete introduction to clustering, see Chapter \ref{unsupervisedLearning}.

\hypertarget{one-hot-clustering}{%
\subsection{One-hot clustering}\label{one-hot-clustering}}

A specific clustering method for NMF data is to assume each sample is driven by one component, i.e.~that the number of clusters \(K\) is the same as the number of latent variables in the model and that each sample may be associated to one of those components. We assign each sample a cluster label based on the latent variable which affects it the most. Figure \ref{fig:monmfheatmap} above (Heatmap of 2-component NMF) shows the latent variable values for the two latent variables, for the 72 tumors, obtained by Joint NMF.

The two rows are the two latent variables, and the columns are the 72 tumors. We can observe that most tumors are indeed driven mainly by one of the factors, and not a combination of the two. We can use this to assign each tumor a cluster label based on its dominant factor, shown in the following code snippet, which also produces the heatmap in Figure \ref{fig:moNMFClustering}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# one-hot clustering in one line of code:}
\CommentTok{# assign each sample the cluster according to its dominant NMF factor}
\CommentTok{# easily accessible using the max.col function}
\NormalTok{nmf.clusters <-}\StringTok{ }\KeywordTok{max.col}\NormalTok{(nmf.h)}
\KeywordTok{names}\NormalTok{(nmf.clusters) <-}\StringTok{ }\KeywordTok{rownames}\NormalTok{(nmf.h)}

\CommentTok{# create an annotation data frame indicating the NMF one-hot clusters}
\CommentTok{# as well as the cancer subtypes, for the heatmap plot below}
\NormalTok{anno_nmf_cl <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \DataTypeTok{nmf.cluster=}\KeywordTok{factor}\NormalTok{(nmf.clusters),}
  \DataTypeTok{cms.subtype=}\KeywordTok{factor}\NormalTok{(covariates[}\KeywordTok{rownames}\NormalTok{(nmf.h),]}\OperatorTok{$}\NormalTok{cms_label)}
\NormalTok{)}

\CommentTok{# generate the plot}
\NormalTok{pheatmap}\OperatorTok{::}\KeywordTok{pheatmap}\NormalTok{(}\KeywordTok{t}\NormalTok{(nmf.h[}\KeywordTok{order}\NormalTok{(nmf.clusters),]),}
  \DataTypeTok{cluster_cols=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{cluster_rows=}\OtherTok{FALSE}\NormalTok{,}
  \DataTypeTok{annotation_col =}\NormalTok{ anno_nmf_cl,}
  \DataTypeTok{show_colnames =} \OtherTok{FALSE}\NormalTok{,}\DataTypeTok{border_color=}\OtherTok{NA}\NormalTok{,}
  \DataTypeTok{main=}\StringTok{"Joint NMF factors}\CharTok{\textbackslash{}n}\StringTok{with clusters and molecular subtypes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{11-multiomics-analysis_files/figure-latex/moNMFClustering-1} 

}

\caption{Joint NMF factors with clusters, and molecular sub-types. One-hot clustering assigns one cluser per dimension, where each sample is assigned a cluster based on its dominant component. The clusters largely recapitulate the CMS sub-types.}\label{fig:moNMFClustering}
\end{figure}

We see that using one-hot clustering with Joint NMF, we were able to find two clusters in the data which correspond fairly well with the molecular subtype of the tumors.

The one-hot clustering method does not lend itself very well to the other methods discussed above, i.e.~iCluster and MFA. The latent variables produced by those other methods may be negative, and further, in the case of iCluster, are going to assume a multivariate Gaussian shape. As such, it is not trivial to pick one ``dominant factor'' for them. For NMF variants, this is a very common way to assign clusters.

\hypertarget{k-means-clustering-1}{%
\subsection{K-means clustering}\label{k-means-clustering-1}}

K-means \index{clustering!k-means} clustering was introduced in Chapter \ref{unsupervisedLearning}. Briefly, k-means is a special case of the EM algorithm, and indeed iCluster was originally conceived as an extension of K-means from binary cluster assignments to real-valued latent variables. The iCluster algorithm, as it is so named, calls for application of K-means clustering on its latent variables, after the inference step. The following code snippet shows how to pull K-means clusters out of the iCluster results, and produces the heatmap in Figure \ref{fig:moiClusterHeatmap}, which shows how well these clusters correspond to cancer subtypes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# use the kmeans function to cluster the iCluster H matrix (here, z)}
\CommentTok{# using 2 as the number of clusters.}
\NormalTok{icluster.clusters <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(icluster.z, }\DecValTok{2}\NormalTok{)}\OperatorTok{$}\NormalTok{cluster}
\KeywordTok{names}\NormalTok{(icluster.clusters) <-}\StringTok{ }\KeywordTok{rownames}\NormalTok{(icluster.z)}

\CommentTok{# create an annotation dataframe for the heatmap plot}
\CommentTok{# containing the kmeans cluster assignments and the cancer subtypes}
\NormalTok{anno_icluster_cl <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \DataTypeTok{iCluster=}\KeywordTok{factor}\NormalTok{(icluster.clusters),}
  \DataTypeTok{cms.subtype=}\KeywordTok{factor}\NormalTok{(covariates}\OperatorTok{$}\NormalTok{cms_label))}

\CommentTok{# generate the figure}
\NormalTok{pheatmap}\OperatorTok{::}\KeywordTok{pheatmap}\NormalTok{(}
  \KeywordTok{t}\NormalTok{(icluster.z[}\KeywordTok{order}\NormalTok{(icluster.clusters),]), }\CommentTok{# order z by the kmeans clusters}
  \DataTypeTok{cluster_cols=}\OtherTok{FALSE}\NormalTok{, }\CommentTok{# use cluster_cols and cluster_rows=FALSE}
  \DataTypeTok{cluster_rows=}\OtherTok{FALSE}\NormalTok{, }\CommentTok{# as we want the ordering by k-means clusters to hold}
  \DataTypeTok{show_colnames =} \OtherTok{FALSE}\NormalTok{,}\DataTypeTok{border_color=}\OtherTok{NA}\NormalTok{,}
  \DataTypeTok{annotation_col =}\NormalTok{ anno_icluster_cl,}
  \DataTypeTok{main=}\StringTok{"iCluster factors}\CharTok{\textbackslash{}n}\StringTok{with clusters and molecular subtypes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{11-multiomics-analysis_files/figure-latex/moiClusterHeatmap-1} 

}

\caption{K-means clustering on iCluster+ factors largely recapitulates the CMS sub-types.}\label{fig:moiClusterHeatmap}
\end{figure}

This demonstrates the ability of iClusterPlus to find clusters which correspond to molecular subtypes, based on multi-omics data.

\hypertarget{biological-interpretation-of-latent-factors}{%
\section{Biological interpretation of latent factors}\label{biological-interpretation-of-latent-factors}}

\hypertarget{inspection-of-feature-weights-in-loading-vectors}{%
\subsection{Inspection of feature weights in loading vectors}\label{inspection-of-feature-weights-in-loading-vectors}}

The most straightforward way to go about interpreting the latent factors in a biological context, is to look at the coefficients which are associated with them. The latent variable models introduced above all take the linear form \(X \approx WH\), where \(W\) is a factor matrix, with coefficients tying each latent variable with each of the features in the \(L\) original multi-omics data matrices. By inspecting these coefficients, we can get a sense of which multi-omics features are co-regulated. The code snippet below generates Figure \ref{fig:moNMFHeatmap}, which shows the coefficients of the Joint NMF analysis above:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create an annotation dataframe for the heatmap}
\CommentTok{# for each feature, indicating its omics-type}
\NormalTok{data_anno <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \DataTypeTok{omics=}\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\StringTok{'expression'}\NormalTok{,}\KeywordTok{dim}\NormalTok{(x1)[}\DecValTok{1}\NormalTok{]),}
          \KeywordTok{rep}\NormalTok{(}\StringTok{'mut'}\NormalTok{,}\KeywordTok{dim}\NormalTok{(x2)[}\DecValTok{1}\NormalTok{]),}
          \KeywordTok{rep}\NormalTok{(}\StringTok{'cnv'}\NormalTok{,}\KeywordTok{dim}\NormalTok{(x3.featnorm.frobnorm.nonneg)[}\DecValTok{1}\NormalTok{])))}
\KeywordTok{rownames}\NormalTok{(data_anno) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rownames}\NormalTok{(x1),}
                         \KeywordTok{paste0}\NormalTok{(}\StringTok{"mut:"}\NormalTok{, }\KeywordTok{rownames}\NormalTok{(x2)),}
                         \KeywordTok{rownames}\NormalTok{(x3.featnorm.frobnorm.nonneg))}
\KeywordTok{rownames}\NormalTok{(nmfw) <-}\StringTok{ }\KeywordTok{rownames}\NormalTok{(data_anno)}

\CommentTok{# generate the heat map}
\NormalTok{pheatmap}\OperatorTok{::}\KeywordTok{pheatmap}\NormalTok{(nmfw,}
                   \DataTypeTok{cluster_cols =} \OtherTok{FALSE}\NormalTok{,}
                   \DataTypeTok{annotation_row =}\NormalTok{ data_anno,}
                   \DataTypeTok{main=}\StringTok{"NMF coefficients"}\NormalTok{,}
                   \DataTypeTok{clustering_distance_rows =} \StringTok{"manhattan"}\NormalTok{,}
                   \DataTypeTok{fontsize_row =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{11-multiomics-analysis_files/figure-latex/moNMFHeatmap-1} 

}

\caption{Heatmap showing the association of input features from multi-omics data (gene expression, copy number variation, and mutations), with JNMF factors. Gene expression features dominate both factors, but copy numbers and mutations mostly affect only one factor each.}\label{fig:moNMFHeatmap}
\end{figure}

Inspection of the factor coefficients in the heatmap above reveals that Joint NMF has found two nearly orthogonal non-negative factors. One is associated with high expression of the HOXC11, ZIC5, and XIRP1 genes, frequent mutations in the BRAF, PCDHGA6, and DNAH5 genes, as well as losses in the 18q12.2 and gains in 8p21.1 cytobands. The other factor is associated with high expression of the SOX1 gene, more frequent mutations in the APC, KRAS, and TP53 genes, and a weak association with some CNVs.

\hypertarget{disentangled-representations}{%
\subsubsection{Disentangled representations}\label{disentangled-representations}}

\index{disentangled representations}The property displayed above, where each feature is predominantly associated with only a single factor, is termed \emph{disentangledness}, i.e.~it leads to \emph{disentangled} latent variable representations, as changing one input feature only affects a single latent variable. This property is very desirable as it greatly simplifies the biological interpretation of modules. Here, we have two modules with a set of co-occurring molecular signatures which merit deeper investigation into the mechanisms by which these different omics features are related. For this reason, NMF is widely used in computational biology today.

\hypertarget{making-sense-of-factors-using-enrichment-analysis}{%
\subsection{Making sense of factors using enrichment analysis}\label{making-sense-of-factors-using-enrichment-analysis}}

\index{enrichment analysis}In order to investigate the oncogenic processes that drive the differences between tumors, we may draw upon biological prior knowledge by looking for overlaps between genes that drive certain tumors, and genes involved in familiar biological processes.

\hypertarget{enrichment-analysis}{%
\subsubsection{Enrichment analysis}\label{enrichment-analysis}}

The recent decades of genomics have uncovered many of the ways in which genes cooperate to perform biological functions in concert. This work has resulted in rich annotations of genes, groups of genes, and the different functions they carry out. Examples of such annotations include the Gene Ontology Consortium's \emph{GO terms} \citep[\citet{go_latest_paper}]{go_first_paper}, the \emph{Reactome pathways database} \citep{reactome_latent_paper}, and the \emph{Kyoto Encyclopaedia of Genes and Genomes} \citep{kegg_latest_paper}. These resources, as well as others, publish lists of so-called \emph{gene sets}, or \emph{pathways}, which are sets of genes which are known to operate together in some biological function, e.g.~protein synthesis, DNA mismatch repair, cellular adhesion, and many other functions. Gene set enrichment analysis is a method which looks for overlaps between genes which we have found to be of interest, e.g.~by them being implicated in a certain tumor type, and the a-priori gene sets discussed above.

In the context of making sense of latent factors, the question we will be asking is whether the genes which drive the value of a latent factor (the genes with the highest factor coefficients) also belong to any interesting annotated gene sets, and whether the overlap is greater than we would expect by chance. If there are \(N\) genes in total, \(K\) of which belong to a gene set, the probability that \(k\) out of the \(n\) genes associated with a latent factor are also associated with a gene set is given by the hypergeometric distribution:

\[
P(k) = \frac{{\binom{K}{k}} - \binom{N-K}{n-k}}{\binom{N}{n}}.
\]

The \textbf{hypergeometric test} \index{statistical test} uses the hypergeometric distribution to assess the statistical significance of the presence of genes belonging to a gene set in the latent factor. The null hypothesis is that there is no relationship between genes in a gene set, and genes in a latent factor. When testing for over-representation of gene set genes in a latent factor, the P value from the hypergeometric test is the probability of getting \(k\) or more genes from a gene set in a latent factor

\[
p = \sum_{i=k}^K P(k=i).
\]

The hypergeometric enrichment test is also referred to as \emph{Fisher's one-sided exact test}. This way, we can determine if the genes associated with a factor significantly overlap (beyond chance) the genes involved in a biological process. Because we will typically be testing many gene sets, we will also need to apply multiple testing correction, such as Benjamini-Hochberg correction (see Chapter 3, multiple testing correction).

\hypertarget{example-in-r}{%
\subsubsection{Example in R}\label{example-in-r}}

In R, we can do this analysis using the \texttt{enrichR} package, which gives us access to many gene set libraries. In the example below, we will find the genes associated with preferentially NMF factor 1 or NMF factor 2, by the contribution of those genes' expression values to the factor. Then, we'll use \texttt{enrichR} to query the Gene Ontology terms which might be overlapping:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(enrichR)}

\CommentTok{# select genes associated preferentially with each factor}
\CommentTok{# by their relative loading in the W matrix}
\NormalTok{genes.factor}\FloatTok{.1}\NormalTok{ <-}\StringTok{ }\KeywordTok{names}\NormalTok{(}\KeywordTok{which}\NormalTok{(nmfw[}\DecValTok{1}\OperatorTok{:}\KeywordTok{dim}\NormalTok{(x1)[}\DecValTok{1}\NormalTok{],}\DecValTok{1}\NormalTok{] }\OperatorTok{>}\StringTok{ }\NormalTok{nmfw[}\DecValTok{1}\OperatorTok{:}\KeywordTok{dim}\NormalTok{(x1)[}\DecValTok{1}\NormalTok{],}\DecValTok{2}\NormalTok{]))}
\NormalTok{genes.factor}\FloatTok{.2}\NormalTok{ <-}\StringTok{ }\KeywordTok{names}\NormalTok{(}\KeywordTok{which}\NormalTok{(nmfw[}\DecValTok{1}\OperatorTok{:}\KeywordTok{dim}\NormalTok{(x1)[}\DecValTok{1}\NormalTok{],}\DecValTok{1}\NormalTok{] }\OperatorTok{<}\StringTok{ }\NormalTok{nmfw[}\DecValTok{1}\OperatorTok{:}\KeywordTok{dim}\NormalTok{(x1)[}\DecValTok{1}\NormalTok{],}\DecValTok{2}\NormalTok{]))}

\CommentTok{# call the enrichr function to find gene sets enriched}
\CommentTok{# in each latent factor in the GO Biological Processes 2018 library}
\NormalTok{go.factor}\FloatTok{.1}\NormalTok{ <-}\StringTok{ }\NormalTok{enrichR}\OperatorTok{::}\KeywordTok{enrichr}\NormalTok{(genes.factor}\FloatTok{.1}\NormalTok{,}
                                \DataTypeTok{databases =} \KeywordTok{c}\NormalTok{(}\StringTok{"GO_Biological_Process_2018"}\NormalTok{)}
\NormalTok{                                )}\OperatorTok{$}\NormalTok{GO_Biological_Process_}\DecValTok{2018}
\NormalTok{go.factor}\FloatTok{.2}\NormalTok{ <-}\StringTok{ }\NormalTok{enrichR}\OperatorTok{::}\KeywordTok{enrichr}\NormalTok{(genes.factor}\FloatTok{.2}\NormalTok{,}
                                \DataTypeTok{databases =} \KeywordTok{c}\NormalTok{(}\StringTok{"GO_Biological_Process_2018"}\NormalTok{)}
\NormalTok{                                )}\OperatorTok{$}\NormalTok{GO_Biological_Process_}\DecValTok{2018}
\end{Highlighting}
\end{Shaded}

The top GO terms associated with NMF factor 2 are shown in Table \ref{tab:moNMFGOTerms}:

\begin{table}

\caption{\label{tab:moNMFGOTerms}GO-terms associated with NMF factor 2}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{l|r|r}
\hline
Term & Adjusted.P.value & Combined.Score\\
\hline
nuclear-transcribed mRNA catabolic process (GO:0000956) & 0 & 207.7403\\
\hline
rRNA metabolic process (GO:0016072) & 0 & 161.4781\\
\hline
nuclear-transcribed mRNA catabolic process, nonsense-mediated decay (GO:0000184) & 0 & 220.4298\\
\hline
\end{tabular}}
\end{table}

\hypertarget{interpretation-using-additional-covariates}{%
\subsection{Interpretation using additional covariates}\label{interpretation-using-additional-covariates}}

Another way to ascribe biological significance to the latent variables is by correlating them with additional covariates we might have about the samples. In our example, the colorectal cancer tumors have also been characterized for microsatellite instability (MSI) status, using an external test (typically PCR-based). By examining the latent variable values as they relate to a tumor's MSI status, we might discover that we've learned latent factors that are related to it. The following code snippet demonstrates how this might be looked into, by generating Figures \ref{fig:moNMFClinicalCovariates} and \ref{fig:moNMFClinicalCovariates2}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a data frame holding covariates (age, gender, MSI status)}
\NormalTok{a <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{age=}\NormalTok{covariates}\OperatorTok{$}\NormalTok{age,}
                \DataTypeTok{gender=}\KeywordTok{as.numeric}\NormalTok{(covariates}\OperatorTok{$}\NormalTok{gender),}
                \DataTypeTok{msi=}\NormalTok{covariates}\OperatorTok{$}\NormalTok{msi)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in data.frame(age = covariates$age, gender =
## as.numeric(covariates$gender), : NAs introduced by coercion
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b <-}\StringTok{ }\NormalTok{nmf.h}
\KeywordTok{colnames}\NormalTok{(b) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'factor1'}\NormalTok{, }\StringTok{'factor2'}\NormalTok{)}

\CommentTok{# concatenate the covariate dataframe with the H matrix}
\NormalTok{cov_factor <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(a,b)}

\CommentTok{# generate the figure}
\NormalTok{ggplot2}\OperatorTok{::}\KeywordTok{ggplot}\NormalTok{(cov_factor, ggplot2}\OperatorTok{::}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{msi, }\DataTypeTok{y=}\NormalTok{factor1, }\DataTypeTok{group=}\NormalTok{msi)) }\OperatorTok{+}
\StringTok{  }\NormalTok{ggplot2}\OperatorTok{::}\KeywordTok{geom_boxplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\NormalTok{ggplot2}\OperatorTok{::}\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"NMF factor 1 microsatellite instability"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{11-multiomics-analysis_files/figure-latex/moNMFClinicalCovariates-1} 

}

\caption{Box plot showing MSI/MSS status distribution and NMF factor 1 values.}\label{fig:moNMFClinicalCovariates}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggplot2}\OperatorTok{::}\KeywordTok{ggplot}\NormalTok{(cov_factor, ggplot2}\OperatorTok{::}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{msi, }\DataTypeTok{y=}\NormalTok{factor2, }\DataTypeTok{group=}\NormalTok{msi)) }\OperatorTok{+}
\StringTok{  }\NormalTok{ggplot2}\OperatorTok{::}\KeywordTok{geom_boxplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\NormalTok{ggplot2}\OperatorTok{::}\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"NMF factor 2 and microsatellite instability"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{11-multiomics-analysis_files/figure-latex/moNMFClinicalCovariates2-1} 

}

\caption{Box plot showing MSI/MSS status distribution and NMF factor 2 values.}\label{fig:moNMFClinicalCovariates2}
\end{figure}

Figures \ref{fig:moNMFClinicalCovariates} and \ref{fig:moNMFClinicalCovariates2} show that NMF factor 1 and NMF factor 2 are separated by the MSI or MSS (microsatellite stability) status of the tumors.

\hypertarget{exercises-9}{%
\section{Exercises}\label{exercises-9}}

\hypertarget{matrix-factorization-methods}{%
\subsection{Matrix factorization methods}\label{matrix-factorization-methods}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Find features associated with iCluster and MFA factors, and visualize the feature weights. {[}Difficulty: \textbf{Beginner}{]}
\item
  Normalizing the data matrices by their \(\lambda_1\)'s as in MFA supposes we wish to assign each data type the same importance in the down-stream analysis. This leads to a natural generalization whereby the different data types may be differently weighted. Provide an implementation of weighed-MFA where the different data types may be assigned individual weights. {[}Difficulty: \textbf{Intermediate}{]}
\item
  In order to use NMF algorithms on data which can be negative, we need to split each feature into two new features, one positive and one negative. Implement the following function, and see that the included test does not fail: {[}Difficulty: \textbf{Intermediate/Advanced}{]}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Implement this function}
\NormalTok{split_neg_columns <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
    \CommentTok{# your code here}
\NormalTok{\}}

\CommentTok{# a test that shows the function above works}
\NormalTok{test_split_neg_columns <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{() \{}
\NormalTok{    input <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\OperatorTok{-}\DecValTok{2}\NormalTok{)))}
\NormalTok{    output <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{)))}
    \KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{all}\NormalTok{(output }\OperatorTok{==}\StringTok{ }\KeywordTok{split_neg_columns}\NormalTok{(input)))}
\NormalTok{\}}

\CommentTok{# run the test to verify your solution}
\KeywordTok{test_split_neg_columns}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  The iCluster+ algorithm has some parameters which may be tuned for maximum performance. The \texttt{iClusterPlus} package has a method, \texttt{iClusterPlus::tune.iClusterPlus}, which does this automatically based on the Bayesian Information Criterion (BIC). Run this method on the data from the examples above and find the optimal lambda and alpha values. {[}Difficulty: \textbf{Beginner/Intermediate}{]}
\end{enumerate}

\hypertarget{clustering-using-latent-factors-1}{%
\subsection{Clustering using latent factors}\label{clustering-using-latent-factors-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why is one-hot clustering more suitable for NMF than iCluster? {[}Difficulty: \textbf{Intermediate}{]}
\item
  Which clustering algorithm produces better results when combined with NMF, K-means, or one-hot clustering? Why do you think that is? {[}Difficulty: \textbf{Intermediate/Advanced}{]}
\end{enumerate}

\hypertarget{biological-interpretation-of-latent-factors-1}{%
\subsection{Biological interpretation of latent factors}\label{biological-interpretation-of-latent-factors-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Another covariate in the metadata of these tumors is their \emph{CpG island methylator Phenotype} (CIMP). This is a phenotype carried by a group of colorectal cancers that display hypermethylation of promoter CpG island sites, resulting in the inactivation of some tumor suppressors. This is also assayed using an external test. Do any of the multi-omics methods surveyed find a latent variable that is associated with the tumor's CIMP phenotype? {[}Difficulty: \textbf{Beginner/Intermediate}{]}
\item
  Does MFA give a disentangled representation? Does \texttt{iCluster} give disentangled representations? Why do you think that is? {[}Difficulty: \textbf{Advanced}{]}
\item
  Figures \ref{fig:moNMFClinicalCovariates} and \ref{fig:moNMFClinicalCovariates2} show that MSI/MSS tumors have different values for NMF factors 1 and 2. Which NMF factor is associated with microsatellite instability? {[}Difficulty: \textbf{Beginner}{]}
\item
  Microsatellite instability (MSI) is associated with hyper-mutated tumors. As seen in Figure \ref{fig:momutationsHeatmap}, one of the subtypes has tumors with significantly more mutations than the other. Which subtype is that? Which NMF factor is associated with that subtype? And which NMF factor is associated with MSI? {[}Difficulty: \textbf{Advanced}{]}
\end{enumerate}

  \bibliography{book.bib}

\backmatter
\printindex

\end{document}
